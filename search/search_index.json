{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview \u00b6 Bactopia is an extensive workflow to process Illumina sequencing for bacterial genomes. The goal of Bactopia is process your data with a broad set of tools, so that you can get to the fun part of analyses quicker! Bactopia prioritizes software available from Bioconda (or other Anaconda channels ) to make installation and setup easier. The Bactopia workflow is also encapsulated as a Nextflow workflow to allow support for many types of environments (e.g. cluster or cloud). Documentation Overview \u00b6 Quick Start Very concise and to straight the point details (unlike this!) for using Bactopia. Installation More detailed information for getting Bactopia set up on your system. Tutorial A brief tutorial on how to replicate the Staphopia Analyis Pipeline using Bactopia. Build Datasets A description on how to make use of datasets (public or private) with Bactopia. Basic Usage A subset of parameters users may commonly adjust. Complete Usage The full set of parameters that users can tweak in Bactopia. About Learn more about the history of Bactopia. Acknowledgements See this page for a list of datasets and software (and many thanks!) used by Bactopia.","title":"Introduction"},{"location":"#overview","text":"Bactopia is an extensive workflow to process Illumina sequencing for bacterial genomes. The goal of Bactopia is process your data with a broad set of tools, so that you can get to the fun part of analyses quicker! Bactopia prioritizes software available from Bioconda (or other Anaconda channels ) to make installation and setup easier. The Bactopia workflow is also encapsulated as a Nextflow workflow to allow support for many types of environments (e.g. cluster or cloud).","title":"Overview"},{"location":"#documentation-overview","text":"Quick Start Very concise and to straight the point details (unlike this!) for using Bactopia. Installation More detailed information for getting Bactopia set up on your system. Tutorial A brief tutorial on how to replicate the Staphopia Analyis Pipeline using Bactopia. Build Datasets A description on how to make use of datasets (public or private) with Bactopia. Basic Usage A subset of parameters users may commonly adjust. Complete Usage The full set of parameters that users can tweak in Bactopia. About Learn more about the history of Bactopia. Acknowledgements See this page for a list of datasets and software (and many thanks!) used by Bactopia.","title":"Documentation Overview"},{"location":"about/","text":"About Bactopia \u00b6 We (Tim Read and myself) initially released Staphopia in the summer of 2018, for the analysis of Staphylococcus aureus genomes. At conferences, or via email, the most common question about Staphopia that we received was: \"How can I use Staphopia for my bacteria of interest?\" . We too, have always been interested in this, because we tend to work with other bacteria and would like a standard pipeline for incoming sequencing projects. Given both the community and self interest, we developed Bactopia, a generic pipeline for the analysis of short-read bacterial sequences ! Side-note... we kept the \"-opia\" in the name to pay homage to Staphopia! While the philosophy behind Staphopia and Bactopia is mostly the same. The development of Bactopia from scratch has really allowed us to take what we learned from Staphopia and use it to our advantage. Bactopia has been developed with usability, portability, and speed in mind from the start. Bactopia uses Nextflow to manage the workflow. We have also prioritized software packages available from Bioconda (or other Anaconda channels ) to make installation as simple as possible for all users. This has also given us the opportunity to update our workflow with the latest methods. Workflow Overview \u00b6 INSERT IMAGE OF WORKFLOW","title":"About"},{"location":"about/#about-bactopia","text":"We (Tim Read and myself) initially released Staphopia in the summer of 2018, for the analysis of Staphylococcus aureus genomes. At conferences, or via email, the most common question about Staphopia that we received was: \"How can I use Staphopia for my bacteria of interest?\" . We too, have always been interested in this, because we tend to work with other bacteria and would like a standard pipeline for incoming sequencing projects. Given both the community and self interest, we developed Bactopia, a generic pipeline for the analysis of short-read bacterial sequences ! Side-note... we kept the \"-opia\" in the name to pay homage to Staphopia! While the philosophy behind Staphopia and Bactopia is mostly the same. The development of Bactopia from scratch has really allowed us to take what we learned from Staphopia and use it to our advantage. Bactopia has been developed with usability, portability, and speed in mind from the start. Bactopia uses Nextflow to manage the workflow. We have also prioritized software packages available from Bioconda (or other Anaconda channels ) to make installation as simple as possible for all users. This has also given us the opportunity to update our workflow with the latest methods.","title":"About Bactopia"},{"location":"about/#workflow-overview","text":"INSERT IMAGE OF WORKFLOW","title":"Workflow Overview"},{"location":"acknowledgements/","text":"Acknowledgements \u00b6 Bactopia is truly a case of \"standing upon the shoulders of giants\" . Nearly every component of Bactopia was created by others and made freely available to the public. I would like to personally extend my many thanks and gratitude to the authors of these software packages and public datasets. If you've made it this far, I owe you a beer \ud83c\udf7b (or coffee \u2615!) if we ever encounter one another in person. Really, thank you very much! Please Cite Datasets and Tools If you have used Bactopia in your work, please be sure to cite any datasets or tools you may have used. A citation link for each dataset/tool has been made available. Public Datasets \u00b6 Below is a list of public datasets (alphabetical) that could have potentially been included during the Build Datasets step. Ariba Reference Datasets \u00b6 These datasets are available using Ariba's getref function. You can learn more about this function at Ariba's Wiki . ARG-ANNOT Gupta, S. K. et al. ARG-ANNOT, a new bioinformatic tool to discover antibiotic resistance genes in bacterial genomes. Antimicrob. Agents Chemother. 58, 212\u2013220 (2014). CARD McArthur, A. G. et al. The comprehensive antibiotic resistance database. Antimicrob. Agents Chemother. 57, 3348\u20133357 (2013). MEGARes Lakin, S. M. et al. MEGARes: an antimicrobial resistance database for high throughput sequencing . Nucleic Acids Res. 45, D574\u2013D580 (2017). plasmidfinder Carattoli, A. et al. In silico detection and typing of plasmids using PlasmidFinder and plasmid multilocus sequence typing. Antimicrob. Agents Chemother. 58, 3895\u20133903 (2014). resfinder Zankari, E. et al. Identification of acquired antimicrobial resistance genes. J. Antimicrob. Chemother. 67, 2640\u20132644 (2012). SRST2 Inouye, M. et al. SRST2: Rapid genomic surveillance for public health and hospital microbiology labs. Genome Med. 6, 90 (2014). VFDB Chen, L., Zheng, D., Liu, B., Yang, J. & Jin, Q. VFDB 2016: hierarchical and refined dataset for big data analysis--10 years on. Nucleic Acids Res. 44, D694\u20137 (2016). VirulenceFinder Joensen, K. G. et al. Real-time whole-genome sequencing for routine typing, surveillance, and outbreak detection of verotoxigenic Escherichia coli. J. Clin. Microbiol. 52, 1501\u20131510 (2014). Minmer Datasets \u00b6 Mash Refseq (release 88) Sketch Ondov, B. D. et al. Mash Screen: High-throughput sequence containment estimation for genome discovery. bioRxiv 557314 (2019). Sourmash Genbank LCA Signature Titus Brown, C. & Irber, L. sourmash: a library for MinHash sketching of DNA. JOSS 1, 27 (2016). Everything Else \u00b6 NCBI RefSeq Database O\u2019Leary, N. A. et al. Reference sequence (RefSeq) database at NCBI: current status, taxonomic expansion, and functional annotation . Nucleic Acids Res. 44, D733\u201345 (2016). PLSDB - A plasmid database Galata, V., Fehlmann, T., Backes, C. & Keller, A. PLSDB: a resource of complete bacterial plasmids . Nucleic Acids Res. 47, D195\u2013D202 (2019). PubMLST.org Jolley, K. A., Bray, J. E. & Maiden, M. C. J. Open-access bacterial population genomics: BIGSdb software, the PubMLST.org website and their applications . Wellcome Open Res 3, 124 (2018). Programs Included In Bactopia \u00b6 Below is a list of tools (alphabetical) directly called by Bactopia. A link to software page as well as the citation (if available) has been included. AMRFinderPlus Find acquired antimicrobial resistance genes and some point mutations in protein or assembled nucleotide sequences. Feldgarden, M. et al. Validating the NCBI AMRFinder Tool and Resistance Gene Database Using Antimicrobial Resistance Genotype-Phenotype Correlations in a Collection of NARMS Isolates . Antimicrob. Agents Chemother. (2019) Ariba Antimicrobial Resistance Identification By Assembly Hunt, M. et al. ARIBA: rapid antimicrobial resistance genotyping directly from sequencing reads . Microb Genom 3, e000131 (2017). Assembly-Scan Generate basic stats for an assembly. Petit III, R.A. assembly-scan: generate basic stats for an assembly . BBTools BBTools is a suite of fast, multithreaded bioinformatics tools designed for analysis of DNA and RNA sequence data. Bushnell B. BBMap short read aligner, and other bioinformatic tools. Bedtools A powerful toolset for genome arithmetic. Quinlan, A. R. & Hall, I. M. BEDTools: a flexible suite of utilities for comparing genomic features . Bioinformatics 26, 841\u2013842 (2010). BLAST Basic Local Alignment Search Tool Camacho, C. et al. BLAST+: architecture and applications . BMC Bioinformatics 10, 421 (2009). BWA Burrow-Wheeler Aligner for short-read alignment Li, H. Aligning sequence reads, clone sequences and assembly contigs with BWA-MEM . arXiv [q-bio.GN] (2013). CD-Hit Accelerated for clustering the next-generation sequencing data Li, W. & Godzik, A. Cd-hit: a fast program for clustering and comparing large sets of protein or nucleotide sequences . Bioinformatics 22, 1658\u20131659 (2006). Fu, L., Niu, B., Zhu, Z., Wu, S. & Li, W. CD-HIT: accelerated for clustering the next-generation sequencing data . Bioinformatics 28, 3150\u20133152 (2012). FastQC A quality control analysis tool for high throughput sequencing data. Andrews, S. FastQC: a quality control tool for high throughput sequence data. ( http://www.bioinformatics.babraham.ac.uk/projects/fastqc ). Fastq-Scan Output FASTQ summary statistics in JSON format Petit III, R.A. fastq-scan: generate summary statistics of input FASTQ sequences. GNU Parallel GNU parallel is a shell tool for executing jobs in parallel using one or more computers. Tange, O. GNU Parallel 2018, March 2018 ISMapper IS mapping software Hawkey, J. et al. ISMapper: identifying transposase insertion sites in bacterial genomes from short read sequence data . BMC Genomics 16, 667 (2015). Lighter Fast and memory-efficient sequencing error corrector Song, L., Florea, L. and Langmead, B., Lighter: Fast and Memory-efficient Sequencing Error Correction without Counting . Genome Biol. 2014 Nov 15;15(11):509. Mash Fast genome and metagenome distance estimation using MinHash Ondov, B. D. et al. Mash: fast genome and metagenome distance estimation using MinHash . Genome Biol. 17, 132 (2016). Ondov, B. D. et al. Mash Screen: High-throughput sequence containment estimation for genome discovery . bioRxiv 557314 (2019). doi:10.1101/557314 McCortex De novo genome assembly and multisample variant calling Turner, I., Garimella, K. V., Iqbal, Z. & McVean, G. Integrating long-range connectivity information into de Bruijn graphs. Bioinformatics 34, 2556\u20132565 (2018). NCBI Genome Download Scripts to download genomes from the NCBI FTP servers Blin, K. NCBI Genome Download: Scripts to download genomes from the NCBI FTP servers Nextflow A DSL for data-driven computational pipelines. Di Tommaso, P., Chatzou, M., Floden, E.W., Barja, P.P., Palumbo, E., Notredame, C., 2017. Nextflow enables reproducible computational workflows. Nat. Biotechnol. 35, 316\u2013319. Pigz A parallel implementation of gzip for modern multi-processor, multi-core machines. Adler, Mark. pigz: A parallel implementation of gzip for modern multi-processor, multi-core machines. Jet Propulsion Laboratory (2015). Prokka Rapid prokaryotic genome annotation Seemann, T. Prokka: rapid prokaryotic genome annotation . Bioinformatics 30, 2068\u20132069 (2014). Samtools Tools (written in C using htslib) for manipulating next-generation sequencing data Li, H. et al. The Sequence Alignment/Map format and SAMtools . Bioinformatics 25, 2078\u20132079 (2009). Seqtk A fast and lightweight tool for processing sequences in the FASTA or FASTQ format. Li, H. Toolkit for processing sequences in FASTA/Q formats Shovill Faster assembly of Illumina reads Seemann, T. De novo assembly pipeline for Illumina paired reads Snippy Rapid haploid variant calling and core genome alignment Seemann, T. snippy: fast bacterial variant calling from NGS reads (2015) Sourmash Compute and compare MinHash signatures for DNA data sets. Titus Brown, C. & Irber, L. sourmash: a library for MinHash sketching of DNA . JOSS 1, 27 (2016). VCF-Annotator Add biological annotations to variants in a given VCF file. Petit III, R.A. VCF-Annotator: Add biological annotations to variants in a given VCF file. .","title":"Acknowledgements"},{"location":"acknowledgements/#acknowledgements","text":"Bactopia is truly a case of \"standing upon the shoulders of giants\" . Nearly every component of Bactopia was created by others and made freely available to the public. I would like to personally extend my many thanks and gratitude to the authors of these software packages and public datasets. If you've made it this far, I owe you a beer \ud83c\udf7b (or coffee \u2615!) if we ever encounter one another in person. Really, thank you very much! Please Cite Datasets and Tools If you have used Bactopia in your work, please be sure to cite any datasets or tools you may have used. A citation link for each dataset/tool has been made available.","title":"Acknowledgements"},{"location":"acknowledgements/#public-datasets","text":"Below is a list of public datasets (alphabetical) that could have potentially been included during the Build Datasets step.","title":"Public Datasets"},{"location":"acknowledgements/#ariba-reference-datasets","text":"These datasets are available using Ariba's getref function. You can learn more about this function at Ariba's Wiki . ARG-ANNOT Gupta, S. K. et al. ARG-ANNOT, a new bioinformatic tool to discover antibiotic resistance genes in bacterial genomes. Antimicrob. Agents Chemother. 58, 212\u2013220 (2014). CARD McArthur, A. G. et al. The comprehensive antibiotic resistance database. Antimicrob. Agents Chemother. 57, 3348\u20133357 (2013). MEGARes Lakin, S. M. et al. MEGARes: an antimicrobial resistance database for high throughput sequencing . Nucleic Acids Res. 45, D574\u2013D580 (2017). plasmidfinder Carattoli, A. et al. In silico detection and typing of plasmids using PlasmidFinder and plasmid multilocus sequence typing. Antimicrob. Agents Chemother. 58, 3895\u20133903 (2014). resfinder Zankari, E. et al. Identification of acquired antimicrobial resistance genes. J. Antimicrob. Chemother. 67, 2640\u20132644 (2012). SRST2 Inouye, M. et al. SRST2: Rapid genomic surveillance for public health and hospital microbiology labs. Genome Med. 6, 90 (2014). VFDB Chen, L., Zheng, D., Liu, B., Yang, J. & Jin, Q. VFDB 2016: hierarchical and refined dataset for big data analysis--10 years on. Nucleic Acids Res. 44, D694\u20137 (2016). VirulenceFinder Joensen, K. G. et al. Real-time whole-genome sequencing for routine typing, surveillance, and outbreak detection of verotoxigenic Escherichia coli. J. Clin. Microbiol. 52, 1501\u20131510 (2014).","title":"Ariba Reference Datasets"},{"location":"acknowledgements/#minmer-datasets","text":"Mash Refseq (release 88) Sketch Ondov, B. D. et al. Mash Screen: High-throughput sequence containment estimation for genome discovery. bioRxiv 557314 (2019). Sourmash Genbank LCA Signature Titus Brown, C. & Irber, L. sourmash: a library for MinHash sketching of DNA. JOSS 1, 27 (2016).","title":"Minmer Datasets"},{"location":"acknowledgements/#everything-else","text":"NCBI RefSeq Database O\u2019Leary, N. A. et al. Reference sequence (RefSeq) database at NCBI: current status, taxonomic expansion, and functional annotation . Nucleic Acids Res. 44, D733\u201345 (2016). PLSDB - A plasmid database Galata, V., Fehlmann, T., Backes, C. & Keller, A. PLSDB: a resource of complete bacterial plasmids . Nucleic Acids Res. 47, D195\u2013D202 (2019). PubMLST.org Jolley, K. A., Bray, J. E. & Maiden, M. C. J. Open-access bacterial population genomics: BIGSdb software, the PubMLST.org website and their applications . Wellcome Open Res 3, 124 (2018).","title":"Everything Else"},{"location":"acknowledgements/#programs-included-in-bactopia","text":"Below is a list of tools (alphabetical) directly called by Bactopia. A link to software page as well as the citation (if available) has been included. AMRFinderPlus Find acquired antimicrobial resistance genes and some point mutations in protein or assembled nucleotide sequences. Feldgarden, M. et al. Validating the NCBI AMRFinder Tool and Resistance Gene Database Using Antimicrobial Resistance Genotype-Phenotype Correlations in a Collection of NARMS Isolates . Antimicrob. Agents Chemother. (2019) Ariba Antimicrobial Resistance Identification By Assembly Hunt, M. et al. ARIBA: rapid antimicrobial resistance genotyping directly from sequencing reads . Microb Genom 3, e000131 (2017). Assembly-Scan Generate basic stats for an assembly. Petit III, R.A. assembly-scan: generate basic stats for an assembly . BBTools BBTools is a suite of fast, multithreaded bioinformatics tools designed for analysis of DNA and RNA sequence data. Bushnell B. BBMap short read aligner, and other bioinformatic tools. Bedtools A powerful toolset for genome arithmetic. Quinlan, A. R. & Hall, I. M. BEDTools: a flexible suite of utilities for comparing genomic features . Bioinformatics 26, 841\u2013842 (2010). BLAST Basic Local Alignment Search Tool Camacho, C. et al. BLAST+: architecture and applications . BMC Bioinformatics 10, 421 (2009). BWA Burrow-Wheeler Aligner for short-read alignment Li, H. Aligning sequence reads, clone sequences and assembly contigs with BWA-MEM . arXiv [q-bio.GN] (2013). CD-Hit Accelerated for clustering the next-generation sequencing data Li, W. & Godzik, A. Cd-hit: a fast program for clustering and comparing large sets of protein or nucleotide sequences . Bioinformatics 22, 1658\u20131659 (2006). Fu, L., Niu, B., Zhu, Z., Wu, S. & Li, W. CD-HIT: accelerated for clustering the next-generation sequencing data . Bioinformatics 28, 3150\u20133152 (2012). FastQC A quality control analysis tool for high throughput sequencing data. Andrews, S. FastQC: a quality control tool for high throughput sequence data. ( http://www.bioinformatics.babraham.ac.uk/projects/fastqc ). Fastq-Scan Output FASTQ summary statistics in JSON format Petit III, R.A. fastq-scan: generate summary statistics of input FASTQ sequences. GNU Parallel GNU parallel is a shell tool for executing jobs in parallel using one or more computers. Tange, O. GNU Parallel 2018, March 2018 ISMapper IS mapping software Hawkey, J. et al. ISMapper: identifying transposase insertion sites in bacterial genomes from short read sequence data . BMC Genomics 16, 667 (2015). Lighter Fast and memory-efficient sequencing error corrector Song, L., Florea, L. and Langmead, B., Lighter: Fast and Memory-efficient Sequencing Error Correction without Counting . Genome Biol. 2014 Nov 15;15(11):509. Mash Fast genome and metagenome distance estimation using MinHash Ondov, B. D. et al. Mash: fast genome and metagenome distance estimation using MinHash . Genome Biol. 17, 132 (2016). Ondov, B. D. et al. Mash Screen: High-throughput sequence containment estimation for genome discovery . bioRxiv 557314 (2019). doi:10.1101/557314 McCortex De novo genome assembly and multisample variant calling Turner, I., Garimella, K. V., Iqbal, Z. & McVean, G. Integrating long-range connectivity information into de Bruijn graphs. Bioinformatics 34, 2556\u20132565 (2018). NCBI Genome Download Scripts to download genomes from the NCBI FTP servers Blin, K. NCBI Genome Download: Scripts to download genomes from the NCBI FTP servers Nextflow A DSL for data-driven computational pipelines. Di Tommaso, P., Chatzou, M., Floden, E.W., Barja, P.P., Palumbo, E., Notredame, C., 2017. Nextflow enables reproducible computational workflows. Nat. Biotechnol. 35, 316\u2013319. Pigz A parallel implementation of gzip for modern multi-processor, multi-core machines. Adler, Mark. pigz: A parallel implementation of gzip for modern multi-processor, multi-core machines. Jet Propulsion Laboratory (2015). Prokka Rapid prokaryotic genome annotation Seemann, T. Prokka: rapid prokaryotic genome annotation . Bioinformatics 30, 2068\u20132069 (2014). Samtools Tools (written in C using htslib) for manipulating next-generation sequencing data Li, H. et al. The Sequence Alignment/Map format and SAMtools . Bioinformatics 25, 2078\u20132079 (2009). Seqtk A fast and lightweight tool for processing sequences in the FASTA or FASTQ format. Li, H. Toolkit for processing sequences in FASTA/Q formats Shovill Faster assembly of Illumina reads Seemann, T. De novo assembly pipeline for Illumina paired reads Snippy Rapid haploid variant calling and core genome alignment Seemann, T. snippy: fast bacterial variant calling from NGS reads (2015) Sourmash Compute and compare MinHash signatures for DNA data sets. Titus Brown, C. & Irber, L. sourmash: a library for MinHash sketching of DNA . JOSS 1, 27 (2016). VCF-Annotator Add biological annotations to variants in a given VCF file. Petit III, R.A. VCF-Annotator: Add biological annotations to variants in a given VCF file. .","title":"Programs Included In Bactopia"},{"location":"datasets/","text":"Build Datasets \u00b6 Bactopia can make use of many existing public datasets, as well as private datasets. The process of downloading, building, and (or) configuring these datasets for Bactopia has been automated. Highly recommended to complete this step! This step is completely optional, but it is highly recommended that you do not. By skipping this step of setting up public datasets, Bactopia will be limited to analyses like quality control, assembly, and 31-mer counting. Included Datasets \u00b6 Some datasets included are applicable to all bacterial species and some are specific to a bacterial species. If specified at runtime, Bactopia will recognize the datasets and execute the appropriate analyses. General \u00b6 Ariba's getref Reference Datasets Allows reference datasets (resistance, virulence, and plamids) to be automatically downloaded and configured for usage by Ariba RefSeq Mash Sketch ~100,000 genomes and plasmids from NCBI RefSeq, used to give an idea of what is your sequencing data (e.g. Are the sequences what you expected?) GenBank Sourmash Signatures ~87,000 microbial genomes (includes viral and fungal) from NCBI GenBank, also gives an idea of what is your sequencing data. PLSDB Mash Sketch & BLAST Includes meta data/annotations, Mash sketches, and BLAST database files of all plasmids stored in PLSDB. Species Specific \u00b6 PubMLST.org MLST Schemas Multi-locus sequence typing (MLST) allelic profiles and seqeunces for a many different bacterial species (and even a few eukaryotes!). Clustered RefSeq Proteins For the given bacterial species, completed RefSeq genomes are downloaded and then the proteins are clustered and formatted for usage with Prokka. Minmer Sketch of RefSeq Genomes Using the completed genomes downloaded for clustering proteins a Mash sketch and Sourmash signature is created for these genomes. These sketches can then be used for automatic selection of reference genomes for variant calling. Optional User Populated Folders A few folders for things such as calling variants, insertion sequences and primers are created that the user can manually populate. More information is available below! Setting Up \u00b6 Included in Bactopia is the setup-datasets.py script (located in the bin folder) to automate the process of downloading and/or building these datasets. Quick Start \u00b6 bactopia datasets datasets/ This will set up Ariba datasets ( card and vfdb_core ), RefSeq Mash sketch, GenBank Sourmash Signatures, and PLSDB in the newly created datasets folder. A Single Bacterial Species \u00b6 bactopia datasets datasets/ --species \"Haemophilus influenzae\" --include_genus Multiple Bacterial Species \u00b6 You can also set up datasets for multiple bacterial species at a time. There are two options to do so. Comma-Separated \u00b6 At runtime, you can separate the the different species bactopia datasets datasets/ --species \"Haemophilus influenzae,Staphylococcus aureus\" --include_genus Text File \u00b6 In order to do so, you will need to create a text file where each line is the name of a species to set up. For example, you could create a species.txt file and include the following species in it. Haemophilus influenzae Staphylococcus aureus Mycobacterium tuberculosis The new command becomes: bactopia datasets datasets/ --species species.txt --include_genus This will setup the MLST schema (if available) and a protein cluster FASTA file for each species in species.txt . Usage \u00b6 usage: setup-datasets [-h] [--ariba STR] [--species STR] [--skip_prokka] [--include_genus] [--identity FLOAT] [--overlap FLOAT] [--max_memory INT] [--fast_cluster] [--skip_minmer] [--skip_plsdb] [--cpus INT] [--clear_cache] [--force] [--force_ariba] [--force_mlst] [--force_prokka] [--force_minmer] [--force_plsdb] [--keep_files] [--list_datasets] [--depends] [--version] [--verbose] [--silent] OUTPUT_DIRECTORY setup-datasets - Setup public datasets for Bactopia positional arguments: OUTPUT_DIRECTORY Directory to write output. optional arguments: -h, --help show this help message and exit Ariba Reference Datasets: --ariba STR Setup Ariba datasets for a given reference or a list of references in a text file. (Default: card,vfdb_core) Bacterial Species: --species STR Download available (cg)MLST schemas and completed genomes for a given species or a list of species in a text file. Custom Prokka Protein FASTA: --skip_prokka Skip creation of a Prokka formatted fasta for each species --include_genus Include all genus members in the Prokka proteins FASTA --identity FLOAT CD-HIT (-c) sequence identity threshold. (Default: 0.9) --overlap FLOAT CD-HIT (-s) length difference cutoff. (Default: 0.8) --max_memory INT CD-HIT (-M) memory limit (in MB). (Default: unlimited --fast_cluster Use CD-HIT's (-g 0) fast clustering algorithm, instead of the accurate but slow algorithm. Minmer Datasets: --skip_minmer Skip download of pre-computed minmer datasets (mash, sourmash) PLSDB (Plasmid) BLAST/Sketch: --skip_plsdb Skip download of pre-computed PLSDB datbases (blast, mash) Helpful Options: --cpus INT Number of cpus to use. (Default: 1) --clear_cache Remove any existing cache. --force Forcibly overwrite existing datasets. --force_ariba Forcibly overwrite existing Ariba datasets. --force_mlst Forcibly overwrite existing MLST datasets. --force_prokka Forcibly overwrite existing Prokka datasets. --force_minmer Forcibly overwrite existing minmer datasets. --force_plsdb Forcibly overwrite existing PLSDB datasets. --keep_files Keep all downloaded and intermediate files. --list_datasets List Ariba reference datasets and (cg)MLST schemas available for setup. --depends Verify dependencies are installed. Adjust Verbosity: --version show program's version number and exit --verbose Print debug related text. --silent Only critical errors will be printed. example usage: setup-public-datasets.py outdir setup-public-datasets.py outdir --ariba 'card' setup-public-datasets.py outdir --species 'Staphylococcus aureus' --include_genus Useful Parameters \u00b6 --clear_cache \u00b6 To determine which MLST schemas are available, PubMLST.org is queryied. To prevent a query every run, a list of available schemas is cached to $HOME/.bactopia/datasets.json . The cache expires after 15 days, but in case a new species has been made available --clear_cache will force a requery of PubMLST.org. --cpus \u00b6 Increasing --cpus (it defaults to 1) is useful for speeding up the download and clustering steps. --force* \u00b6 If a dataset exists, it will only be overwritten if one of the --force parameters are used. --include_genus \u00b6 Completed RefSeq genomes are downloaded for a given species to be used for protein clustering. --include_genus will also download completed RefSeq genomes for each genus member. --keep_files \u00b6 Many intermediate files are downloaded/created (e.g. completed genomes) and deleted during the building process, use --keep_files to retain these files. Tweaking CD-HIT \u00b6 There are parameters ( --identity , --overlap , --max_memory , and --fast_cluster ) to tweak CD-HIT if you find it necessary. Please keep in mind, the only goal of the protein clustering step is to help speed up Prokka, by providing a decent set of proteins to annotate against first. User Populated Folders \u00b6 Built into the Bactopia dataset structure are folders that you, the user, can populate for species specific analysis. These could include specific genes you want BLASTed against your samples or a specific reference you want all your samples mapped to and variants called. Directory Structure \u00b6 When a species specific dataset is created, a folder named optional is also created. For example, a S. aureus specific dataset will have the following directory structure: datasets/species-specific/staphylococcus-aureus/ \u251c\u2500\u2500 annotation \u251c\u2500\u2500 minmer \u251c\u2500\u2500 mlst \u2514\u2500\u2500 optional \u251c\u2500\u2500 blast \u2502 \u251c\u2500\u2500 genes \u2502 \u251c\u2500\u2500 primers \u2502 \u2514\u2500\u2500 proteins \u251c\u2500\u2500 insertion-sequences \u251c\u2500\u2500 mapping-sequences \u2514\u2500\u2500 reference-genomes Within the optional folder are folders that you can added your own data up interest to. BLAST \u00b6 datasets/species-specific/staphylococcus-aureus/ \u2514\u2500\u2500 optional \u2514\u2500\u2500 blast \u251c\u2500\u2500 genes \u251c\u2500\u2500 primers \u2514\u2500\u2500 proteins In the blast directory there are three more directories! The genes folder is where you can place gene seqeunces (nucleotides) in FASTA format to query against assemblies using blastn . The primers folder is where you can place primer sequences (nucleotides) in FASTA format to query against assemblies using blastn , but with primer-specific parameters and cut-offs. Finally, the proteins (as you probably guessed!) is where you can place protein sequnces (amino acids) in FASTA format to query against assemblies using blastp . Insertion Sequences \u00b6 datasets/species-specific/staphylococcus-aureus/ \u2514\u2500\u2500 optional \u2514\u2500\u2500 insertion-sequences In the insertion-sequences directory you can place FASTA files of insertion seqeunces you would like searched for using ISMapper . Mapping \u00b6 datasets/species-specific/staphylococcus-aureus/ \u2514\u2500\u2500 optional \u2514\u2500\u2500 mapping-sequences In the mapping-sequences directory you can place FASTA files of any nucleotide sequence you would like FASTQ reads to be mapped against using BWA . This can be useful if you are interested if whether a certain region or gene is covered or not. Reference Genomes \u00b6 datasets/species-specific/staphylococcus-aureus/ \u2514\u2500\u2500 optional \u2514\u2500\u2500 reference-genomes In the reference-genomes directory you can put a GenBank (preferred!) or FASTA file of a reference genome you would like variants to be called against using Snippy .","title":"Build Datasets"},{"location":"datasets/#build-datasets","text":"Bactopia can make use of many existing public datasets, as well as private datasets. The process of downloading, building, and (or) configuring these datasets for Bactopia has been automated. Highly recommended to complete this step! This step is completely optional, but it is highly recommended that you do not. By skipping this step of setting up public datasets, Bactopia will be limited to analyses like quality control, assembly, and 31-mer counting.","title":"Build Datasets"},{"location":"datasets/#included-datasets","text":"Some datasets included are applicable to all bacterial species and some are specific to a bacterial species. If specified at runtime, Bactopia will recognize the datasets and execute the appropriate analyses.","title":"Included Datasets"},{"location":"datasets/#general","text":"Ariba's getref Reference Datasets Allows reference datasets (resistance, virulence, and plamids) to be automatically downloaded and configured for usage by Ariba RefSeq Mash Sketch ~100,000 genomes and plasmids from NCBI RefSeq, used to give an idea of what is your sequencing data (e.g. Are the sequences what you expected?) GenBank Sourmash Signatures ~87,000 microbial genomes (includes viral and fungal) from NCBI GenBank, also gives an idea of what is your sequencing data. PLSDB Mash Sketch & BLAST Includes meta data/annotations, Mash sketches, and BLAST database files of all plasmids stored in PLSDB.","title":"General"},{"location":"datasets/#species-specific","text":"PubMLST.org MLST Schemas Multi-locus sequence typing (MLST) allelic profiles and seqeunces for a many different bacterial species (and even a few eukaryotes!). Clustered RefSeq Proteins For the given bacterial species, completed RefSeq genomes are downloaded and then the proteins are clustered and formatted for usage with Prokka. Minmer Sketch of RefSeq Genomes Using the completed genomes downloaded for clustering proteins a Mash sketch and Sourmash signature is created for these genomes. These sketches can then be used for automatic selection of reference genomes for variant calling. Optional User Populated Folders A few folders for things such as calling variants, insertion sequences and primers are created that the user can manually populate. More information is available below!","title":"Species Specific"},{"location":"datasets/#setting-up","text":"Included in Bactopia is the setup-datasets.py script (located in the bin folder) to automate the process of downloading and/or building these datasets.","title":"Setting Up"},{"location":"datasets/#quick-start","text":"bactopia datasets datasets/ This will set up Ariba datasets ( card and vfdb_core ), RefSeq Mash sketch, GenBank Sourmash Signatures, and PLSDB in the newly created datasets folder.","title":"Quick Start"},{"location":"datasets/#a-single-bacterial-species","text":"bactopia datasets datasets/ --species \"Haemophilus influenzae\" --include_genus","title":"A Single Bacterial Species"},{"location":"datasets/#multiple-bacterial-species","text":"You can also set up datasets for multiple bacterial species at a time. There are two options to do so.","title":"Multiple Bacterial Species"},{"location":"datasets/#comma-separated","text":"At runtime, you can separate the the different species bactopia datasets datasets/ --species \"Haemophilus influenzae,Staphylococcus aureus\" --include_genus","title":"Comma-Separated"},{"location":"datasets/#text-file","text":"In order to do so, you will need to create a text file where each line is the name of a species to set up. For example, you could create a species.txt file and include the following species in it. Haemophilus influenzae Staphylococcus aureus Mycobacterium tuberculosis The new command becomes: bactopia datasets datasets/ --species species.txt --include_genus This will setup the MLST schema (if available) and a protein cluster FASTA file for each species in species.txt .","title":"Text File"},{"location":"datasets/#usage","text":"usage: setup-datasets [-h] [--ariba STR] [--species STR] [--skip_prokka] [--include_genus] [--identity FLOAT] [--overlap FLOAT] [--max_memory INT] [--fast_cluster] [--skip_minmer] [--skip_plsdb] [--cpus INT] [--clear_cache] [--force] [--force_ariba] [--force_mlst] [--force_prokka] [--force_minmer] [--force_plsdb] [--keep_files] [--list_datasets] [--depends] [--version] [--verbose] [--silent] OUTPUT_DIRECTORY setup-datasets - Setup public datasets for Bactopia positional arguments: OUTPUT_DIRECTORY Directory to write output. optional arguments: -h, --help show this help message and exit Ariba Reference Datasets: --ariba STR Setup Ariba datasets for a given reference or a list of references in a text file. (Default: card,vfdb_core) Bacterial Species: --species STR Download available (cg)MLST schemas and completed genomes for a given species or a list of species in a text file. Custom Prokka Protein FASTA: --skip_prokka Skip creation of a Prokka formatted fasta for each species --include_genus Include all genus members in the Prokka proteins FASTA --identity FLOAT CD-HIT (-c) sequence identity threshold. (Default: 0.9) --overlap FLOAT CD-HIT (-s) length difference cutoff. (Default: 0.8) --max_memory INT CD-HIT (-M) memory limit (in MB). (Default: unlimited --fast_cluster Use CD-HIT's (-g 0) fast clustering algorithm, instead of the accurate but slow algorithm. Minmer Datasets: --skip_minmer Skip download of pre-computed minmer datasets (mash, sourmash) PLSDB (Plasmid) BLAST/Sketch: --skip_plsdb Skip download of pre-computed PLSDB datbases (blast, mash) Helpful Options: --cpus INT Number of cpus to use. (Default: 1) --clear_cache Remove any existing cache. --force Forcibly overwrite existing datasets. --force_ariba Forcibly overwrite existing Ariba datasets. --force_mlst Forcibly overwrite existing MLST datasets. --force_prokka Forcibly overwrite existing Prokka datasets. --force_minmer Forcibly overwrite existing minmer datasets. --force_plsdb Forcibly overwrite existing PLSDB datasets. --keep_files Keep all downloaded and intermediate files. --list_datasets List Ariba reference datasets and (cg)MLST schemas available for setup. --depends Verify dependencies are installed. Adjust Verbosity: --version show program's version number and exit --verbose Print debug related text. --silent Only critical errors will be printed. example usage: setup-public-datasets.py outdir setup-public-datasets.py outdir --ariba 'card' setup-public-datasets.py outdir --species 'Staphylococcus aureus' --include_genus","title":"Usage"},{"location":"datasets/#useful-parameters","text":"","title":"Useful Parameters"},{"location":"datasets/#-clear_cache","text":"To determine which MLST schemas are available, PubMLST.org is queryied. To prevent a query every run, a list of available schemas is cached to $HOME/.bactopia/datasets.json . The cache expires after 15 days, but in case a new species has been made available --clear_cache will force a requery of PubMLST.org.","title":"--clear_cache"},{"location":"datasets/#-cpus","text":"Increasing --cpus (it defaults to 1) is useful for speeding up the download and clustering steps.","title":"--cpus"},{"location":"datasets/#-force","text":"If a dataset exists, it will only be overwritten if one of the --force parameters are used.","title":"--force*"},{"location":"datasets/#-include_genus","text":"Completed RefSeq genomes are downloaded for a given species to be used for protein clustering. --include_genus will also download completed RefSeq genomes for each genus member.","title":"--include_genus"},{"location":"datasets/#-keep_files","text":"Many intermediate files are downloaded/created (e.g. completed genomes) and deleted during the building process, use --keep_files to retain these files.","title":"--keep_files"},{"location":"datasets/#tweaking-cd-hit","text":"There are parameters ( --identity , --overlap , --max_memory , and --fast_cluster ) to tweak CD-HIT if you find it necessary. Please keep in mind, the only goal of the protein clustering step is to help speed up Prokka, by providing a decent set of proteins to annotate against first.","title":"Tweaking CD-HIT"},{"location":"datasets/#user-populated-folders","text":"Built into the Bactopia dataset structure are folders that you, the user, can populate for species specific analysis. These could include specific genes you want BLASTed against your samples or a specific reference you want all your samples mapped to and variants called.","title":"User Populated Folders"},{"location":"datasets/#directory-structure","text":"When a species specific dataset is created, a folder named optional is also created. For example, a S. aureus specific dataset will have the following directory structure: datasets/species-specific/staphylococcus-aureus/ \u251c\u2500\u2500 annotation \u251c\u2500\u2500 minmer \u251c\u2500\u2500 mlst \u2514\u2500\u2500 optional \u251c\u2500\u2500 blast \u2502 \u251c\u2500\u2500 genes \u2502 \u251c\u2500\u2500 primers \u2502 \u2514\u2500\u2500 proteins \u251c\u2500\u2500 insertion-sequences \u251c\u2500\u2500 mapping-sequences \u2514\u2500\u2500 reference-genomes Within the optional folder are folders that you can added your own data up interest to.","title":"Directory Structure"},{"location":"datasets/#blast","text":"datasets/species-specific/staphylococcus-aureus/ \u2514\u2500\u2500 optional \u2514\u2500\u2500 blast \u251c\u2500\u2500 genes \u251c\u2500\u2500 primers \u2514\u2500\u2500 proteins In the blast directory there are three more directories! The genes folder is where you can place gene seqeunces (nucleotides) in FASTA format to query against assemblies using blastn . The primers folder is where you can place primer sequences (nucleotides) in FASTA format to query against assemblies using blastn , but with primer-specific parameters and cut-offs. Finally, the proteins (as you probably guessed!) is where you can place protein sequnces (amino acids) in FASTA format to query against assemblies using blastp .","title":"BLAST"},{"location":"datasets/#insertion-sequences","text":"datasets/species-specific/staphylococcus-aureus/ \u2514\u2500\u2500 optional \u2514\u2500\u2500 insertion-sequences In the insertion-sequences directory you can place FASTA files of insertion seqeunces you would like searched for using ISMapper .","title":"Insertion Sequences"},{"location":"datasets/#mapping","text":"datasets/species-specific/staphylococcus-aureus/ \u2514\u2500\u2500 optional \u2514\u2500\u2500 mapping-sequences In the mapping-sequences directory you can place FASTA files of any nucleotide sequence you would like FASTQ reads to be mapped against using BWA . This can be useful if you are interested if whether a certain region or gene is covered or not.","title":"Mapping"},{"location":"datasets/#reference-genomes","text":"datasets/species-specific/staphylococcus-aureus/ \u2514\u2500\u2500 optional \u2514\u2500\u2500 reference-genomes In the reference-genomes directory you can put a GenBank (preferred!) or FASTA file of a reference genome you would like variants to be called against using Snippy .","title":"Reference Genomes"},{"location":"examples/","text":"Examples \u00b6 Example Use Cases of Bactopia Replicating Staphopia \u00b6","title":"Examples"},{"location":"examples/#examples","text":"Example Use Cases of Bactopia","title":"Examples"},{"location":"examples/#replicating-staphopia","text":"","title":"Replicating Staphopia"},{"location":"faq/","text":"FAQ \u00b6 How do I ...","title":"FAQ"},{"location":"faq/#faq","text":"How do I ...","title":"FAQ"},{"location":"installation/","text":"Installation \u00b6 Bactopia has a a lot of tools built into its workflow. As you can imagine, all these tools lead to numerous dependencies, and navigating dependencies can often turn into a very frustrating process. With this in mind, from the onset Bactopia was developed to only include programs that are installable using Conda . Conda is an open source package management system and environment management system that runs on Windows, macOS and Linux. In other words, it makes it super easy to get the tools you need installed! The official Conda documentation is a good starting point for getting started with Conda. Bactopia has been tested using the Miniconda installer , but the Anaconda installer should work the same. A Docker container based on the Bioconda install is also available. Bioconda \u00b6 Once you have Conda all set up, you are ready to create an environment for Bactopia. To do so, you can use the following command: conda create -n bactopia -c conda-forge -c bioconda bactopia After a few minutes you will have a new conda environment suitably named bactopia . To activate this environment, you will can use the following command: conda activate bactopia And voil\u00e0, you are all set to get started processing your data! But first, it is highly recommended that you take the time to Build Datasets that Bactopia can take advantage of. Docker \u00b6 A Docker container, that is also Singularity compatible, has been created that is based off the Conda install. docker pull bactopia/bactopia","title":"Installation"},{"location":"installation/#installation","text":"Bactopia has a a lot of tools built into its workflow. As you can imagine, all these tools lead to numerous dependencies, and navigating dependencies can often turn into a very frustrating process. With this in mind, from the onset Bactopia was developed to only include programs that are installable using Conda . Conda is an open source package management system and environment management system that runs on Windows, macOS and Linux. In other words, it makes it super easy to get the tools you need installed! The official Conda documentation is a good starting point for getting started with Conda. Bactopia has been tested using the Miniconda installer , but the Anaconda installer should work the same. A Docker container based on the Bioconda install is also available.","title":"Installation"},{"location":"installation/#bioconda","text":"Once you have Conda all set up, you are ready to create an environment for Bactopia. To do so, you can use the following command: conda create -n bactopia -c conda-forge -c bioconda bactopia After a few minutes you will have a new conda environment suitably named bactopia . To activate this environment, you will can use the following command: conda activate bactopia And voil\u00e0, you are all set to get started processing your data! But first, it is highly recommended that you take the time to Build Datasets that Bactopia can take advantage of.","title":"Bioconda"},{"location":"installation/#docker","text":"A Docker container, that is also Singularity compatible, has been created that is based off the Conda install. docker pull bactopia/bactopia","title":"Docker"},{"location":"quick-start/","text":"Quick Start \u00b6 Here we go! No time to waste, let's get the ball rolling! Why are you still reading this?!? Go! Go! Go! Installation \u00b6 conda create -n bactopia -c conda-forge -c bioconda bactopia conda activate bactopia Build Dataset \u00b6 bactopia datasets datasets/ Run Bactopia! \u00b6 Single Sample \u00b6 Paired-End \u00b6 bactopia --R1 ${SAMPLE}_R1.fastq.gz --R2 ${SAMPLE}_R2.fastq.gz --sample ${SAMPLE} \\ --dataset datasets/ --outdir ${OUTDIR} Single-End \u00b6 bactopia --SE ${SAMPLE}.fastq.gz --sample ${SAMPLE} --dataset datasets/ --outdir ${OUTDIR} Multiple Samples \u00b6 fastqs-fofn directory-of-fastqs/ > fastqs.txt bactopia --fastqs fastqs.txt --dataset datasets --outdir ${OUTDIR}","title":"Quick Start"},{"location":"quick-start/#quick-start","text":"Here we go! No time to waste, let's get the ball rolling! Why are you still reading this?!? Go! Go! Go!","title":"Quick Start"},{"location":"quick-start/#installation","text":"conda create -n bactopia -c conda-forge -c bioconda bactopia conda activate bactopia","title":"Installation"},{"location":"quick-start/#build-dataset","text":"bactopia datasets datasets/","title":"Build Dataset"},{"location":"quick-start/#run-bactopia","text":"","title":"Run Bactopia!"},{"location":"quick-start/#single-sample","text":"","title":"Single Sample"},{"location":"quick-start/#paired-end","text":"bactopia --R1 ${SAMPLE}_R1.fastq.gz --R2 ${SAMPLE}_R2.fastq.gz --sample ${SAMPLE} \\ --dataset datasets/ --outdir ${OUTDIR}","title":"Paired-End"},{"location":"quick-start/#single-end","text":"bactopia --SE ${SAMPLE}.fastq.gz --sample ${SAMPLE} --dataset datasets/ --outdir ${OUTDIR}","title":"Single-End"},{"location":"quick-start/#multiple-samples","text":"fastqs-fofn directory-of-fastqs/ > fastqs.txt bactopia --fastqs fastqs.txt --dataset datasets --outdir ${OUTDIR}","title":"Multiple Samples"},{"location":"tutorial/","text":"You should now have a directory named datasets that has all the available datasets to be used by Bactopia.# Tutorial For this tutorial, we will attempt to replicate the Staphopia analysis pipeline with Bactopia. We will use S. aureus samples associated with cystic fibrosis lung infections that were recently published (details below, shameless self plug!) and are available from BioProject accession PRJNA480016 . Bernardy, Eryn E., et al. \"Whole-Genome Sequences of Staphylococcus aureus Isolates from Cystic Fibrosis Lung Infections.\" Microbiol Resour Announc 8.3 (2019): e01564-18. Overall the goal of the tutorial is to: Build datasets Acquire Staphopia datasets Use Bactopia to process: A sample from ENA Multiple samples from ENA Single local sample Multiple local samples using FOFN Bactopia Should Be Installed This tutorial assumes you have already installed Bactopia. If you have not, please check out how to at Installation . Build Datasets \u00b6 First let's create a directory to work in and activate our Bactopia environment. mkdir bactopia-tutorial cd bactopia-tutorial conda activate bactopia Now we are ready to build our datasets! bactopia datasets datasets/ --ariba \"vfdb_core,card\" \\ --species \"Staphylococcus aureus\" \\ --include_genus --cpus 4 Let's review what is happening here. datasets/ is where our datasets will be downloaded, processed and stored. --ariba \"vfdb_core,card\" says to download and setup the VFDB Core and the CARD databases to be used by Ariba. --species \"Staphylococcus aureus\" will download MLST schemas associated with S. aureus it will also download completed S. aureus genomes (RefSeq only) that are used to create a set of protein set for annotation, a Mash sketch for automatic variant calling to the nearest neighbor, and calulate genome size statistics. --include_genus will also download completed genomes from the Staphylococcus genus that will be used for the protein set. These completed genomes are not used for the sketch creation or genome size calculation. --cpus 4 use 4 cpus for downloading and the clustering step. Adjust this number according to your setup! Use CARD over MEGARes Staphopia v1 made use of MEGARes, for the purposes of this tutorial we are going to use the CARD database instead. If all goes well, the newly created datasets are now available in the folder datasets/ . We have now completed the dataset creation step! Pat yourself on the back! Next we'll supplement these datasets with some optional S. aureus specific datasets. Acquire Staphopia Datasets \u00b6 Staphopia includes a few optional datasets such as S. aureus N315 reference genome and SCCmec sequences (primers, proteins, full cassettes). We will acquire these files using the Bactopia Datasets GitHub repository. For this tutorial a Staphopia v1 branch has been created, which includes this optional dataset. Now let's clone the repository. git clone -b staphopia-v1 https://github.com/bactopia/bactopia-datasets.git Next we'll copy the files into our recently built datasets folder and delete the bactopia-datasets repository since we no longer need it. cp -r bactopia-datasets/species-specific/ datasets/ rm -rf bactopia-datasets/ ~Voil\u00e0! That should be it. You should now have the Staphopia v1 datasets included with your recentely built datasets (e.g. S. aureus protein clusters, RefSeq sketch, etc...) Running Bactopia \u00b6 OK! Get your servers started up! It is time to get processing! Samples on ENA \u00b6 Single Sample \u00b6 Let's start this by downloading a single sample from ENA, and processing it through Bactopia. bactopia --accession SRX4563634 \\ --dataset datasets/ \\ --species staphylococcus-aureus \\ --coverage 100 \\ --genome_size median \\ --max_cpus 8 \\ --cpus 2 \\ --outdir ena-single-sample So, what's happening here? --accession SRX4563634 is telling Bactopia to download FASTQs associated with Exeriment accession SRX4563634. --dataset datasets/ tells Bactopia your pre-built datasets are in the folder datasets . --species staphylococcus-aureus tells Bactopia, within the datasets folder, use the species specific dataset for S. aureus . --coverage 100 will limit the cleaned up FASTQ file to an estimated 100x coverage based on the genome size. --genome_size median tells Bactopia to use the median genome size of completed S. aureus genomes. The minimum, maximum, median, and mean genome sizes were calculated during the dataset building step. If a genome size was not given, it would have been estimated by Mash. --max_cpus 8 and --cpus 2 tells Bactopia to use a maximum of 8 cpus ( --max_cpus ) and each job within the workflow can use a maximum of 2 cpus ( --cpus ). So at most 8 jobs (using 1 cpu each) can run at a time, and a minimum of 4 jobs (using 2 cpus each). Adjust these parameters to fit your setup! --outdir ena-single-sample tells Bactopia to dump the results into the ena-single-sample folder. Please keep in mind, this will not stop Nextflow from creating files (.nextflow, trace.txt, etc...) and directories (work and .nextflow/) within your current directory. Once you launch this command, sit back, relax and watch the Nextflow give realtime updates for SRX4563634's analysis! The approximate completion time is ~15-30 minutes depending on the number of cpus given and download times from ENA. Once complete, the results from numerous tools available to you in ena-single-sample/SRX4563634/ . Multiple Samples \u00b6 Now we are going to have Bactopia download and process 5 samples from ENA. To do this we will need to create a text file with a list of Experiment accessions we want to process. printf \"SRX4563678\\nSRX4563679\\nSRX4563680\\nSRX4563681\\nSRX4563682\\n\" > accessions.txt accessions.txt will now have five Experiment accessions, a single one per line. Just like this: SRX4563678 SRX4563679 SRX4563680 SRX4563681 SRX4563682 To process these 5 samples, we will adjust our command used previously. bactopia --accessions accessions.txt \\ --dataset datasets/ \\ --species staphylococcus-aureus \\ --coverage 100 \\ --genome_size median \\ --max_cpus 8 \\ --cpus 2 \\ --outdir ena-multiple-samples Instead of --accession we are now using --accessions accession.txt which tells Bactopia to read accessions.txt , and for each Experiment accession download in from ENA and then process it. At this point, you might want to go for a walk or make yourself a coffee! This step has an approximate completion time of ~45-120 minutes , which again is fully dependent on the cpus used and the download times from ENA. Once this is complete, the results for all five samples will be found in the ena-multiple-samples directory. Each sample will have there own folder of results. Local Samples \u00b6 So for the local samples, we're going to recycle some of the samples we downloaded from ENA. First let's make a directory to put the FASTQs into: mkdir fastqs Now let's move some of the FASTQs from our ENA samples into this folder. cp ena-single-sample/SRX4563634/quality-control/SRX4563634*.fastq.gz fastqs/ cp ena-multiple-samples/SRX4563680/quality-control/SRX4563680*.fastq.gz fastqs/ cp ena-multiple-samples/SRX4563682/quality-control/SRX4563682*.fastq.gz fastqs/ Finally let's make one of these paired-end reads into a single-end read (don't tell on me!). cat fastqs/SRX4563634*.fastq.gz > fastqs/SRX4563634-SE.fastq.gz OK! Now we are ready to continue the tutorial! Single Sample \u00b6 Again we'll mostly be using the same parameters as previous, but with a few new ones. To process a single sample you can use the --R1 / --R2 (paired-end), --SE (single-end), and --sample parameters. Paired-End \u00b6 For paired-end reads you will want to use --R1 , --R2 , and --sample . For this paired-end example we'll use SRX4563634 again which we've copied to the fastqs folder. bactopia --R1 fastqs/SRX4563634_R1.fastq.gz \\ --R2 fastqs/SRX4563634_R2.fastq.gz \\ --sample SRX4563634 \\ --dataset datasets/ \\ --species staphylococcus-aureus \\ --coverage 100 \\ --genome_size median \\ --max_cpus 8 \\ --cpus 2 \\ --outdir local-single-sample Now Bactopia will recognize the --R1 and --R2 parameters as paired-end reads and process. The --sample is required and will be used for naming the output. Similar to the single ENA sample, the approximate completion time is ~15-30 minutes depending on the number of cpus given. Once complete, results can be found in local-single-sample/SRX4563634/ . Single-End \u00b6 In the case of Illumina reads, you're very unlikely to produce single-end reads, but they do exist in the wild (early days of Illumina!). Nevertheless, because single-end reads do exist, single-end support was built into Bactopia. To analyze single-end reads, the --SE parameter will replace --R1 and --R2 . bactopia --SE fastqs/SRX4563634-SE.fastq.gz \\ --sample SRX4563634-SE \\ --dataset datasets/ \\ --species staphylococcus-aureus \\ --coverage 100 \\ --genome_size median \\ --max_cpus 8 \\ --cpus 2 \\ --outdir local-single-sample Now SRX4563634-SE will be processed as a single-end sample. For single-end processing there are some paired-end only analyses (e.g. error correction, insertion sequences) that will be skipped. This leads to single-end samples being processed a little bit faster than pair-end samples. But, the approximate completion time is still ~15-30 minutes . Once complete, you'll the results from numerous tools available to you in local-single-sample/SRX4563634-SE/ . If you made it this far, you're almost done! Multiple Samples (FOFN) \u00b6 Here we go! The final way you can process samples in Bactopia! Bactopia allows you to give a text file describing the input samples. This file of file names (FOFN), contains sample names and location to associated FASTQs. The Bactopia FOFN format is described in detail at Basic Usage -> Multiple Samples . First we'll need to prepare a FOFN describing the FASTQ files in our fastqs folder. We can use bactopia prepare to do so: bactopia prepare fastqs/ > fastqs.txt This command will try to create a FOFN for you. For this turorial, the FASTQ names are pretty straight forward and should produce a correct FOFN (or at least it should! ... hopefully!). If that wasn't the case for you, there are ways to tweak bactopia prepare . Now we can use the --fastqs parameters to process samples in the FOFN. bactopia --fastqs fastqs.txt \\ --dataset datasets/ \\ --species staphylococcus-aureus \\ --coverage 100 \\ --genome_size median \\ --max_cpus 8 \\ --cpus 2 \\ --outdir local-multiple-samples We no longer need --R1 , --R2 , --SE , or --sample as the values for these parameters can be determined from the FOFN. Here it is, the final wait! This step has an approximate completion time of ~45-120 minutes . So, you will definitely want to go for a walk or make yourself a coffee! You've earned it! Once this is complete, the results for each sample (within their own folder) will be found in the local-multiple-samples directory. FOFN is more cpu efficient, making it faster The real benefit of using the FOFN method to process multiple samples is Nextflow's queue system will make better use of cpus. Processing multiple samples one at a time (via --R1 / --R2 or --SE ) will lead more instances of jobs waiting on other jobs to finish, during which cpus aren't being used. What's next? \u00b6 That should do it! Hopefully you have succeeded (yay! \ud83c\udf89) and would like to use Bactopia on your own data! If your ran into any issues, please let me know by submitting a GitHub Issue .","title":"Tutorial"},{"location":"tutorial/#build-datasets","text":"First let's create a directory to work in and activate our Bactopia environment. mkdir bactopia-tutorial cd bactopia-tutorial conda activate bactopia Now we are ready to build our datasets! bactopia datasets datasets/ --ariba \"vfdb_core,card\" \\ --species \"Staphylococcus aureus\" \\ --include_genus --cpus 4 Let's review what is happening here. datasets/ is where our datasets will be downloaded, processed and stored. --ariba \"vfdb_core,card\" says to download and setup the VFDB Core and the CARD databases to be used by Ariba. --species \"Staphylococcus aureus\" will download MLST schemas associated with S. aureus it will also download completed S. aureus genomes (RefSeq only) that are used to create a set of protein set for annotation, a Mash sketch for automatic variant calling to the nearest neighbor, and calulate genome size statistics. --include_genus will also download completed genomes from the Staphylococcus genus that will be used for the protein set. These completed genomes are not used for the sketch creation or genome size calculation. --cpus 4 use 4 cpus for downloading and the clustering step. Adjust this number according to your setup! Use CARD over MEGARes Staphopia v1 made use of MEGARes, for the purposes of this tutorial we are going to use the CARD database instead. If all goes well, the newly created datasets are now available in the folder datasets/ . We have now completed the dataset creation step! Pat yourself on the back! Next we'll supplement these datasets with some optional S. aureus specific datasets.","title":"Build Datasets"},{"location":"tutorial/#acquire-staphopia-datasets","text":"Staphopia includes a few optional datasets such as S. aureus N315 reference genome and SCCmec sequences (primers, proteins, full cassettes). We will acquire these files using the Bactopia Datasets GitHub repository. For this tutorial a Staphopia v1 branch has been created, which includes this optional dataset. Now let's clone the repository. git clone -b staphopia-v1 https://github.com/bactopia/bactopia-datasets.git Next we'll copy the files into our recently built datasets folder and delete the bactopia-datasets repository since we no longer need it. cp -r bactopia-datasets/species-specific/ datasets/ rm -rf bactopia-datasets/ ~Voil\u00e0! That should be it. You should now have the Staphopia v1 datasets included with your recentely built datasets (e.g. S. aureus protein clusters, RefSeq sketch, etc...)","title":"Acquire Staphopia Datasets"},{"location":"tutorial/#running-bactopia","text":"OK! Get your servers started up! It is time to get processing!","title":"Running Bactopia"},{"location":"tutorial/#samples-on-ena","text":"","title":"Samples on ENA"},{"location":"tutorial/#single-sample","text":"Let's start this by downloading a single sample from ENA, and processing it through Bactopia. bactopia --accession SRX4563634 \\ --dataset datasets/ \\ --species staphylococcus-aureus \\ --coverage 100 \\ --genome_size median \\ --max_cpus 8 \\ --cpus 2 \\ --outdir ena-single-sample So, what's happening here? --accession SRX4563634 is telling Bactopia to download FASTQs associated with Exeriment accession SRX4563634. --dataset datasets/ tells Bactopia your pre-built datasets are in the folder datasets . --species staphylococcus-aureus tells Bactopia, within the datasets folder, use the species specific dataset for S. aureus . --coverage 100 will limit the cleaned up FASTQ file to an estimated 100x coverage based on the genome size. --genome_size median tells Bactopia to use the median genome size of completed S. aureus genomes. The minimum, maximum, median, and mean genome sizes were calculated during the dataset building step. If a genome size was not given, it would have been estimated by Mash. --max_cpus 8 and --cpus 2 tells Bactopia to use a maximum of 8 cpus ( --max_cpus ) and each job within the workflow can use a maximum of 2 cpus ( --cpus ). So at most 8 jobs (using 1 cpu each) can run at a time, and a minimum of 4 jobs (using 2 cpus each). Adjust these parameters to fit your setup! --outdir ena-single-sample tells Bactopia to dump the results into the ena-single-sample folder. Please keep in mind, this will not stop Nextflow from creating files (.nextflow, trace.txt, etc...) and directories (work and .nextflow/) within your current directory. Once you launch this command, sit back, relax and watch the Nextflow give realtime updates for SRX4563634's analysis! The approximate completion time is ~15-30 minutes depending on the number of cpus given and download times from ENA. Once complete, the results from numerous tools available to you in ena-single-sample/SRX4563634/ .","title":"Single Sample"},{"location":"tutorial/#multiple-samples","text":"Now we are going to have Bactopia download and process 5 samples from ENA. To do this we will need to create a text file with a list of Experiment accessions we want to process. printf \"SRX4563678\\nSRX4563679\\nSRX4563680\\nSRX4563681\\nSRX4563682\\n\" > accessions.txt accessions.txt will now have five Experiment accessions, a single one per line. Just like this: SRX4563678 SRX4563679 SRX4563680 SRX4563681 SRX4563682 To process these 5 samples, we will adjust our command used previously. bactopia --accessions accessions.txt \\ --dataset datasets/ \\ --species staphylococcus-aureus \\ --coverage 100 \\ --genome_size median \\ --max_cpus 8 \\ --cpus 2 \\ --outdir ena-multiple-samples Instead of --accession we are now using --accessions accession.txt which tells Bactopia to read accessions.txt , and for each Experiment accession download in from ENA and then process it. At this point, you might want to go for a walk or make yourself a coffee! This step has an approximate completion time of ~45-120 minutes , which again is fully dependent on the cpus used and the download times from ENA. Once this is complete, the results for all five samples will be found in the ena-multiple-samples directory. Each sample will have there own folder of results.","title":"Multiple Samples"},{"location":"tutorial/#local-samples","text":"So for the local samples, we're going to recycle some of the samples we downloaded from ENA. First let's make a directory to put the FASTQs into: mkdir fastqs Now let's move some of the FASTQs from our ENA samples into this folder. cp ena-single-sample/SRX4563634/quality-control/SRX4563634*.fastq.gz fastqs/ cp ena-multiple-samples/SRX4563680/quality-control/SRX4563680*.fastq.gz fastqs/ cp ena-multiple-samples/SRX4563682/quality-control/SRX4563682*.fastq.gz fastqs/ Finally let's make one of these paired-end reads into a single-end read (don't tell on me!). cat fastqs/SRX4563634*.fastq.gz > fastqs/SRX4563634-SE.fastq.gz OK! Now we are ready to continue the tutorial!","title":"Local Samples"},{"location":"tutorial/#single-sample_1","text":"Again we'll mostly be using the same parameters as previous, but with a few new ones. To process a single sample you can use the --R1 / --R2 (paired-end), --SE (single-end), and --sample parameters.","title":"Single Sample"},{"location":"tutorial/#paired-end","text":"For paired-end reads you will want to use --R1 , --R2 , and --sample . For this paired-end example we'll use SRX4563634 again which we've copied to the fastqs folder. bactopia --R1 fastqs/SRX4563634_R1.fastq.gz \\ --R2 fastqs/SRX4563634_R2.fastq.gz \\ --sample SRX4563634 \\ --dataset datasets/ \\ --species staphylococcus-aureus \\ --coverage 100 \\ --genome_size median \\ --max_cpus 8 \\ --cpus 2 \\ --outdir local-single-sample Now Bactopia will recognize the --R1 and --R2 parameters as paired-end reads and process. The --sample is required and will be used for naming the output. Similar to the single ENA sample, the approximate completion time is ~15-30 minutes depending on the number of cpus given. Once complete, results can be found in local-single-sample/SRX4563634/ .","title":"Paired-End"},{"location":"tutorial/#single-end","text":"In the case of Illumina reads, you're very unlikely to produce single-end reads, but they do exist in the wild (early days of Illumina!). Nevertheless, because single-end reads do exist, single-end support was built into Bactopia. To analyze single-end reads, the --SE parameter will replace --R1 and --R2 . bactopia --SE fastqs/SRX4563634-SE.fastq.gz \\ --sample SRX4563634-SE \\ --dataset datasets/ \\ --species staphylococcus-aureus \\ --coverage 100 \\ --genome_size median \\ --max_cpus 8 \\ --cpus 2 \\ --outdir local-single-sample Now SRX4563634-SE will be processed as a single-end sample. For single-end processing there are some paired-end only analyses (e.g. error correction, insertion sequences) that will be skipped. This leads to single-end samples being processed a little bit faster than pair-end samples. But, the approximate completion time is still ~15-30 minutes . Once complete, you'll the results from numerous tools available to you in local-single-sample/SRX4563634-SE/ . If you made it this far, you're almost done!","title":"Single-End"},{"location":"tutorial/#multiple-samples-fofn","text":"Here we go! The final way you can process samples in Bactopia! Bactopia allows you to give a text file describing the input samples. This file of file names (FOFN), contains sample names and location to associated FASTQs. The Bactopia FOFN format is described in detail at Basic Usage -> Multiple Samples . First we'll need to prepare a FOFN describing the FASTQ files in our fastqs folder. We can use bactopia prepare to do so: bactopia prepare fastqs/ > fastqs.txt This command will try to create a FOFN for you. For this turorial, the FASTQ names are pretty straight forward and should produce a correct FOFN (or at least it should! ... hopefully!). If that wasn't the case for you, there are ways to tweak bactopia prepare . Now we can use the --fastqs parameters to process samples in the FOFN. bactopia --fastqs fastqs.txt \\ --dataset datasets/ \\ --species staphylococcus-aureus \\ --coverage 100 \\ --genome_size median \\ --max_cpus 8 \\ --cpus 2 \\ --outdir local-multiple-samples We no longer need --R1 , --R2 , --SE , or --sample as the values for these parameters can be determined from the FOFN. Here it is, the final wait! This step has an approximate completion time of ~45-120 minutes . So, you will definitely want to go for a walk or make yourself a coffee! You've earned it! Once this is complete, the results for each sample (within their own folder) will be found in the local-multiple-samples directory. FOFN is more cpu efficient, making it faster The real benefit of using the FOFN method to process multiple samples is Nextflow's queue system will make better use of cpus. Processing multiple samples one at a time (via --R1 / --R2 or --SE ) will lead more instances of jobs waiting on other jobs to finish, during which cpus aren't being used.","title":"Multiple Samples (FOFN)"},{"location":"tutorial/#whats-next","text":"That should do it! Hopefully you have succeeded (yay! \ud83c\udf89) and would like to use Bactopia on your own data! If your ran into any issues, please let me know by submitting a GitHub Issue .","title":"What's next?"},{"location":"usage-basic/","text":"Basic Usage For Bactopia \u00b6 Bactopia is a wrapper around many different tools. Each of these tools may (or may not) have there own configurable parameters for you to tweak. In order to facilitate getting started with Bactopia, this section has been limited to discussion of only a few parameters. However, if you are interested in the full list of configurable parameters in Bactopia, please check out the Complete Usage section. Usage \u00b6 bactopia Required Parameters: ### For Procesessing Multiple Samples --fastqs STR An input file containing the sample name and absolute paths to FASTQs to process ### For Processing A Single Sample --R1 STR First set of reads for paired end in compressed (gzip) FASTQ format --R2 STR Second set of reads for paired end in compressed (gzip) FASTQ format --SE STR Single end set of reads in compressed (gzip) FASTQ format --sample STR The name of the input sequences ### For Downloading from ENA --accessions An input file containing ENA/SRA experiement accessions to be processed --accession A single ENA/SRA Experiment accession to be processed Dataset Parameters: --datasets DIR The path to available datasets that have already been set up --species STR Determines which species-specific dataset to use for the input sequencing Optional Parameters: --coverage INT Reduce samples to a given coverage Default: 100x --genome_size INT Expected genome size (bp) for all samples Default: Mash Estimate --outdir DIR Directory to write results to Default . --max_memory INT The maximum amount of memory (Gb) allowed to a single process. Default: 32 Gb --max_cpus INT The maximum number of processors this workflow should have access to at any given moment Default: 1 --cpus INT Number of processors made available to a single process. If greater than \"--max_cpus\" it will be set equal to \"--max_cpus\" Default: 1 Useful Parameters: --available_datasets Print a list of available datasets found based on location given by \"--datasets\" --example_fastqs Print example of expected input for FASTQs file --check_fastqs Verify \"--fastqs\" produces the expected inputs --clean_cache Removes 'work' and '.nextflow' logs. Caution, if used, the Nextflow run cannot be resumed. --keep_all_files Keeps all analysis files created. By default, intermediate files are removed. This will not affect the ability to resume Nextflow runs, and only occurs at the end of the process. --version Print workflow version information --help Show this message and exit --help_all Show a complete list of adjustable parameters FASTQ Inputs \u00b6 Bactopia has multiple approaches to specify your input sequences. You can make use of your local FASTQs or download FASTQs from the European Nucleotide Archive (ENA) . Which approach really depends on what you need to achieve! The following sections describe methods to process single samples, multiple samples, downloading samples from the ENA. Local \u00b6 Single Sample \u00b6 When you only need to process a single sample at a time, Bactopia allows that! You only have to the sample name ( --sample ) and the whether the read set is paired-end ( --R1 and --R2 ) or a single-end ( --SE ). Use --R1, --R2 for Paired-End FASTQs bactopia --sample my-sample --R1 /path/to/my-sample_R1.fastq.gz --R2 /path/to/my-sample_R2.fastq.gz Use --SE for Single-End FASTQs bactopia --sample my-sample --SE /path/to/my-sample.fastq.gz Multiple Samples \u00b6 For multiple samples, you must create a file with information about the inputs, a file of filenames (FOFN). This file specifies sample names and location of FASTQs to be processed. Using this information, paired-end or single-end information can be extracted as well as naming output files. While this is an additional step for you, the user, it helps to avoid potential pattern matching errors. Most importantly, by taking this approach, you can process hundreds of samples in a single command. There is also the added benefit of knowing which FASTQs were analysed and their location at a later time! Use --fastqs for Multiple Samples bactopia --fastqs my-samples.txt The FOFN Format \u00b6 You can use the --example_fastqs to get an example of the expected structure for the input FASTQs FOFN. bactopia --example_fastqs N E X T F L O W ~ version 19.01.0 Launching `/home/rpetit/illumina-cleanup/bin/illumina-cleanup` [naughty_borg] - revision: 0416ba407c Printing example input for \"--fastqs\" sample r1 r2 test001 /path/to/fastqs/test_R1.fastq.gz /path/to/fastqs/test_R2.fastq.gz test002 /path/to/fastqs/test.fastq.gz The expected structure is a tab-delimited table with three columns: sample : A unique prefix, or unique name, to be used for naming output files r1 : If paired-end, the first pair of reads, else the single-end reads r2 : If paired-end, the second pair of reads These three columns are used as the header for the file. In other words, all input FOFNs require their first line to be: sample r1 r2 All lines after the header line, contain unique sample names and location(s) to associated FASTQ file(s). Absolute paths should be used to prevent any file not found errors due to the relative path changing. In the example above, two samples would be processed by Bactopia. Sample test001 has two FASTQs and would be processed as pair-end reads. While sample test002 only has a single FASTQ and would be processed as single-end reads. Generating A FOFN \u00b6 A script named prepare-fofn has been included to help aid (hopefully!) the process of creating a FOFN for your samples. This script will attempt to find FASTQ files in a given directory and output the expected FOFN format. It will also output any potential issues associated with the pattern matching. Verify accuracy of FOFN This is currently an experimental function. There are likely bugs to be ironed out. Please be sure to give the resulting FOFN a quick look over. Usage \u00b6 bactopia prepare [-h] [-e STR] [-s STR] [--pattern STR] [--version] STR bactopia prepare - Read a directory and prepare a FOFN of FASTQs positional arguments: STR Directory where FASTQ files are stored optional arguments: -h, --help show this help message and exit -e STR, --ext STR Extension of the FASTQs. Default: .fastq.gz -s STR, --sep STR Split FASTQ name on the last occurrence of the separator. Default: _ --pattern STR Glob pattern to match FASTQs. Default: *.fastq.gz --version show program's version number and exit Examples \u00b6 Here is an example using the default parameters. In the example, sample SRR00000 has more than 2 FASTQs matched to it, which is recognized as an error. bactopia prepare tests/dummy-fastqs/ sample r1 r2 SRR00000 /home/rpetit/projects/bactopia/bactopia/tests/dummy-fastqs/SRR00000.fastq.gz /home/rpetit/projects/bactopia/bactopia/tests/dummy-fastqs/SRR00000_1.fastq.gz /home/rpetit/projects/bactopia/bactopia/tests/dummy-fastqs/SRR00000_2.fastq.gz ERROR: \"SRR00000\" has more than two different FASTQ files, please check. After tweaking the --pattern parameter a little bit. The error is corrected and sample SRR00000 is properly recognized as a paired-end read set. bactopia prepare tests/dummy-fastqs/ --pattern *_[12].fastq.gz sample r1 r2 SRR00000 /home/rpetit/projects/bactopia/bactopia/tests/dummy-fastqs/SRR00000_1.fastq.gz /home/rpetit/projects/bactopia/bactopia/tests/dummy-fastqs/SRR00000_2.fastq.gz There are a number of ways to tweak the pattern. Just please be sure to give a quick look over of the resulting FOFN. Validating FOFN \u00b6 When a FOFN is given, the first thing Bactopia does is verify all FASTQ files are found. If everything checks out, each sample will then be processed, otherwise a list of samples with errors will be output to STDERR. If you would like to only validate your FOFN (and not run the full pipeline), you can use the --check_fastqs parameter. Without Errors \u00b6 bactopia --check_fastqs --fastqs example-data/good-fastqs.txt N E X T F L O W ~ version 19.01.0 Launching `/home/rpetit3/bactopia/bactopia` [astonishing_colden] - revision: 96c6a1a7ae Printing what would have been processed. Each line consists of an array of three elements: [SAMPLE_NAME, IS_SINGLE_END, [FASTQ_1, FASTQ_2]] Found: [test001, false, [/home/rpetit3/bactopia/tests/fastqs/test_R1.fastq.gz, /home/rpetit3/bactopia/tests/fastqs/test_R2.fastq.gz]] [test002, true, [/home/rpetit3/bactopia/tests/fastqs/test.fastq.gz]] Each sample has passed validation and is put into a three element array: sample - the name for this sample is_single_end - the reads are single-end (true) or paired-end (false) fastq_array - the fastqs associated with the sample This array is then automatically queued up for proccessing by Nextflow. With errors \u00b6 bactopia --check_fastqs --fastqs tests/data/bad-fastqs.txt N E X T F L O W ~ version 19.01.0 Launching `/home/rpetit3/bactopia/bactopia` [kickass_mestorf] - revision: 222a5ad8b1 LINE 4:ERROR: Please verify /home/rpetit3/bactopia/test/fastqs/test003_R1.fastq.gz exists, and try again LINE 5:ERROR: Please verify /home/rpetit3/bactopia/test/fastqs/test003_R1.fastq.gz exists, and try again LINE 5:ERROR: Please verify /home/rpetit3/bactopia/test/fastqs/test002_R2.fastq.gz exists, and try again Sample name \"test002\" is not unique, please revise sample names The header line (line 1) does not follow expected structure. Verify sample names are unique and/or FASTQ paths are correct See \"--example_fastqs\" for an example Exiting In the above example, there are mulitple errors. Lines 4 and 5 ( LINE 4:ERROR or LINE 5:ERROR ) suggest that based on the given paths the FASTQs do not exist. The sample name test002 has been used multiple times, and must be corrected. There is also an issue with the header line that must be looked into. European Nucleotide Archive \u00b6 There are a lot of publicly avilable sequences, and you might want to include some of those in your analysis! If that sounds like you, Bactopia has that built in for you! You can give a single Experiment accession ( --accession ) or a file where each line is a single Experiment accession ( --accessions ). Bactopia will then query ENA to determine Run accession(s) associated with the given Experiment accession and proceed download (from ENA) corresponding FASTQ files. After the download is completed, it will be processed through Bactopia. Use --accession for a Single Experiment Accession bactopia --accession SRX476958 Use --accessions for Multiple Experiment Accessions bactopia --accessions my-accessions.txt --max_cpus & --cpus \u00b6 When Nextflow executes, it uses all available cpus to queue up processes. As you might imagine, if you are on a single server with multiple users, this approach of using all cpus might annoy other users! (Whoops sorry!) To circumvent this feature, two parmeters have been included --max_cpus and --cpus . --max_cpus INT The maximum number of processors this workflow should have access to at any given moment Default: 1 --cpus INT Number of processors made available to a single process. If greater than \"--max_cpus\" it will be set equal to \"--max_cpus\" Default: 1 What --max_cpus does is specify to Nextflow the maximum number of cpus it is allowed to occupy at any given time. --cpus on the other hand, specifies how many cpus any given step (qc, assembly, annotation, etc...) can occupy. By default --max_cpus is set to 1 and if --cpus is set to a value greater than --max_cpus it will be set equal to --max_cpus . This appoach errs on the side of caution, by not occupying all cpus on the server without the user's consent! --clean_cache \u00b6 Bactopia will keep Nextflow's work cache even after successfully completing. While the cache is maintained Bactopia is resumable using the -resume parameter. This does however introduce a potentential storage overhead. The cache will contain multiple intermediate files (e.g. uncompressed FASTQs, BAMs, etc...) for each sample that was processed. In other words, it can get pretty large! If you would like to clean up the cache you can use --clean_cache . This will remove the cache only after a successful execution (e.g. everything thing finished without errors). This is accomplished by removing the work directory created by Nextflow. As you might have guessed, by using removing the cache, Bactopia will no longer be resumeable. At the end of the day, you can always decide to not use --clean_cache and manually remove the work directory when you feel it is safe! --keep_all_files \u00b6 In some processes, Bactopia will delete large intermediate files (e.g. multiple uncompressed FASTQs) only after a process successfully completes. Since this a per-process function, it does not affect Nextflow's ability to resume ( -resume )a workflow. You can deactivate this feature using --keep_all_files . Please, keep in mind the work directory is already large, this will make it 2-3 times larger.","title":"Basic Usage"},{"location":"usage-basic/#basic-usage-for-bactopia","text":"Bactopia is a wrapper around many different tools. Each of these tools may (or may not) have there own configurable parameters for you to tweak. In order to facilitate getting started with Bactopia, this section has been limited to discussion of only a few parameters. However, if you are interested in the full list of configurable parameters in Bactopia, please check out the Complete Usage section.","title":"Basic Usage For Bactopia"},{"location":"usage-basic/#usage","text":"bactopia Required Parameters: ### For Procesessing Multiple Samples --fastqs STR An input file containing the sample name and absolute paths to FASTQs to process ### For Processing A Single Sample --R1 STR First set of reads for paired end in compressed (gzip) FASTQ format --R2 STR Second set of reads for paired end in compressed (gzip) FASTQ format --SE STR Single end set of reads in compressed (gzip) FASTQ format --sample STR The name of the input sequences ### For Downloading from ENA --accessions An input file containing ENA/SRA experiement accessions to be processed --accession A single ENA/SRA Experiment accession to be processed Dataset Parameters: --datasets DIR The path to available datasets that have already been set up --species STR Determines which species-specific dataset to use for the input sequencing Optional Parameters: --coverage INT Reduce samples to a given coverage Default: 100x --genome_size INT Expected genome size (bp) for all samples Default: Mash Estimate --outdir DIR Directory to write results to Default . --max_memory INT The maximum amount of memory (Gb) allowed to a single process. Default: 32 Gb --max_cpus INT The maximum number of processors this workflow should have access to at any given moment Default: 1 --cpus INT Number of processors made available to a single process. If greater than \"--max_cpus\" it will be set equal to \"--max_cpus\" Default: 1 Useful Parameters: --available_datasets Print a list of available datasets found based on location given by \"--datasets\" --example_fastqs Print example of expected input for FASTQs file --check_fastqs Verify \"--fastqs\" produces the expected inputs --clean_cache Removes 'work' and '.nextflow' logs. Caution, if used, the Nextflow run cannot be resumed. --keep_all_files Keeps all analysis files created. By default, intermediate files are removed. This will not affect the ability to resume Nextflow runs, and only occurs at the end of the process. --version Print workflow version information --help Show this message and exit --help_all Show a complete list of adjustable parameters","title":"Usage"},{"location":"usage-basic/#fastq-inputs","text":"Bactopia has multiple approaches to specify your input sequences. You can make use of your local FASTQs or download FASTQs from the European Nucleotide Archive (ENA) . Which approach really depends on what you need to achieve! The following sections describe methods to process single samples, multiple samples, downloading samples from the ENA.","title":"FASTQ Inputs"},{"location":"usage-basic/#local","text":"","title":"Local"},{"location":"usage-basic/#single-sample","text":"When you only need to process a single sample at a time, Bactopia allows that! You only have to the sample name ( --sample ) and the whether the read set is paired-end ( --R1 and --R2 ) or a single-end ( --SE ). Use --R1, --R2 for Paired-End FASTQs bactopia --sample my-sample --R1 /path/to/my-sample_R1.fastq.gz --R2 /path/to/my-sample_R2.fastq.gz Use --SE for Single-End FASTQs bactopia --sample my-sample --SE /path/to/my-sample.fastq.gz","title":"Single Sample"},{"location":"usage-basic/#multiple-samples","text":"For multiple samples, you must create a file with information about the inputs, a file of filenames (FOFN). This file specifies sample names and location of FASTQs to be processed. Using this information, paired-end or single-end information can be extracted as well as naming output files. While this is an additional step for you, the user, it helps to avoid potential pattern matching errors. Most importantly, by taking this approach, you can process hundreds of samples in a single command. There is also the added benefit of knowing which FASTQs were analysed and their location at a later time! Use --fastqs for Multiple Samples bactopia --fastqs my-samples.txt","title":"Multiple Samples"},{"location":"usage-basic/#the-fofn-format","text":"You can use the --example_fastqs to get an example of the expected structure for the input FASTQs FOFN. bactopia --example_fastqs N E X T F L O W ~ version 19.01.0 Launching `/home/rpetit/illumina-cleanup/bin/illumina-cleanup` [naughty_borg] - revision: 0416ba407c Printing example input for \"--fastqs\" sample r1 r2 test001 /path/to/fastqs/test_R1.fastq.gz /path/to/fastqs/test_R2.fastq.gz test002 /path/to/fastqs/test.fastq.gz The expected structure is a tab-delimited table with three columns: sample : A unique prefix, or unique name, to be used for naming output files r1 : If paired-end, the first pair of reads, else the single-end reads r2 : If paired-end, the second pair of reads These three columns are used as the header for the file. In other words, all input FOFNs require their first line to be: sample r1 r2 All lines after the header line, contain unique sample names and location(s) to associated FASTQ file(s). Absolute paths should be used to prevent any file not found errors due to the relative path changing. In the example above, two samples would be processed by Bactopia. Sample test001 has two FASTQs and would be processed as pair-end reads. While sample test002 only has a single FASTQ and would be processed as single-end reads.","title":"The FOFN Format"},{"location":"usage-basic/#generating-a-fofn","text":"A script named prepare-fofn has been included to help aid (hopefully!) the process of creating a FOFN for your samples. This script will attempt to find FASTQ files in a given directory and output the expected FOFN format. It will also output any potential issues associated with the pattern matching. Verify accuracy of FOFN This is currently an experimental function. There are likely bugs to be ironed out. Please be sure to give the resulting FOFN a quick look over.","title":"Generating A FOFN"},{"location":"usage-basic/#usage_1","text":"bactopia prepare [-h] [-e STR] [-s STR] [--pattern STR] [--version] STR bactopia prepare - Read a directory and prepare a FOFN of FASTQs positional arguments: STR Directory where FASTQ files are stored optional arguments: -h, --help show this help message and exit -e STR, --ext STR Extension of the FASTQs. Default: .fastq.gz -s STR, --sep STR Split FASTQ name on the last occurrence of the separator. Default: _ --pattern STR Glob pattern to match FASTQs. Default: *.fastq.gz --version show program's version number and exit","title":"Usage"},{"location":"usage-basic/#examples","text":"Here is an example using the default parameters. In the example, sample SRR00000 has more than 2 FASTQs matched to it, which is recognized as an error. bactopia prepare tests/dummy-fastqs/ sample r1 r2 SRR00000 /home/rpetit/projects/bactopia/bactopia/tests/dummy-fastqs/SRR00000.fastq.gz /home/rpetit/projects/bactopia/bactopia/tests/dummy-fastqs/SRR00000_1.fastq.gz /home/rpetit/projects/bactopia/bactopia/tests/dummy-fastqs/SRR00000_2.fastq.gz ERROR: \"SRR00000\" has more than two different FASTQ files, please check. After tweaking the --pattern parameter a little bit. The error is corrected and sample SRR00000 is properly recognized as a paired-end read set. bactopia prepare tests/dummy-fastqs/ --pattern *_[12].fastq.gz sample r1 r2 SRR00000 /home/rpetit/projects/bactopia/bactopia/tests/dummy-fastqs/SRR00000_1.fastq.gz /home/rpetit/projects/bactopia/bactopia/tests/dummy-fastqs/SRR00000_2.fastq.gz There are a number of ways to tweak the pattern. Just please be sure to give a quick look over of the resulting FOFN.","title":"Examples"},{"location":"usage-basic/#validating-fofn","text":"When a FOFN is given, the first thing Bactopia does is verify all FASTQ files are found. If everything checks out, each sample will then be processed, otherwise a list of samples with errors will be output to STDERR. If you would like to only validate your FOFN (and not run the full pipeline), you can use the --check_fastqs parameter.","title":"Validating FOFN"},{"location":"usage-basic/#without-errors","text":"bactopia --check_fastqs --fastqs example-data/good-fastqs.txt N E X T F L O W ~ version 19.01.0 Launching `/home/rpetit3/bactopia/bactopia` [astonishing_colden] - revision: 96c6a1a7ae Printing what would have been processed. Each line consists of an array of three elements: [SAMPLE_NAME, IS_SINGLE_END, [FASTQ_1, FASTQ_2]] Found: [test001, false, [/home/rpetit3/bactopia/tests/fastqs/test_R1.fastq.gz, /home/rpetit3/bactopia/tests/fastqs/test_R2.fastq.gz]] [test002, true, [/home/rpetit3/bactopia/tests/fastqs/test.fastq.gz]] Each sample has passed validation and is put into a three element array: sample - the name for this sample is_single_end - the reads are single-end (true) or paired-end (false) fastq_array - the fastqs associated with the sample This array is then automatically queued up for proccessing by Nextflow.","title":"Without Errors"},{"location":"usage-basic/#with-errors","text":"bactopia --check_fastqs --fastqs tests/data/bad-fastqs.txt N E X T F L O W ~ version 19.01.0 Launching `/home/rpetit3/bactopia/bactopia` [kickass_mestorf] - revision: 222a5ad8b1 LINE 4:ERROR: Please verify /home/rpetit3/bactopia/test/fastqs/test003_R1.fastq.gz exists, and try again LINE 5:ERROR: Please verify /home/rpetit3/bactopia/test/fastqs/test003_R1.fastq.gz exists, and try again LINE 5:ERROR: Please verify /home/rpetit3/bactopia/test/fastqs/test002_R2.fastq.gz exists, and try again Sample name \"test002\" is not unique, please revise sample names The header line (line 1) does not follow expected structure. Verify sample names are unique and/or FASTQ paths are correct See \"--example_fastqs\" for an example Exiting In the above example, there are mulitple errors. Lines 4 and 5 ( LINE 4:ERROR or LINE 5:ERROR ) suggest that based on the given paths the FASTQs do not exist. The sample name test002 has been used multiple times, and must be corrected. There is also an issue with the header line that must be looked into.","title":"With errors"},{"location":"usage-basic/#european-nucleotide-archive","text":"There are a lot of publicly avilable sequences, and you might want to include some of those in your analysis! If that sounds like you, Bactopia has that built in for you! You can give a single Experiment accession ( --accession ) or a file where each line is a single Experiment accession ( --accessions ). Bactopia will then query ENA to determine Run accession(s) associated with the given Experiment accession and proceed download (from ENA) corresponding FASTQ files. After the download is completed, it will be processed through Bactopia. Use --accession for a Single Experiment Accession bactopia --accession SRX476958 Use --accessions for Multiple Experiment Accessions bactopia --accessions my-accessions.txt","title":"European Nucleotide Archive"},{"location":"usage-basic/#-max_cpus-cpus","text":"When Nextflow executes, it uses all available cpus to queue up processes. As you might imagine, if you are on a single server with multiple users, this approach of using all cpus might annoy other users! (Whoops sorry!) To circumvent this feature, two parmeters have been included --max_cpus and --cpus . --max_cpus INT The maximum number of processors this workflow should have access to at any given moment Default: 1 --cpus INT Number of processors made available to a single process. If greater than \"--max_cpus\" it will be set equal to \"--max_cpus\" Default: 1 What --max_cpus does is specify to Nextflow the maximum number of cpus it is allowed to occupy at any given time. --cpus on the other hand, specifies how many cpus any given step (qc, assembly, annotation, etc...) can occupy. By default --max_cpus is set to 1 and if --cpus is set to a value greater than --max_cpus it will be set equal to --max_cpus . This appoach errs on the side of caution, by not occupying all cpus on the server without the user's consent!","title":"--max_cpus &amp; --cpus"},{"location":"usage-basic/#-clean_cache","text":"Bactopia will keep Nextflow's work cache even after successfully completing. While the cache is maintained Bactopia is resumable using the -resume parameter. This does however introduce a potentential storage overhead. The cache will contain multiple intermediate files (e.g. uncompressed FASTQs, BAMs, etc...) for each sample that was processed. In other words, it can get pretty large! If you would like to clean up the cache you can use --clean_cache . This will remove the cache only after a successful execution (e.g. everything thing finished without errors). This is accomplished by removing the work directory created by Nextflow. As you might have guessed, by using removing the cache, Bactopia will no longer be resumeable. At the end of the day, you can always decide to not use --clean_cache and manually remove the work directory when you feel it is safe!","title":"--clean_cache"},{"location":"usage-basic/#-keep_all_files","text":"In some processes, Bactopia will delete large intermediate files (e.g. multiple uncompressed FASTQs) only after a process successfully completes. Since this a per-process function, it does not affect Nextflow's ability to resume ( -resume )a workflow. You can deactivate this feature using --keep_all_files . Please, keep in mind the work directory is already large, this will make it 2-3 times larger.","title":"--keep_all_files"},{"location":"usage-complete/","text":"Runtime Parameters \u00b6 Bactopia includes numerous (100+) configurable parameters. Basically for each step of the pipeline, you can modify the default parameters of a specific tool. Required \u00b6 The required parameters depends on how many samples are to be proccessed. You can learn more about which approach to take at Specifying Input FASTQs . ### For Procesessing Multiple Samples --fastqs STR An input file containing the sample name and absolute paths to FASTQs to process ### For Processing A Single Sample --R1 STR First set of reads for paired end in compressed (gzip) FASTQ format --R2 STR Second set of reads for paired end in compressed (gzip) FASTQ format --SE STR Single end set of reads in compressed (gzip) FASTQ format --sample STR The name of the input sequences ### For Downloading from ENA --accessions An input file containing ENA/SRA experiement accessions to be processed --accession A single ENA/SRA Experiment accession to be processed Dataset \u00b6 If you followed the steps in Build Datasets , you can use the following parameters to point Bactopia to you datasets. --datasets DIR The path to available public datasets that have already been set up --species STR Determines which species-specific dataset to use for the input sequencing Optional \u00b6 These optional parameters, while not required, will be quite useful (especially --max_cpus and --cpus !) to tweak. --coverage INT Reduce samples to a given coverage Default: 100x --genome_size INT Expected genome size (bp) for all samples Default: Mash Estimate --outdir DIR Directory to write results to Default . --max_memory INT The maximum amount of memory (Gb) allowed to a single process. Default: 32 Gb --max_cpus vs --cpus \u00b6 By default when Nextflow executes, it uses all available cpus to queue up processes. As you might imagine, if you are on a single server with multiple users, this approach of using all cpus might annoy other users! (Whoops sorry!) To circumvent this feature, two parmeters have been included --max_cpus and --cpus . --max_cpus INT The maximum number of processors this workflow should have access to at any given moment Default: 1 --cpus INT Number of processors made available to a single process. If greater than \"--max_cpus\" it will be set equal to \"--max_cpus\" Default: 1 What --max_cpus does is specify to Nextflow the maximum number of cpus it is allowed to occupy at any given time. --cpus on the other hand, specifies how many cpus any given step (qc, assembly, annotation, etc...) can occupy. By default --max_cpus is set to 1 and if --cpus is set to a value greater than --max_cpus it will be set equal to --max_cpus . This appoach errs on the side of caution, by not occupying all cpus on the server without the users consent! Helpers \u00b6 The following parameters are useful to test to test input parameters. --available_datasets Print a list of available datasets found based on location given by \"--datasets\" --example_fastqs Print example of expected input for FASTQs file --check_fastqs Verify \"--fastqs\" produces the expected inputs --clean_cache Removes 'work' and '.nextflow' logs. Caution, if used, the Nextflow run cannot be resumed. --keep_all_files Keeps all analysis files created. By default, intermediate files are removed. This will not affect the ability to resume Nextflow runs, and only occurs at the end of the process. --version Print workflow version information --help Show this message and exit --help_all Show a complete list of adjustable parameters --clean_cache \u00b6 Bactopia will keep Nextflow's work cache even after successfully completing. While the cache is maintained Bactopia is resumable using the -resume parameter. This does however introduce a potentential storage overhead. The cache will contain multiple intermediate files (e.g. uncompressed FASTQs, BAMs, etc...) for each sample that was processed. In other words, it can get pretty large! If you would like to clean up the cache you can use --clean_cache . This will remove the cache only after a successful execution (e.g. everything thing finished without errors). This is accomplished by removing the work directory created by Nextflow. As you might have guessed, by using removing the cache, Bactopia will no longer be resumeable. At the end of the day, you can always decide to not use --clean_cache and manually remove the work directory when you feel it is safe! --keep_all_files \u00b6 In some processes, Bactopia will delete large intermediate files (e.g. multiple uncompressed FASTQs) only after a process successfully completes. Since this a per-process function, it does not affect Nextflow's ability to resume ( -resume )a workflow. You can deactivate this feature using --keep_all_files . Please, keep in mind the work directory is already large, this will make it 2-3 times larger. Program Specific \u00b6 The remaining parameters are associated with specific programs. In the following sections, these parameters are grouped by which Nextflow process they are applicable to. The description and default values for these parameters were taken from the program to which they apply. It is important to note, not all of the available parameters for each and every program are available in Bactopia. If there is a parameter that was overlooked and should probably be included, please make a suggestion! Annotation \u00b6 --centre STR Sequencing centre ID Default: '' --addgenes Add 'gene' features for each 'CDS' feature --addmrna Add 'mRNA' features for each 'CDS' feature --rawproduct Do not clean up /product annotation --cdsrnaolap Allow [tr]RNA to overlap CDS --prokka_evalue STR Similarity e-value cut-off Default: 1e-09 --prokka_coverage INT Minimum coverage on query protein Default: 80 --norrna Don't run rRNA search --notrna Don't run tRNA search --rnammer Prefer RNAmmer over Barrnap for rRNA prediction Antimicrobial Resistance \u00b6 --update_amr Force amrfinder to update its database. --amr_ident_min Minimum identity for nucleotide hit (0..1). -1 means use a curated threshold if it exists and 0.9 otherwise Default: -1 --amr_coverage_min Minimum coverage of the reference protein (0..1) Default: 0.5 --amr_organism Taxonomy group: Campylobacter, Escherichia, Klebsiella Salmonella, Staphylococcus, Vibrio Default: '' --amr_translation_table NCBI genetic code for translated BLAST Default: 11 --amr_plus Add the plus genes to the report --amr_report_common Suppress proteins common to a taxonomy group Ariba \u00b6 --nucmer_min_id INT Minimum alignment identity (delta-filter -i) Default: 90 --nucmer_min_len INT Minimum alignment length (delta-filter -i) Default: 20 --nucmer_breaklen INT Value to use for -breaklen when running nucmer Default: 200 --assembly_cov INT Target read coverage when sampling reads for assembly Default: 50 --min_scaff_depth INT Minimum number of read pairs needed as evidence for scaffold link between two contigs Default: 10 --spades_options STR Extra options to pass to Spades assembler Default: null --assembled_threshold FLOAT (between 0 and 1) If proportion of gene assembled (regardless of into how many contigs) is at least this value then the flag gene_assembled is set Default: 0.95 --gene_nt_extend INT Max number of nucleotides to extend ends of gene matches to look for start/stop codons Default: 30 --unique_threshold FLOAT (between 0 and 1) If proportion of bases in gene assembled more than once is <= this value, then the flag unique_contig is set Default: 0.03 Assembly \u00b6 --shovill_ram INT Try to keep RAM usage below this many GB Default: 32 --assembler STR Assembler: megahit velvet skesa spades Default: skesa --min_contig_len INT Minimum contig length <0=AUTO> Default: 500 --min_contig_cov INT Minimum contig coverage <0=AUTO> Default: 2 --contig_namefmt STR Format of contig FASTA IDs in 'printf' style Default: contig%05d --shovill_opts STR Extra assembler options in quotes eg. spades: \"--untrusted-contigs locus.fna\" ... Default: '' --shovill_kmers STR K-mers to use <blank=AUTO> Default: '' --trim Enable adaptor trimming --nostitch Disable read stitching --nocorr Disable post-assembly correction BLAST \u00b6 --perc_identity INT Percent identity Default: 50 --qcov_hsp_perc INT Percent query coverage per hsp Default: 50 --max_target_seqs INT Maximum number of aligned sequences to keep Default: 2000 --outfmt STR BLAST alignment view options Default: '6 qseqid qlen qstart qend sseqid slen sstart send length evalue bitscore pident nident mismatch gaps qcovs qcovhsp' Counting 31mers \u00b6 --keep_singletons Keep all counted 31-mers Default: Filter out singletons Download FASTQ \u00b6 --aspera_speed STR Speed at which Aspera Connect will download. Default: 100M --max_retry INT Maximum times to retry downloads Default: 10 --use_ftp Only use FTP to download FASTQs from ENA Insertion Mapping \u00b6 --min_clip INT Minimum size for softclipped region to be extracted from initial mapping Default: 10 --max_clip INT Maximum size for softclipped regions to be included Default: 30 --cutoff INT Minimum depth for mapped region to be kept in bed file Default: 6 --novel_gap_size INT Distance in base pairs between left and right flanks to be called a novel hit Default: 15 --min_range FLOAT Minimum percent size of the gap to be called a known hit Default: 0.9 --max_range FLOAT Maximum percent size of the gap to be called a known hit Default: 1.1 --merging INT Value for merging left and right hits in bed files together to simply calculation of closest and intersecting regions Default: 100 --ismap_all Switch on all alignment reporting for bwa --ismap_minqual INT Mapping quality score for bwa Default: 30 Mapping \u00b6 --keep_unmapped_reads Keep unmapped reads, this does not affect variant calling. --bwa_mem_opts STR Extra BWA MEM options Default: '' --bwa_aln_opts STR Extra BWA ALN options Default: '' --bwa_samse_opts STR Extra BWA SAMSE options Default: '' --bwa_sampe_opts STR Extra BWA SAMPE options Default: '' --bwa_n INT Maximum number of alignments to output in the XA tag for reads paired properly. If a read has more than INT hits, the XA tag will not be written. Default: 9999 Minmer Query \u00b6 --screen_w Winner-takes-all strategy for identity estimates. After counting hashes for each query, hashes that appear in multiple queries will be removed from all except the one with the best identity (ties broken by larger query), and other identities will be reduced. This removes output redundancy, providing a rough compositional outline. Default: True --screen_i FLOAT Minimum identity to report. Inclusive unless set to zero, in which case only identities greater than zero (i.e. with at least one shared hash) will be reported. Set to -1 to output everything. Default: 0.8 Minmer Sketch \u00b6 --mash_sketch INT Sketch size. Each sketch will have at most this many non-redundant min-hashes. Default: 10000 --sourmash_scale INT Choose number of hashes as 1 in FRACTION of input k-mers Default: 10000 Quality Control \u00b6 --adapters FASTA Illumina adapters to remove Default: BBmap adapters --adapter_k INT Kmer length used for finding adapters. Adapters shorter than k will not be found Default: 23 --phix FASTA phiX174 reference genome to remove Default: NC_001422 --phix_k INT Kmer length used for finding phiX174. Contaminants shorter than k will not be found Default: 31 --ktrim STR Trim reads to remove bases matching reference kmers Values: f (do not trim) r (trim to the right, Default) l (trim to the left) --mink INT Look for shorter kmers at read tips down to this length, when k-trimming or masking. 0 means disabled. Enabling this will disable maskmiddle Default: 11 --hdist INT Maximum Hamming distance for ref kmers (subs only) Memory use is proportional to (3*K)^hdist Default: 1 --tpe BOOL When kmer right-trimming, trim both reads to the minimum length of either Values: f (do not equally trim) t (equally trim to the right, Default) --tbo BOOL Trim adapters based on where paired reads overlap Values: f (do not trim by overlap) t (trim by overlap, Default) --qtrim STR Trim read ends to remove bases with quality below trimq. Performed AFTER looking for kmers Values: rl (trim both ends, Default) f (neither end) r (right end only) l (left end only) w (sliding window) --trimq FLOAT Regions with average quality BELOW this will be trimmed if qtrim is set to something other than f Default: 6 --maq INT Reads with average quality (after trimming) below this will be discarded Default: 20 --minlength INT Reads shorter than this after trimming will be discarded. Pairs will be discarded if both are shorter Default: 35 --ftm INT If positive, right-trim length to be equal to zero, modulo this number Default: 5 --tossjunk Discard reads with invalid characters as bases Values: f (keep all reads) t (toss reads with ambiguous bases, Default) --qout STR Output quality offset Values: 33 (PHRED33 offset quality scores, Default) 64 (PHRED64 offset quality scores) auto (keeps the current input offset) --xmx STR This will be passed to Java to set memory usage Examples: '8g' will specify 8 gigs of RAM (Default) '20g' will specify 20 gigs of RAM '200m' will specify 200 megs of RAM --maxcor INT Max number of corrections within a 20bp window Default: 1 --sampleseed INT Set to a positive number to use as the rng seed for sampling Default: 42 Variant Calling \u00b6 --snippy_ram INT Try and keep RAM under this many GB Default: 8 --mapqual INT Minimum read mapping quality to consider Default: 60 --basequal INT Minimum base quality to consider Default: 13 --mincov INT Minimum site depth to for calling alleles Default: 10 --minfrac FLOAT Minimum proportion for variant evidence (0=AUTO) Default: 0 --minqual INT Minimum QUALITY in VCF column 6 Default: 100 --maxsoft INT Maximum soft clipping to allow Default: 10 --bwaopt STR Extra BWA MEM options, eg. -x pacbio Default: '' --fbopt STR Extra Freebayes options, eg. --theta 1E-6 --read-snp-limit 2 Default: ''","title":"Complete Usage"},{"location":"usage-complete/#runtime-parameters","text":"Bactopia includes numerous (100+) configurable parameters. Basically for each step of the pipeline, you can modify the default parameters of a specific tool.","title":"Runtime Parameters"},{"location":"usage-complete/#required","text":"The required parameters depends on how many samples are to be proccessed. You can learn more about which approach to take at Specifying Input FASTQs . ### For Procesessing Multiple Samples --fastqs STR An input file containing the sample name and absolute paths to FASTQs to process ### For Processing A Single Sample --R1 STR First set of reads for paired end in compressed (gzip) FASTQ format --R2 STR Second set of reads for paired end in compressed (gzip) FASTQ format --SE STR Single end set of reads in compressed (gzip) FASTQ format --sample STR The name of the input sequences ### For Downloading from ENA --accessions An input file containing ENA/SRA experiement accessions to be processed --accession A single ENA/SRA Experiment accession to be processed","title":"Required"},{"location":"usage-complete/#dataset","text":"If you followed the steps in Build Datasets , you can use the following parameters to point Bactopia to you datasets. --datasets DIR The path to available public datasets that have already been set up --species STR Determines which species-specific dataset to use for the input sequencing","title":"Dataset"},{"location":"usage-complete/#optional","text":"These optional parameters, while not required, will be quite useful (especially --max_cpus and --cpus !) to tweak. --coverage INT Reduce samples to a given coverage Default: 100x --genome_size INT Expected genome size (bp) for all samples Default: Mash Estimate --outdir DIR Directory to write results to Default . --max_memory INT The maximum amount of memory (Gb) allowed to a single process. Default: 32 Gb","title":"Optional"},{"location":"usage-complete/#-max_cpus-vs-cpus","text":"By default when Nextflow executes, it uses all available cpus to queue up processes. As you might imagine, if you are on a single server with multiple users, this approach of using all cpus might annoy other users! (Whoops sorry!) To circumvent this feature, two parmeters have been included --max_cpus and --cpus . --max_cpus INT The maximum number of processors this workflow should have access to at any given moment Default: 1 --cpus INT Number of processors made available to a single process. If greater than \"--max_cpus\" it will be set equal to \"--max_cpus\" Default: 1 What --max_cpus does is specify to Nextflow the maximum number of cpus it is allowed to occupy at any given time. --cpus on the other hand, specifies how many cpus any given step (qc, assembly, annotation, etc...) can occupy. By default --max_cpus is set to 1 and if --cpus is set to a value greater than --max_cpus it will be set equal to --max_cpus . This appoach errs on the side of caution, by not occupying all cpus on the server without the users consent!","title":"--max_cpus vs --cpus"},{"location":"usage-complete/#helpers","text":"The following parameters are useful to test to test input parameters. --available_datasets Print a list of available datasets found based on location given by \"--datasets\" --example_fastqs Print example of expected input for FASTQs file --check_fastqs Verify \"--fastqs\" produces the expected inputs --clean_cache Removes 'work' and '.nextflow' logs. Caution, if used, the Nextflow run cannot be resumed. --keep_all_files Keeps all analysis files created. By default, intermediate files are removed. This will not affect the ability to resume Nextflow runs, and only occurs at the end of the process. --version Print workflow version information --help Show this message and exit --help_all Show a complete list of adjustable parameters","title":"Helpers"},{"location":"usage-complete/#-clean_cache","text":"Bactopia will keep Nextflow's work cache even after successfully completing. While the cache is maintained Bactopia is resumable using the -resume parameter. This does however introduce a potentential storage overhead. The cache will contain multiple intermediate files (e.g. uncompressed FASTQs, BAMs, etc...) for each sample that was processed. In other words, it can get pretty large! If you would like to clean up the cache you can use --clean_cache . This will remove the cache only after a successful execution (e.g. everything thing finished without errors). This is accomplished by removing the work directory created by Nextflow. As you might have guessed, by using removing the cache, Bactopia will no longer be resumeable. At the end of the day, you can always decide to not use --clean_cache and manually remove the work directory when you feel it is safe!","title":"--clean_cache"},{"location":"usage-complete/#-keep_all_files","text":"In some processes, Bactopia will delete large intermediate files (e.g. multiple uncompressed FASTQs) only after a process successfully completes. Since this a per-process function, it does not affect Nextflow's ability to resume ( -resume )a workflow. You can deactivate this feature using --keep_all_files . Please, keep in mind the work directory is already large, this will make it 2-3 times larger.","title":"--keep_all_files"},{"location":"usage-complete/#program-specific","text":"The remaining parameters are associated with specific programs. In the following sections, these parameters are grouped by which Nextflow process they are applicable to. The description and default values for these parameters were taken from the program to which they apply. It is important to note, not all of the available parameters for each and every program are available in Bactopia. If there is a parameter that was overlooked and should probably be included, please make a suggestion!","title":"Program Specific"},{"location":"usage-complete/#annotation","text":"--centre STR Sequencing centre ID Default: '' --addgenes Add 'gene' features for each 'CDS' feature --addmrna Add 'mRNA' features for each 'CDS' feature --rawproduct Do not clean up /product annotation --cdsrnaolap Allow [tr]RNA to overlap CDS --prokka_evalue STR Similarity e-value cut-off Default: 1e-09 --prokka_coverage INT Minimum coverage on query protein Default: 80 --norrna Don't run rRNA search --notrna Don't run tRNA search --rnammer Prefer RNAmmer over Barrnap for rRNA prediction","title":"Annotation"},{"location":"usage-complete/#antimicrobial-resistance","text":"--update_amr Force amrfinder to update its database. --amr_ident_min Minimum identity for nucleotide hit (0..1). -1 means use a curated threshold if it exists and 0.9 otherwise Default: -1 --amr_coverage_min Minimum coverage of the reference protein (0..1) Default: 0.5 --amr_organism Taxonomy group: Campylobacter, Escherichia, Klebsiella Salmonella, Staphylococcus, Vibrio Default: '' --amr_translation_table NCBI genetic code for translated BLAST Default: 11 --amr_plus Add the plus genes to the report --amr_report_common Suppress proteins common to a taxonomy group","title":"Antimicrobial Resistance"},{"location":"usage-complete/#ariba","text":"--nucmer_min_id INT Minimum alignment identity (delta-filter -i) Default: 90 --nucmer_min_len INT Minimum alignment length (delta-filter -i) Default: 20 --nucmer_breaklen INT Value to use for -breaklen when running nucmer Default: 200 --assembly_cov INT Target read coverage when sampling reads for assembly Default: 50 --min_scaff_depth INT Minimum number of read pairs needed as evidence for scaffold link between two contigs Default: 10 --spades_options STR Extra options to pass to Spades assembler Default: null --assembled_threshold FLOAT (between 0 and 1) If proportion of gene assembled (regardless of into how many contigs) is at least this value then the flag gene_assembled is set Default: 0.95 --gene_nt_extend INT Max number of nucleotides to extend ends of gene matches to look for start/stop codons Default: 30 --unique_threshold FLOAT (between 0 and 1) If proportion of bases in gene assembled more than once is <= this value, then the flag unique_contig is set Default: 0.03","title":"Ariba"},{"location":"usage-complete/#assembly","text":"--shovill_ram INT Try to keep RAM usage below this many GB Default: 32 --assembler STR Assembler: megahit velvet skesa spades Default: skesa --min_contig_len INT Minimum contig length <0=AUTO> Default: 500 --min_contig_cov INT Minimum contig coverage <0=AUTO> Default: 2 --contig_namefmt STR Format of contig FASTA IDs in 'printf' style Default: contig%05d --shovill_opts STR Extra assembler options in quotes eg. spades: \"--untrusted-contigs locus.fna\" ... Default: '' --shovill_kmers STR K-mers to use <blank=AUTO> Default: '' --trim Enable adaptor trimming --nostitch Disable read stitching --nocorr Disable post-assembly correction","title":"Assembly"},{"location":"usage-complete/#blast","text":"--perc_identity INT Percent identity Default: 50 --qcov_hsp_perc INT Percent query coverage per hsp Default: 50 --max_target_seqs INT Maximum number of aligned sequences to keep Default: 2000 --outfmt STR BLAST alignment view options Default: '6 qseqid qlen qstart qend sseqid slen sstart send length evalue bitscore pident nident mismatch gaps qcovs qcovhsp'","title":"BLAST"},{"location":"usage-complete/#counting-31mers","text":"--keep_singletons Keep all counted 31-mers Default: Filter out singletons","title":"Counting 31mers"},{"location":"usage-complete/#download-fastq","text":"--aspera_speed STR Speed at which Aspera Connect will download. Default: 100M --max_retry INT Maximum times to retry downloads Default: 10 --use_ftp Only use FTP to download FASTQs from ENA","title":"Download FASTQ"},{"location":"usage-complete/#insertion-mapping","text":"--min_clip INT Minimum size for softclipped region to be extracted from initial mapping Default: 10 --max_clip INT Maximum size for softclipped regions to be included Default: 30 --cutoff INT Minimum depth for mapped region to be kept in bed file Default: 6 --novel_gap_size INT Distance in base pairs between left and right flanks to be called a novel hit Default: 15 --min_range FLOAT Minimum percent size of the gap to be called a known hit Default: 0.9 --max_range FLOAT Maximum percent size of the gap to be called a known hit Default: 1.1 --merging INT Value for merging left and right hits in bed files together to simply calculation of closest and intersecting regions Default: 100 --ismap_all Switch on all alignment reporting for bwa --ismap_minqual INT Mapping quality score for bwa Default: 30","title":"Insertion Mapping"},{"location":"usage-complete/#mapping","text":"--keep_unmapped_reads Keep unmapped reads, this does not affect variant calling. --bwa_mem_opts STR Extra BWA MEM options Default: '' --bwa_aln_opts STR Extra BWA ALN options Default: '' --bwa_samse_opts STR Extra BWA SAMSE options Default: '' --bwa_sampe_opts STR Extra BWA SAMPE options Default: '' --bwa_n INT Maximum number of alignments to output in the XA tag for reads paired properly. If a read has more than INT hits, the XA tag will not be written. Default: 9999","title":"Mapping"},{"location":"usage-complete/#minmer-query","text":"--screen_w Winner-takes-all strategy for identity estimates. After counting hashes for each query, hashes that appear in multiple queries will be removed from all except the one with the best identity (ties broken by larger query), and other identities will be reduced. This removes output redundancy, providing a rough compositional outline. Default: True --screen_i FLOAT Minimum identity to report. Inclusive unless set to zero, in which case only identities greater than zero (i.e. with at least one shared hash) will be reported. Set to -1 to output everything. Default: 0.8","title":"Minmer Query"},{"location":"usage-complete/#minmer-sketch","text":"--mash_sketch INT Sketch size. Each sketch will have at most this many non-redundant min-hashes. Default: 10000 --sourmash_scale INT Choose number of hashes as 1 in FRACTION of input k-mers Default: 10000","title":"Minmer Sketch"},{"location":"usage-complete/#quality-control","text":"--adapters FASTA Illumina adapters to remove Default: BBmap adapters --adapter_k INT Kmer length used for finding adapters. Adapters shorter than k will not be found Default: 23 --phix FASTA phiX174 reference genome to remove Default: NC_001422 --phix_k INT Kmer length used for finding phiX174. Contaminants shorter than k will not be found Default: 31 --ktrim STR Trim reads to remove bases matching reference kmers Values: f (do not trim) r (trim to the right, Default) l (trim to the left) --mink INT Look for shorter kmers at read tips down to this length, when k-trimming or masking. 0 means disabled. Enabling this will disable maskmiddle Default: 11 --hdist INT Maximum Hamming distance for ref kmers (subs only) Memory use is proportional to (3*K)^hdist Default: 1 --tpe BOOL When kmer right-trimming, trim both reads to the minimum length of either Values: f (do not equally trim) t (equally trim to the right, Default) --tbo BOOL Trim adapters based on where paired reads overlap Values: f (do not trim by overlap) t (trim by overlap, Default) --qtrim STR Trim read ends to remove bases with quality below trimq. Performed AFTER looking for kmers Values: rl (trim both ends, Default) f (neither end) r (right end only) l (left end only) w (sliding window) --trimq FLOAT Regions with average quality BELOW this will be trimmed if qtrim is set to something other than f Default: 6 --maq INT Reads with average quality (after trimming) below this will be discarded Default: 20 --minlength INT Reads shorter than this after trimming will be discarded. Pairs will be discarded if both are shorter Default: 35 --ftm INT If positive, right-trim length to be equal to zero, modulo this number Default: 5 --tossjunk Discard reads with invalid characters as bases Values: f (keep all reads) t (toss reads with ambiguous bases, Default) --qout STR Output quality offset Values: 33 (PHRED33 offset quality scores, Default) 64 (PHRED64 offset quality scores) auto (keeps the current input offset) --xmx STR This will be passed to Java to set memory usage Examples: '8g' will specify 8 gigs of RAM (Default) '20g' will specify 20 gigs of RAM '200m' will specify 200 megs of RAM --maxcor INT Max number of corrections within a 20bp window Default: 1 --sampleseed INT Set to a positive number to use as the rng seed for sampling Default: 42","title":"Quality Control"},{"location":"usage-complete/#variant-calling","text":"--snippy_ram INT Try and keep RAM under this many GB Default: 8 --mapqual INT Minimum read mapping quality to consider Default: 60 --basequal INT Minimum base quality to consider Default: 13 --mincov INT Minimum site depth to for calling alleles Default: 10 --minfrac FLOAT Minimum proportion for variant evidence (0=AUTO) Default: 0 --minqual INT Minimum QUALITY in VCF column 6 Default: 100 --maxsoft INT Maximum soft clipping to allow Default: 10 --bwaopt STR Extra BWA MEM options, eg. -x pacbio Default: '' --fbopt STR Extra Freebayes options, eg. --theta 1E-6 --read-snp-limit 2 Default: ''","title":"Variant Calling"}]}