{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview \u00b6 Bactopia is an extensive workflow for processing Illumina sequencing of bacterial genomes. The goal of Bactopia is process your data with a broad set of tools, so that you can get to the fun part of analyses quicker! Bactopia was inspired by Staphopia , a workflow we (Tim Read and myself) released that targets Staphylococcus aureus genomes. Using what we learned from Staphopia and user feedback, Bactopia was developed from scratch with usability, portability, and speed in mind from the start. Bactopia uses Nextflow to manage the workflow, allowing for support of many types of environments (e.g. cluster or cloud). Bactopia allows for the usage of many public datasets as well as your own datasets to further enhance the analysis of your seqeuncing. Bactopia only uses software packages available from Bioconda (or other Anaconda channels ) to make installation as simple as possible for all users. Bactopia Workflow \u00b6 Documentation Overview \u00b6 Quick Start Very concise and to straight the point details (unlike this!) for using Bactopia. Installation More detailed information for getting Bactopia set up on your system. Basic Usage A subset of parameters users may commonly adjust. Tutorial A brief tutorial on how to replicate the Staphopia Analyis Pipeline using Bactopia. Build Datasets A description on how to make use of datasets (public or private) with Bactopia. Workflow Overview A description of Bactopia workflow and software used. Output Overview A description of Bactopia output directories and files. Complete Usage The full set of parameters that users can tweak in Bactopia. Acknowledgements A list of datasets and software (and many thanks!) used by Bactopia.","title":"Introduction"},{"location":"#overview","text":"Bactopia is an extensive workflow for processing Illumina sequencing of bacterial genomes. The goal of Bactopia is process your data with a broad set of tools, so that you can get to the fun part of analyses quicker! Bactopia was inspired by Staphopia , a workflow we (Tim Read and myself) released that targets Staphylococcus aureus genomes. Using what we learned from Staphopia and user feedback, Bactopia was developed from scratch with usability, portability, and speed in mind from the start. Bactopia uses Nextflow to manage the workflow, allowing for support of many types of environments (e.g. cluster or cloud). Bactopia allows for the usage of many public datasets as well as your own datasets to further enhance the analysis of your seqeuncing. Bactopia only uses software packages available from Bioconda (or other Anaconda channels ) to make installation as simple as possible for all users.","title":"Overview"},{"location":"#bactopia-workflow","text":"","title":"Bactopia Workflow"},{"location":"#documentation-overview","text":"Quick Start Very concise and to straight the point details (unlike this!) for using Bactopia. Installation More detailed information for getting Bactopia set up on your system. Basic Usage A subset of parameters users may commonly adjust. Tutorial A brief tutorial on how to replicate the Staphopia Analyis Pipeline using Bactopia. Build Datasets A description on how to make use of datasets (public or private) with Bactopia. Workflow Overview A description of Bactopia workflow and software used. Output Overview A description of Bactopia output directories and files. Complete Usage The full set of parameters that users can tweak in Bactopia. Acknowledgements A list of datasets and software (and many thanks!) used by Bactopia.","title":"Documentation Overview"},{"location":"acknowledgements/","text":"Acknowledgements \u00b6 Bactopia is truly a case of \"standing upon the shoulders of giants\" . Nearly every component of Bactopia was created by others and made freely available to the public. I would like to personally extend my many thanks and gratitude to the authors of these software packages and public datasets. If you've made it this far, I owe you a beer \ud83c\udf7b (or coffee \u2615!) if we ever encounter one another in person. Really, thank you very much! Please Cite Datasets and Tools If you have used Bactopia in your work, please be sure to cite any datasets or software you may have used. A citation link for each dataset/software has been made available. A BibTeX file of each citation is also available at Bactopia Datasets and Software BibTeX Public Datasets \u00b6 Below is a list of public datasets (alphabetical) that could have potentially been included during the Build Datasets step. Ariba Reference Datasets \u00b6 These datasets are available using Ariba's getref function. You can learn more about this function at Ariba's Wiki . ARG-ANNOT Gupta, S. K. et al. ARG-ANNOT, a new bioinformatic tool to discover antibiotic resistance genes in bacterial genomes. Antimicrob. Agents Chemother. 58, 212\u2013220 (2014). CARD McArthur, A. G. et al. The comprehensive antibiotic resistance database. Antimicrob. Agents Chemother. 57, 3348\u20133357 (2013). MEGARes Lakin, S. M. et al. MEGARes: an antimicrobial resistance database for high throughput sequencing . Nucleic Acids Res. 45, D574\u2013D580 (2017). NCBI Reference Gene Catalog Feldgarden, M. et al. Validating the NCBI AMRFinder Tool and Resistance Gene Database Using Antimicrobial Resistance Genotype-Phenotype Correlations in a Collection of NARMS Isolates . Antimicrob. Agents Chemother. (2019) plasmidfinder Carattoli, A. et al. In silico detection and typing of plasmids using PlasmidFinder and plasmid multilocus sequence typing. Antimicrob. Agents Chemother. 58, 3895\u20133903 (2014). resfinder Zankari, E. et al. Identification of acquired antimicrobial resistance genes. J. Antimicrob. Chemother. 67, 2640\u20132644 (2012). SRST2 Inouye, M. et al. SRST2: Rapid genomic surveillance for public health and hospital microbiology labs. Genome Med. 6, 90 (2014). VFDB Chen, L., Zheng, D., Liu, B., Yang, J. & Jin, Q. VFDB 2016: hierarchical and refined dataset for big data analysis--10 years on. Nucleic Acids Res. 44, D694\u20137 (2016). VirulenceFinder Joensen, K. G. et al. Real-time whole-genome sequencing for routine typing, surveillance, and outbreak detection of verotoxigenic Escherichia coli. J. Clin. Microbiol. 52, 1501\u20131510 (2014). Minmer Datasets \u00b6 Mash Refseq (release 88) Sketch Ondov, B. D. et al. Mash Screen: High-throughput sequence containment estimation for genome discovery. bioRxiv 557314 (2019). Sourmash Genbank LCA Signature Titus Brown, C. & Irber, L. sourmash: a library for MinHash sketching of DNA. JOSS 1, 27 (2016). Everything Else \u00b6 NCBI RefSeq Database O\u2019Leary, N. A. et al. Reference sequence (RefSeq) database at NCBI: current status, taxonomic expansion, and functional annotation . Nucleic Acids Res. 44, D733\u201345 (2016). PLSDB - A plasmid database Galata, V., Fehlmann, T., Backes, C. & Keller, A. PLSDB: a resource of complete bacterial plasmids . Nucleic Acids Res. 47, D195\u2013D202 (2019). PubMLST.org Jolley, K. A., Bray, J. E. & Maiden, M. C. J. Open-access bacterial population genomics: BIGSdb software, the PubMLST.org website and their applications . Wellcome Open Res 3, 124 (2018). Software Included In Bactopia \u00b6 Below is a list of software (alphabetical) used (directly and indirectly) by Bactopia. A link to the software page as well as the citation (if available) have been included. AMRFinderPlus Find acquired antimicrobial resistance genes and some point mutations in protein or assembled nucleotide sequences. Feldgarden, M. et al. Validating the NCBI AMRFinder Tool and Resistance Gene Database Using Antimicrobial Resistance Genotype-Phenotype Correlations in a Collection of NARMS Isolates . Antimicrob. Agents Chemother. (2019) Aragorn Finds transfer RNA features (tRNA) Laslett D. and B. Canback, ARAGORN, a program to detect tRNA genes and tmRNA genes in nucleotide sequences. Nucleic Acids Res. 32(1):11-6. (2004) Ariba Antimicrobial Resistance Identification By Assembly Hunt, M. et al. ARIBA: rapid antimicrobial resistance genotyping directly from sequencing reads . Microb Genom 3, e000131 (2017). assembly-scan Generate basic stats for an assembly. Petit III, R. A. assembly-scan: generate basic stats for an assembly . Barrnap Bacterial ribosomal RNA predictor Seemann, T. Barrnap: Bacterial ribosomal RNA predictor BBTools BBTools is a suite of fast, multithreaded bioinformatics tools designed for analysis of DNA and RNA sequence data. Bushnell, B. BBMap short read aligner, and other bioinformatic tools. BCFtools Utilities for variant calling and manipulating VCFs and BCFs. Danecek, P. et al. BCFtools - Utilities for variant calling and manipulating VCFs and BCFs. Bedtools A powerful toolset for genome arithmetic. Quinlan, A. R. & Hall, I. M. BEDTools: a flexible suite of utilities for comparing genomic features . Bioinformatics 26, 841\u2013842 (2010). BLAST Basic Local Alignment Search Tool Camacho, C. et al. BLAST+: architecture and applications . BMC Bioinformatics 10, 421 (2009). BWA Burrow-Wheeler Aligner for short-read alignment Li, H. Aligning sequence reads, clone sequences and assembly contigs with BWA-MEM . arXiv [q-bio.GN] (2013). CD-Hit Accelerated for clustering the next-generation sequencing data Li, W. & Godzik, A. Cd-hit: a fast program for clustering and comparing large sets of protein or nucleotide sequences . Bioinformatics 22, 1658\u20131659 (2006). Fu, L., Niu, B., Zhu, Z., Wu, S. & Li, W. CD-HIT: accelerated for clustering the next-generation sequencing data . Bioinformatics 28, 3150\u20133152 (2012). ena-dl Download FASTQ files from ENA and group runs by Experiment accession. Petit III, R. A. ena-dl: download FASTQs from the European Nucleotide Archive. FastQC A quality control analysis tool for high throughput sequencing data. Andrews, S. FastQC: a quality control tool for high throughput sequence data. . fastq-scan Output FASTQ summary statistics in JSON format Petit III, R. A. fastq-scan: generate summary statistics of input FASTQ sequences. FLASH A fast and accurate tool to merge paired-end reads. Mago\u010d, T., and S. L. Salzberg, FLASH: fast length adjustment of short reads to improve genome assemblies. Bioinformatics 27.21 (2011): 2957-2963. freebayes Bayesian haplotype-based genetic polymorphism discovery and genotyping Garrison E., and G. Marth, Haplotype-based variant detection from short-read sequencing. arXiv preprint arXiv:1207.3907 [q-bio.GN] (2012) GNU Parallel A shell tool for executing jobs in parallel Tange, O. GNU Parallel 2018, March 2018 HMMER Biosequence analysis using profile hidden Markov models Finn R. D. et al. HMMER web server: interactive sequence similarity searching. Nucleic Acids Res. ;39:W29-37. (2011) Infernal Searches DNA sequence databases for RNA structure and sequence similarities Nawrocki, E. P., and S. R. Eddy, Infernal 1.1: 100-fold faster RNA homology searches. Bioinformatics, 29(22), 2933-2935. (2013) ISMapper IS mapping software Hawkey, J. et al. ISMapper: identifying transposase insertion sites in bacterial genomes from short read sequence data . BMC Genomics 16, 667 (2015). Lighter Fast and memory-efficient sequencing error corrector Song, L., Florea, L. and B. Langmead, Lighter: Fast and Memory-efficient Sequencing Error Correction without Counting . Genome Biol. 2014 Nov 15;15(11):509. Mash Fast genome and metagenome distance estimation using MinHash Ondov, B. D. et al. Mash: fast genome and metagenome distance estimation using MinHash . Genome Biol. 17, 132 (2016). Ondov, B. D. et al. Mash Screen: High-throughput sequence containment estimation for genome discovery . bioRxiv 557314 (2019). McCortex De novo genome assembly and multisample variant calling Turner, I., Garimella, K. V., Iqbal, Z. and G. McVean, Integrating long-range connectivity information into de Bruijn graphs. Bioinformatics 34, 2556\u20132565 (2018). MEGAHIT Ultra-fast and memory-efficient (meta-)genome assembler Li, D., et al. MEGAHIT: an ultra-fast single-node solution for large and complex metagenomics assembly via succinct de Bruijn graph. Bioinformatics 31.10 (2015): 1674-1676. MinCED Mining CRISPRs in Environmental Datasets Skennerton, C. MinCED: Mining CRISPRs in Environmental Datasets Minimap2 A versatile pairwise aligner for genomic and spliced nucleotide sequences Li, H. Minimap2: pairwise alignment for nucleotide sequences. Bioinformatics, 34:3094-3100. (2018) NCBI Genome Download Scripts to download genomes from the NCBI FTP servers Blin, K. NCBI Genome Download: Scripts to download genomes from the NCBI FTP servers Nextflow A DSL for data-driven computational pipelines. Di Tommaso, P., Chatzou, M., Floden, E. W., Barja, P.P., Palumbo, E., Notredame, C., 2017. Nextflow enables reproducible computational workflows. Nat. Biotechnol. 35, 316\u2013319. Pigz A parallel implementation of gzip for modern multi-processor, multi-core machines. Adler, M. pigz: A parallel implementation of gzip for modern multi-processor, multi-core machines. Jet Propulsion Laboratory (2015). Pilon An automated genome assembly improvement and variant detection tool Walker, B. J., et al. Pilon: an integrated tool for comprehensive microbial variant detection and genome assembly improvement. PloS one 9.11 (2014): e112963. Prodigal Fast, reliable protein-coding gene prediction for prokaryotic genomes. Hyatt, D., et al. Prodigal: prokaryotic gene recognition and translation initiation site identification. BMC Bioinformatics 11.1 (2010): 119. Prokka Rapid prokaryotic genome annotation Seemann, T. Prokka: rapid prokaryotic genome annotation . Bioinformatics 30, 2068\u20132069 (2014). RNAmmer Consistent and rapid annotation of ribosomal RNA genes Lagesen, K., et al. RNAmmer: consistent annotation of rRNA genes in genomic sequences. Nucleic Acids Res 35.9: 3100-3108. (2007) samclip Filter SAM file for soft and hard clipped alignments Seemann, T. Samclip: Filter SAM file for soft and hard clipped alignments Samtools Tools for manipulating next-generation sequencing data Li, H. et al. The Sequence Alignment/Map format and SAMtools . Bioinformatics 25, 2078\u20132079 (2009). Seqtk A fast and lightweight tool for processing sequences in the FASTA or FASTQ format. Li, H. Toolkit for processing sequences in FASTA/Q formats Shovill Faster assembly of Illumina reads Seemann, T. Shovill: De novo assembly pipeline for Illumina paired reads SignalP Finds signal peptide features in CDS Petersen, T. N., et al. SignalP 4.0: discriminating signal peptides from transmembrane regions. Nature methods 8.10: 785.(2011) SKESA Strategic Kmer Extension for Scrupulous Assemblies Souvorov, A., Agarwala, R. and D. J. Lipman. SKESA: strategic k-mer extension for scrupulous assemblies. Genome Biology 19:153 (2018)._ Snippy Rapid haploid variant calling and core genome alignment Seemann, T. Snippy: fast bacterial variant calling from NGS reads (2015) SnpEff Genomic variant annotations and functional effect prediction toolbox. Cingolani, P., et al. A program for annotating and predicting the effects of single nucleotide polymorphisms, SnpEff: SNPs in the genome of Drosophila melanogaster strain w1118; iso-2; iso-3. Fly, 6(2), 80-92 (2012) SNP-sites Rapidly extracts SNPs from a multi-FASTA alignment. Page, A. J., et al. SNP-sites: rapid efficient extraction of SNPs from multi-FASTA alignments. Microbial Genomics 2.4 (2016). Sourmash Compute and compare MinHash signatures for DNA data sets. Titus Brown, C. and L. Irber sourmash: a library for MinHash sketching of DNA . JOSS 1, 27 (2016). SPAdes An assembly toolkit containing various assembly pipelines. Bankevich, A., et al. SPAdes: a new genome assembly algorithm and its applications to single-cell sequencing. Journal of computational biology 19.5 (2012): 455-477. Trimmomatic A flexible read trimming tool for Illumina NGS data Bolger, A. M., Lohse, M., and B. Usadel. Trimmomatic: a flexible trimmer for Illumina sequence data. Bioinformatics 30.15 (2014): 2114-2120. VCF-Annotator Add biological annotations to variants in a VCF file. Petit III, R. A. VCF-Annotator: Add biological annotations to variants in a VCF file. . Vcflib a simple C++ library for parsing and manipulating VCF files Garrison, E. Vcflib: A C++ library for parsing and manipulating VCF files Velvet Short read de novo assembler using de Bruijn graphs Zerbino, D. R., and E. Birney. Velvet: algorithms for de novo short read assembly using de Bruijn graphs. Genome research 18.5 (2008): 821-829. vt A tool set for short variant discovery in genetic sequence data. Tan, A., Abecasis, G. R., and H. M. Kang, Unified representation of genetic variants. Bioinformatics, 31(13), 2202-2204. (2015)","title":"Acknowledgements"},{"location":"acknowledgements/#acknowledgements","text":"Bactopia is truly a case of \"standing upon the shoulders of giants\" . Nearly every component of Bactopia was created by others and made freely available to the public. I would like to personally extend my many thanks and gratitude to the authors of these software packages and public datasets. If you've made it this far, I owe you a beer \ud83c\udf7b (or coffee \u2615!) if we ever encounter one another in person. Really, thank you very much! Please Cite Datasets and Tools If you have used Bactopia in your work, please be sure to cite any datasets or software you may have used. A citation link for each dataset/software has been made available. A BibTeX file of each citation is also available at Bactopia Datasets and Software BibTeX","title":"Acknowledgements"},{"location":"acknowledgements/#public-datasets","text":"Below is a list of public datasets (alphabetical) that could have potentially been included during the Build Datasets step.","title":"Public Datasets"},{"location":"acknowledgements/#ariba-reference-datasets","text":"These datasets are available using Ariba's getref function. You can learn more about this function at Ariba's Wiki . ARG-ANNOT Gupta, S. K. et al. ARG-ANNOT, a new bioinformatic tool to discover antibiotic resistance genes in bacterial genomes. Antimicrob. Agents Chemother. 58, 212\u2013220 (2014). CARD McArthur, A. G. et al. The comprehensive antibiotic resistance database. Antimicrob. Agents Chemother. 57, 3348\u20133357 (2013). MEGARes Lakin, S. M. et al. MEGARes: an antimicrobial resistance database for high throughput sequencing . Nucleic Acids Res. 45, D574\u2013D580 (2017). NCBI Reference Gene Catalog Feldgarden, M. et al. Validating the NCBI AMRFinder Tool and Resistance Gene Database Using Antimicrobial Resistance Genotype-Phenotype Correlations in a Collection of NARMS Isolates . Antimicrob. Agents Chemother. (2019) plasmidfinder Carattoli, A. et al. In silico detection and typing of plasmids using PlasmidFinder and plasmid multilocus sequence typing. Antimicrob. Agents Chemother. 58, 3895\u20133903 (2014). resfinder Zankari, E. et al. Identification of acquired antimicrobial resistance genes. J. Antimicrob. Chemother. 67, 2640\u20132644 (2012). SRST2 Inouye, M. et al. SRST2: Rapid genomic surveillance for public health and hospital microbiology labs. Genome Med. 6, 90 (2014). VFDB Chen, L., Zheng, D., Liu, B., Yang, J. & Jin, Q. VFDB 2016: hierarchical and refined dataset for big data analysis--10 years on. Nucleic Acids Res. 44, D694\u20137 (2016). VirulenceFinder Joensen, K. G. et al. Real-time whole-genome sequencing for routine typing, surveillance, and outbreak detection of verotoxigenic Escherichia coli. J. Clin. Microbiol. 52, 1501\u20131510 (2014).","title":"Ariba Reference Datasets"},{"location":"acknowledgements/#minmer-datasets","text":"Mash Refseq (release 88) Sketch Ondov, B. D. et al. Mash Screen: High-throughput sequence containment estimation for genome discovery. bioRxiv 557314 (2019). Sourmash Genbank LCA Signature Titus Brown, C. & Irber, L. sourmash: a library for MinHash sketching of DNA. JOSS 1, 27 (2016).","title":"Minmer Datasets"},{"location":"acknowledgements/#everything-else","text":"NCBI RefSeq Database O\u2019Leary, N. A. et al. Reference sequence (RefSeq) database at NCBI: current status, taxonomic expansion, and functional annotation . Nucleic Acids Res. 44, D733\u201345 (2016). PLSDB - A plasmid database Galata, V., Fehlmann, T., Backes, C. & Keller, A. PLSDB: a resource of complete bacterial plasmids . Nucleic Acids Res. 47, D195\u2013D202 (2019). PubMLST.org Jolley, K. A., Bray, J. E. & Maiden, M. C. J. Open-access bacterial population genomics: BIGSdb software, the PubMLST.org website and their applications . Wellcome Open Res 3, 124 (2018).","title":"Everything Else"},{"location":"acknowledgements/#software-included-in-bactopia","text":"Below is a list of software (alphabetical) used (directly and indirectly) by Bactopia. A link to the software page as well as the citation (if available) have been included. AMRFinderPlus Find acquired antimicrobial resistance genes and some point mutations in protein or assembled nucleotide sequences. Feldgarden, M. et al. Validating the NCBI AMRFinder Tool and Resistance Gene Database Using Antimicrobial Resistance Genotype-Phenotype Correlations in a Collection of NARMS Isolates . Antimicrob. Agents Chemother. (2019) Aragorn Finds transfer RNA features (tRNA) Laslett D. and B. Canback, ARAGORN, a program to detect tRNA genes and tmRNA genes in nucleotide sequences. Nucleic Acids Res. 32(1):11-6. (2004) Ariba Antimicrobial Resistance Identification By Assembly Hunt, M. et al. ARIBA: rapid antimicrobial resistance genotyping directly from sequencing reads . Microb Genom 3, e000131 (2017). assembly-scan Generate basic stats for an assembly. Petit III, R. A. assembly-scan: generate basic stats for an assembly . Barrnap Bacterial ribosomal RNA predictor Seemann, T. Barrnap: Bacterial ribosomal RNA predictor BBTools BBTools is a suite of fast, multithreaded bioinformatics tools designed for analysis of DNA and RNA sequence data. Bushnell, B. BBMap short read aligner, and other bioinformatic tools. BCFtools Utilities for variant calling and manipulating VCFs and BCFs. Danecek, P. et al. BCFtools - Utilities for variant calling and manipulating VCFs and BCFs. Bedtools A powerful toolset for genome arithmetic. Quinlan, A. R. & Hall, I. M. BEDTools: a flexible suite of utilities for comparing genomic features . Bioinformatics 26, 841\u2013842 (2010). BLAST Basic Local Alignment Search Tool Camacho, C. et al. BLAST+: architecture and applications . BMC Bioinformatics 10, 421 (2009). BWA Burrow-Wheeler Aligner for short-read alignment Li, H. Aligning sequence reads, clone sequences and assembly contigs with BWA-MEM . arXiv [q-bio.GN] (2013). CD-Hit Accelerated for clustering the next-generation sequencing data Li, W. & Godzik, A. Cd-hit: a fast program for clustering and comparing large sets of protein or nucleotide sequences . Bioinformatics 22, 1658\u20131659 (2006). Fu, L., Niu, B., Zhu, Z., Wu, S. & Li, W. CD-HIT: accelerated for clustering the next-generation sequencing data . Bioinformatics 28, 3150\u20133152 (2012). ena-dl Download FASTQ files from ENA and group runs by Experiment accession. Petit III, R. A. ena-dl: download FASTQs from the European Nucleotide Archive. FastQC A quality control analysis tool for high throughput sequencing data. Andrews, S. FastQC: a quality control tool for high throughput sequence data. . fastq-scan Output FASTQ summary statistics in JSON format Petit III, R. A. fastq-scan: generate summary statistics of input FASTQ sequences. FLASH A fast and accurate tool to merge paired-end reads. Mago\u010d, T., and S. L. Salzberg, FLASH: fast length adjustment of short reads to improve genome assemblies. Bioinformatics 27.21 (2011): 2957-2963. freebayes Bayesian haplotype-based genetic polymorphism discovery and genotyping Garrison E., and G. Marth, Haplotype-based variant detection from short-read sequencing. arXiv preprint arXiv:1207.3907 [q-bio.GN] (2012) GNU Parallel A shell tool for executing jobs in parallel Tange, O. GNU Parallel 2018, March 2018 HMMER Biosequence analysis using profile hidden Markov models Finn R. D. et al. HMMER web server: interactive sequence similarity searching. Nucleic Acids Res. ;39:W29-37. (2011) Infernal Searches DNA sequence databases for RNA structure and sequence similarities Nawrocki, E. P., and S. R. Eddy, Infernal 1.1: 100-fold faster RNA homology searches. Bioinformatics, 29(22), 2933-2935. (2013) ISMapper IS mapping software Hawkey, J. et al. ISMapper: identifying transposase insertion sites in bacterial genomes from short read sequence data . BMC Genomics 16, 667 (2015). Lighter Fast and memory-efficient sequencing error corrector Song, L., Florea, L. and B. Langmead, Lighter: Fast and Memory-efficient Sequencing Error Correction without Counting . Genome Biol. 2014 Nov 15;15(11):509. Mash Fast genome and metagenome distance estimation using MinHash Ondov, B. D. et al. Mash: fast genome and metagenome distance estimation using MinHash . Genome Biol. 17, 132 (2016). Ondov, B. D. et al. Mash Screen: High-throughput sequence containment estimation for genome discovery . bioRxiv 557314 (2019). McCortex De novo genome assembly and multisample variant calling Turner, I., Garimella, K. V., Iqbal, Z. and G. McVean, Integrating long-range connectivity information into de Bruijn graphs. Bioinformatics 34, 2556\u20132565 (2018). MEGAHIT Ultra-fast and memory-efficient (meta-)genome assembler Li, D., et al. MEGAHIT: an ultra-fast single-node solution for large and complex metagenomics assembly via succinct de Bruijn graph. Bioinformatics 31.10 (2015): 1674-1676. MinCED Mining CRISPRs in Environmental Datasets Skennerton, C. MinCED: Mining CRISPRs in Environmental Datasets Minimap2 A versatile pairwise aligner for genomic and spliced nucleotide sequences Li, H. Minimap2: pairwise alignment for nucleotide sequences. Bioinformatics, 34:3094-3100. (2018) NCBI Genome Download Scripts to download genomes from the NCBI FTP servers Blin, K. NCBI Genome Download: Scripts to download genomes from the NCBI FTP servers Nextflow A DSL for data-driven computational pipelines. Di Tommaso, P., Chatzou, M., Floden, E. W., Barja, P.P., Palumbo, E., Notredame, C., 2017. Nextflow enables reproducible computational workflows. Nat. Biotechnol. 35, 316\u2013319. Pigz A parallel implementation of gzip for modern multi-processor, multi-core machines. Adler, M. pigz: A parallel implementation of gzip for modern multi-processor, multi-core machines. Jet Propulsion Laboratory (2015). Pilon An automated genome assembly improvement and variant detection tool Walker, B. J., et al. Pilon: an integrated tool for comprehensive microbial variant detection and genome assembly improvement. PloS one 9.11 (2014): e112963. Prodigal Fast, reliable protein-coding gene prediction for prokaryotic genomes. Hyatt, D., et al. Prodigal: prokaryotic gene recognition and translation initiation site identification. BMC Bioinformatics 11.1 (2010): 119. Prokka Rapid prokaryotic genome annotation Seemann, T. Prokka: rapid prokaryotic genome annotation . Bioinformatics 30, 2068\u20132069 (2014). RNAmmer Consistent and rapid annotation of ribosomal RNA genes Lagesen, K., et al. RNAmmer: consistent annotation of rRNA genes in genomic sequences. Nucleic Acids Res 35.9: 3100-3108. (2007) samclip Filter SAM file for soft and hard clipped alignments Seemann, T. Samclip: Filter SAM file for soft and hard clipped alignments Samtools Tools for manipulating next-generation sequencing data Li, H. et al. The Sequence Alignment/Map format and SAMtools . Bioinformatics 25, 2078\u20132079 (2009). Seqtk A fast and lightweight tool for processing sequences in the FASTA or FASTQ format. Li, H. Toolkit for processing sequences in FASTA/Q formats Shovill Faster assembly of Illumina reads Seemann, T. Shovill: De novo assembly pipeline for Illumina paired reads SignalP Finds signal peptide features in CDS Petersen, T. N., et al. SignalP 4.0: discriminating signal peptides from transmembrane regions. Nature methods 8.10: 785.(2011) SKESA Strategic Kmer Extension for Scrupulous Assemblies Souvorov, A., Agarwala, R. and D. J. Lipman. SKESA: strategic k-mer extension for scrupulous assemblies. Genome Biology 19:153 (2018)._ Snippy Rapid haploid variant calling and core genome alignment Seemann, T. Snippy: fast bacterial variant calling from NGS reads (2015) SnpEff Genomic variant annotations and functional effect prediction toolbox. Cingolani, P., et al. A program for annotating and predicting the effects of single nucleotide polymorphisms, SnpEff: SNPs in the genome of Drosophila melanogaster strain w1118; iso-2; iso-3. Fly, 6(2), 80-92 (2012) SNP-sites Rapidly extracts SNPs from a multi-FASTA alignment. Page, A. J., et al. SNP-sites: rapid efficient extraction of SNPs from multi-FASTA alignments. Microbial Genomics 2.4 (2016). Sourmash Compute and compare MinHash signatures for DNA data sets. Titus Brown, C. and L. Irber sourmash: a library for MinHash sketching of DNA . JOSS 1, 27 (2016). SPAdes An assembly toolkit containing various assembly pipelines. Bankevich, A., et al. SPAdes: a new genome assembly algorithm and its applications to single-cell sequencing. Journal of computational biology 19.5 (2012): 455-477. Trimmomatic A flexible read trimming tool for Illumina NGS data Bolger, A. M., Lohse, M., and B. Usadel. Trimmomatic: a flexible trimmer for Illumina sequence data. Bioinformatics 30.15 (2014): 2114-2120. VCF-Annotator Add biological annotations to variants in a VCF file. Petit III, R. A. VCF-Annotator: Add biological annotations to variants in a VCF file. . Vcflib a simple C++ library for parsing and manipulating VCF files Garrison, E. Vcflib: A C++ library for parsing and manipulating VCF files Velvet Short read de novo assembler using de Bruijn graphs Zerbino, D. R., and E. Birney. Velvet: algorithms for de novo short read assembly using de Bruijn graphs. Genome research 18.5 (2008): 821-829. vt A tool set for short variant discovery in genetic sequence data. Tan, A., Abecasis, G. R., and H. M. Kang, Unified representation of genetic variants. Bioinformatics, 31(13), 2202-2204. (2015)","title":"Software Included In Bactopia"},{"location":"datasets/","text":"Build Datasets \u00b6 Bactopia can make use of many existing public datasets, as well as private datasets. The process of downloading, building, and (or) configuring these datasets for Bactopia has been automated. Highly recommended to complete this step! This step is completely optional, but it is highly recommended that you do not. By skipping this step of setting up public datasets, Bactopia will be limited to analyses like quality control, assembly, and 31-mer counting. Included Datasets \u00b6 Some datasets included are applicable to all bacterial species and some are specific to a bacterial species. If specified at runtime, Bactopia will recognize the datasets and execute the appropriate analyses. General \u00b6 Ariba's getref Reference Datasets Allows reference datasets (resistance, virulence, and plamids) to be automatically downloaded and configured for usage by Ariba RefSeq Mash Sketch ~100,000 genomes and plasmids from NCBI RefSeq, used to give an idea of what is your sequencing data (e.g. Are the sequences what you expected?) GenBank Sourmash Signatures ~87,000 microbial genomes (includes viral and fungal) from NCBI GenBank, also gives an idea of what is your sequencing data. PLSDB Mash Sketch & BLAST Includes meta data/annotations, Mash sketches, and BLAST database files of all plasmids stored in PLSDB. Species Specific \u00b6 PubMLST.org MLST Schemas Multi-locus sequence typing (MLST) allelic profiles and seqeunces for a many different bacterial species (and even a few eukaryotes!). Clustered RefSeq Proteins For the given bacterial species, completed RefSeq genomes are downloaded and then the proteins are clustered and formatted for usage with Prokka. Minmer Sketch of RefSeq Genomes Using the completed genomes downloaded for clustering proteins a Mash sketch and Sourmash signature is created for these genomes. These sketches can then be used for automatic selection of reference genomes for variant calling. Optional User Populated Folders A few folders for things such as calling variants, insertion sequences and primers are created that the user can manually populate. More information is available below! Setting Up \u00b6 Included in Bactopia is the setup-datasets.py script (located in the bin folder) to automate the process of downloading and/or building these datasets. Quick Start \u00b6 bactopia datasets datasets/ This will set up Ariba datasets ( card and vfdb_core ), RefSeq Mash sketch, GenBank Sourmash Signatures, and PLSDB in the newly created datasets folder. A Single Bacterial Species \u00b6 bactopia datasets datasets/ --species \"Haemophilus influenzae\" --include_genus Multiple Bacterial Species \u00b6 You can also set up datasets for multiple bacterial species at a time. There are two options to do so. Comma-Separated \u00b6 At runtime, you can separate the the different species bactopia datasets datasets/ --species \"Haemophilus influenzae,Staphylococcus aureus\" --include_genus Text File \u00b6 In order to do so, you will need to create a text file where each line is the name of a species to set up. For example, you could create a species.txt file and include the following species in it. Haemophilus influenzae Staphylococcus aureus Mycobacterium tuberculosis The new command becomes: bactopia datasets datasets/ --species species.txt --include_genus This will setup the MLST schema (if available) and a protein cluster FASTA file for each species in species.txt . Usage \u00b6 usage: setup-datasets [-h] [--ariba STR] [--species STR] [--skip_prokka] [--include_genus] [--identity FLOAT] [--overlap FLOAT] [--max_memory INT] [--fast_cluster] [--skip_minmer] [--skip_plsdb] [--cpus INT] [--clear_cache] [--force] [--force_ariba] [--force_mlst] [--force_prokka] [--force_minmer] [--force_plsdb] [--keep_files] [--list_datasets] [--depends] [--version] [--verbose] [--silent] OUTPUT_DIRECTORY setup-datasets - Setup public datasets for Bactopia positional arguments: OUTPUT_DIRECTORY Directory to write output. optional arguments: -h, --help show this help message and exit Ariba Reference Datasets: --ariba STR Setup Ariba datasets for a given reference or a list of references in a text file. (Default: card,vfdb_core) Bacterial Species: --species STR Download available (cg)MLST schemas and completed genomes for a given species or a list of species in a text file. Custom Prokka Protein FASTA: --skip_prokka Skip creation of a Prokka formatted fasta for each species --include_genus Include all genus members in the Prokka proteins FASTA --identity FLOAT CD-HIT (-c) sequence identity threshold. (Default: 0.9) --overlap FLOAT CD-HIT (-s) length difference cutoff. (Default: 0.8) --max_memory INT CD-HIT (-M) memory limit (in MB). (Default: unlimited --fast_cluster Use CD-HIT's (-g 0) fast clustering algorithm, instead of the accurate but slow algorithm. Minmer Datasets: --skip_minmer Skip download of pre-computed minmer datasets (mash, sourmash) PLSDB (Plasmid) BLAST/Sketch: --skip_plsdb Skip download of pre-computed PLSDB datbases (blast, mash) Helpful Options: --cpus INT Number of cpus to use. (Default: 1) --clear_cache Remove any existing cache. --force Forcibly overwrite existing datasets. --force_ariba Forcibly overwrite existing Ariba datasets. --force_mlst Forcibly overwrite existing MLST datasets. --force_prokka Forcibly overwrite existing Prokka datasets. --force_minmer Forcibly overwrite existing minmer datasets. --force_plsdb Forcibly overwrite existing PLSDB datasets. --keep_files Keep all downloaded and intermediate files. --list_datasets List Ariba reference datasets and (cg)MLST schemas available for setup. --depends Verify dependencies are installed. Adjust Verbosity: --version show program's version number and exit --verbose Print debug related text. --silent Only critical errors will be printed. example usage: setup-public-datasets.py outdir setup-public-datasets.py outdir --ariba 'card' setup-public-datasets.py outdir --species 'Staphylococcus aureus' --include_genus Useful Parameters \u00b6 --clear_cache \u00b6 To determine which MLST schemas are available, PubMLST.org is queryied. To prevent a query every run, a list of available schemas is cached to $HOME/.bactopia/datasets.json . The cache expires after 15 days, but in case a new species has been made available --clear_cache will force a requery of PubMLST.org. --cpus \u00b6 Increasing --cpus (it defaults to 1) is useful for speeding up the download and clustering steps. --force* \u00b6 If a dataset exists, it will only be overwritten if one of the --force parameters are used. --include_genus \u00b6 Completed RefSeq genomes are downloaded for a given species to be used for protein clustering. --include_genus will also download completed RefSeq genomes for each genus member. --keep_files \u00b6 Many intermediate files are downloaded/created (e.g. completed genomes) and deleted during the building process, use --keep_files to retain these files. Tweaking CD-HIT \u00b6 There are parameters ( --identity , --overlap , --max_memory , and --fast_cluster ) to tweak CD-HIT if you find it necessary. Please keep in mind, the only goal of the protein clustering step is to help speed up Prokka, by providing a decent set of proteins to annotate against first. User Populated Folders \u00b6 Built into the Bactopia dataset structure are folders that you, the user, can populate for species specific analysis. These could include specific genes you want BLASTed against your samples or a specific reference you want all your samples mapped to and variants called. Directory Structure \u00b6 When a species specific dataset is created, a folder named optional is also created. For example, a S. aureus specific dataset will have the following directory structure: datasets/species-specific/staphylococcus-aureus/ \u251c\u2500\u2500 annotation \u251c\u2500\u2500 minmer \u251c\u2500\u2500 mlst \u2514\u2500\u2500 optional \u251c\u2500\u2500 blast \u2502 \u251c\u2500\u2500 genes \u2502 \u251c\u2500\u2500 primers \u2502 \u2514\u2500\u2500 proteins \u251c\u2500\u2500 insertion-sequences \u251c\u2500\u2500 mapping-sequences \u2514\u2500\u2500 reference-genomes Within the optional folder are folders that you can added your own data up interest to. BLAST \u00b6 datasets/species-specific/staphylococcus-aureus/ \u2514\u2500\u2500 optional \u2514\u2500\u2500 blast \u251c\u2500\u2500 genes \u251c\u2500\u2500 primers \u2514\u2500\u2500 proteins In the blast directory there are three more directories! The genes folder is where you can place gene seqeunces (nucleotides) in FASTA format to query against assemblies using blastn . The primers folder is where you can place primer sequences (nucleotides) in FASTA format to query against assemblies using blastn , but with primer-specific parameters and cut-offs. Finally, the proteins (as you probably guessed!) is where you can place protein sequnces (amino acids) in FASTA format to query against assemblies using blastp . Insertion Sequences \u00b6 datasets/species-specific/staphylococcus-aureus/ \u2514\u2500\u2500 optional \u2514\u2500\u2500 insertion-sequences In the insertion-sequences directory you can place FASTA files of insertion seqeunces you would like searched for using ISMapper . Mapping \u00b6 datasets/species-specific/staphylococcus-aureus/ \u2514\u2500\u2500 optional \u2514\u2500\u2500 mapping-sequences In the mapping-sequences directory you can place FASTA files of any nucleotide sequence you would like FASTQ reads to be mapped against using BWA . This can be useful if you are interested if whether a certain region or gene is covered or not. Reference Genomes \u00b6 datasets/species-specific/staphylococcus-aureus/ \u2514\u2500\u2500 optional \u2514\u2500\u2500 reference-genomes In the reference-genomes directory you can put a GenBank (preferred!) or FASTA file of a reference genome you would like variants to be called against using Snippy .","title":"Build Datasets"},{"location":"datasets/#build-datasets","text":"Bactopia can make use of many existing public datasets, as well as private datasets. The process of downloading, building, and (or) configuring these datasets for Bactopia has been automated. Highly recommended to complete this step! This step is completely optional, but it is highly recommended that you do not. By skipping this step of setting up public datasets, Bactopia will be limited to analyses like quality control, assembly, and 31-mer counting.","title":"Build Datasets"},{"location":"datasets/#included-datasets","text":"Some datasets included are applicable to all bacterial species and some are specific to a bacterial species. If specified at runtime, Bactopia will recognize the datasets and execute the appropriate analyses.","title":"Included Datasets"},{"location":"datasets/#general","text":"Ariba's getref Reference Datasets Allows reference datasets (resistance, virulence, and plamids) to be automatically downloaded and configured for usage by Ariba RefSeq Mash Sketch ~100,000 genomes and plasmids from NCBI RefSeq, used to give an idea of what is your sequencing data (e.g. Are the sequences what you expected?) GenBank Sourmash Signatures ~87,000 microbial genomes (includes viral and fungal) from NCBI GenBank, also gives an idea of what is your sequencing data. PLSDB Mash Sketch & BLAST Includes meta data/annotations, Mash sketches, and BLAST database files of all plasmids stored in PLSDB.","title":"General"},{"location":"datasets/#species-specific","text":"PubMLST.org MLST Schemas Multi-locus sequence typing (MLST) allelic profiles and seqeunces for a many different bacterial species (and even a few eukaryotes!). Clustered RefSeq Proteins For the given bacterial species, completed RefSeq genomes are downloaded and then the proteins are clustered and formatted for usage with Prokka. Minmer Sketch of RefSeq Genomes Using the completed genomes downloaded for clustering proteins a Mash sketch and Sourmash signature is created for these genomes. These sketches can then be used for automatic selection of reference genomes for variant calling. Optional User Populated Folders A few folders for things such as calling variants, insertion sequences and primers are created that the user can manually populate. More information is available below!","title":"Species Specific"},{"location":"datasets/#setting-up","text":"Included in Bactopia is the setup-datasets.py script (located in the bin folder) to automate the process of downloading and/or building these datasets.","title":"Setting Up"},{"location":"datasets/#quick-start","text":"bactopia datasets datasets/ This will set up Ariba datasets ( card and vfdb_core ), RefSeq Mash sketch, GenBank Sourmash Signatures, and PLSDB in the newly created datasets folder.","title":"Quick Start"},{"location":"datasets/#a-single-bacterial-species","text":"bactopia datasets datasets/ --species \"Haemophilus influenzae\" --include_genus","title":"A Single Bacterial Species"},{"location":"datasets/#multiple-bacterial-species","text":"You can also set up datasets for multiple bacterial species at a time. There are two options to do so.","title":"Multiple Bacterial Species"},{"location":"datasets/#comma-separated","text":"At runtime, you can separate the the different species bactopia datasets datasets/ --species \"Haemophilus influenzae,Staphylococcus aureus\" --include_genus","title":"Comma-Separated"},{"location":"datasets/#text-file","text":"In order to do so, you will need to create a text file where each line is the name of a species to set up. For example, you could create a species.txt file and include the following species in it. Haemophilus influenzae Staphylococcus aureus Mycobacterium tuberculosis The new command becomes: bactopia datasets datasets/ --species species.txt --include_genus This will setup the MLST schema (if available) and a protein cluster FASTA file for each species in species.txt .","title":"Text File"},{"location":"datasets/#usage","text":"usage: setup-datasets [-h] [--ariba STR] [--species STR] [--skip_prokka] [--include_genus] [--identity FLOAT] [--overlap FLOAT] [--max_memory INT] [--fast_cluster] [--skip_minmer] [--skip_plsdb] [--cpus INT] [--clear_cache] [--force] [--force_ariba] [--force_mlst] [--force_prokka] [--force_minmer] [--force_plsdb] [--keep_files] [--list_datasets] [--depends] [--version] [--verbose] [--silent] OUTPUT_DIRECTORY setup-datasets - Setup public datasets for Bactopia positional arguments: OUTPUT_DIRECTORY Directory to write output. optional arguments: -h, --help show this help message and exit Ariba Reference Datasets: --ariba STR Setup Ariba datasets for a given reference or a list of references in a text file. (Default: card,vfdb_core) Bacterial Species: --species STR Download available (cg)MLST schemas and completed genomes for a given species or a list of species in a text file. Custom Prokka Protein FASTA: --skip_prokka Skip creation of a Prokka formatted fasta for each species --include_genus Include all genus members in the Prokka proteins FASTA --identity FLOAT CD-HIT (-c) sequence identity threshold. (Default: 0.9) --overlap FLOAT CD-HIT (-s) length difference cutoff. (Default: 0.8) --max_memory INT CD-HIT (-M) memory limit (in MB). (Default: unlimited --fast_cluster Use CD-HIT's (-g 0) fast clustering algorithm, instead of the accurate but slow algorithm. Minmer Datasets: --skip_minmer Skip download of pre-computed minmer datasets (mash, sourmash) PLSDB (Plasmid) BLAST/Sketch: --skip_plsdb Skip download of pre-computed PLSDB datbases (blast, mash) Helpful Options: --cpus INT Number of cpus to use. (Default: 1) --clear_cache Remove any existing cache. --force Forcibly overwrite existing datasets. --force_ariba Forcibly overwrite existing Ariba datasets. --force_mlst Forcibly overwrite existing MLST datasets. --force_prokka Forcibly overwrite existing Prokka datasets. --force_minmer Forcibly overwrite existing minmer datasets. --force_plsdb Forcibly overwrite existing PLSDB datasets. --keep_files Keep all downloaded and intermediate files. --list_datasets List Ariba reference datasets and (cg)MLST schemas available for setup. --depends Verify dependencies are installed. Adjust Verbosity: --version show program's version number and exit --verbose Print debug related text. --silent Only critical errors will be printed. example usage: setup-public-datasets.py outdir setup-public-datasets.py outdir --ariba 'card' setup-public-datasets.py outdir --species 'Staphylococcus aureus' --include_genus","title":"Usage"},{"location":"datasets/#useful-parameters","text":"","title":"Useful Parameters"},{"location":"datasets/#-clear_cache","text":"To determine which MLST schemas are available, PubMLST.org is queryied. To prevent a query every run, a list of available schemas is cached to $HOME/.bactopia/datasets.json . The cache expires after 15 days, but in case a new species has been made available --clear_cache will force a requery of PubMLST.org.","title":"--clear_cache"},{"location":"datasets/#-cpus","text":"Increasing --cpus (it defaults to 1) is useful for speeding up the download and clustering steps.","title":"--cpus"},{"location":"datasets/#-force","text":"If a dataset exists, it will only be overwritten if one of the --force parameters are used.","title":"--force*"},{"location":"datasets/#-include_genus","text":"Completed RefSeq genomes are downloaded for a given species to be used for protein clustering. --include_genus will also download completed RefSeq genomes for each genus member.","title":"--include_genus"},{"location":"datasets/#-keep_files","text":"Many intermediate files are downloaded/created (e.g. completed genomes) and deleted during the building process, use --keep_files to retain these files.","title":"--keep_files"},{"location":"datasets/#tweaking-cd-hit","text":"There are parameters ( --identity , --overlap , --max_memory , and --fast_cluster ) to tweak CD-HIT if you find it necessary. Please keep in mind, the only goal of the protein clustering step is to help speed up Prokka, by providing a decent set of proteins to annotate against first.","title":"Tweaking CD-HIT"},{"location":"datasets/#user-populated-folders","text":"Built into the Bactopia dataset structure are folders that you, the user, can populate for species specific analysis. These could include specific genes you want BLASTed against your samples or a specific reference you want all your samples mapped to and variants called.","title":"User Populated Folders"},{"location":"datasets/#directory-structure","text":"When a species specific dataset is created, a folder named optional is also created. For example, a S. aureus specific dataset will have the following directory structure: datasets/species-specific/staphylococcus-aureus/ \u251c\u2500\u2500 annotation \u251c\u2500\u2500 minmer \u251c\u2500\u2500 mlst \u2514\u2500\u2500 optional \u251c\u2500\u2500 blast \u2502 \u251c\u2500\u2500 genes \u2502 \u251c\u2500\u2500 primers \u2502 \u2514\u2500\u2500 proteins \u251c\u2500\u2500 insertion-sequences \u251c\u2500\u2500 mapping-sequences \u2514\u2500\u2500 reference-genomes Within the optional folder are folders that you can added your own data up interest to.","title":"Directory Structure"},{"location":"datasets/#blast","text":"datasets/species-specific/staphylococcus-aureus/ \u2514\u2500\u2500 optional \u2514\u2500\u2500 blast \u251c\u2500\u2500 genes \u251c\u2500\u2500 primers \u2514\u2500\u2500 proteins In the blast directory there are three more directories! The genes folder is where you can place gene seqeunces (nucleotides) in FASTA format to query against assemblies using blastn . The primers folder is where you can place primer sequences (nucleotides) in FASTA format to query against assemblies using blastn , but with primer-specific parameters and cut-offs. Finally, the proteins (as you probably guessed!) is where you can place protein sequnces (amino acids) in FASTA format to query against assemblies using blastp .","title":"BLAST"},{"location":"datasets/#insertion-sequences","text":"datasets/species-specific/staphylococcus-aureus/ \u2514\u2500\u2500 optional \u2514\u2500\u2500 insertion-sequences In the insertion-sequences directory you can place FASTA files of insertion seqeunces you would like searched for using ISMapper .","title":"Insertion Sequences"},{"location":"datasets/#mapping","text":"datasets/species-specific/staphylococcus-aureus/ \u2514\u2500\u2500 optional \u2514\u2500\u2500 mapping-sequences In the mapping-sequences directory you can place FASTA files of any nucleotide sequence you would like FASTQ reads to be mapped against using BWA . This can be useful if you are interested if whether a certain region or gene is covered or not.","title":"Mapping"},{"location":"datasets/#reference-genomes","text":"datasets/species-specific/staphylococcus-aureus/ \u2514\u2500\u2500 optional \u2514\u2500\u2500 reference-genomes In the reference-genomes directory you can put a GenBank (preferred!) or FASTA file of a reference genome you would like variants to be called against using Snippy .","title":"Reference Genomes"},{"location":"examples/","text":"Examples \u00b6 Example Use Cases of Bactopia Replicating Staphopia \u00b6","title":"Examples"},{"location":"examples/#examples","text":"Example Use Cases of Bactopia","title":"Examples"},{"location":"examples/#replicating-staphopia","text":"","title":"Replicating Staphopia"},{"location":"faq/","text":"FAQ \u00b6 How do I ...","title":"FAQ"},{"location":"faq/#faq","text":"How do I ...","title":"FAQ"},{"location":"installation/","text":"Installation \u00b6 Bactopia has a a lot of tools built into its workflow. As you can imagine, all these tools lead to numerous dependencies, and navigating dependencies can often turn into a very frustrating process. With this in mind, from the onset Bactopia was developed to only include programs that are installable using Conda . Conda is an open source package management system and environment management system that runs on Windows, macOS and Linux. In other words, it makes it super easy to get the tools you need installed! The official Conda documentation is a good starting point for getting started with Conda. Bactopia has been tested using the Miniconda installer , but the Anaconda installer should work the same. A Docker container based on the Bioconda install is also available. Bioconda \u00b6 Once you have Conda all set up, you are ready to create an environment for Bactopia. To do so, you can use the following command: conda create -n bactopia -c conda-forge -c bioconda bactopia After a few minutes you will have a new conda environment suitably named bactopia . To activate this environment, you will can use the following command: conda activate bactopia And voil\u00e0, you are all set to get started processing your data! But first, it is highly recommended that you take the time to Build Datasets that Bactopia can take advantage of. Container \u00b6 A Docker and Singularity container has been created that is based off the Conda install. # Docker docker pull bactopia/bactopia # Singularity singularity pull library://rpetit3/bactopia/bactopia","title":"Installation"},{"location":"installation/#installation","text":"Bactopia has a a lot of tools built into its workflow. As you can imagine, all these tools lead to numerous dependencies, and navigating dependencies can often turn into a very frustrating process. With this in mind, from the onset Bactopia was developed to only include programs that are installable using Conda . Conda is an open source package management system and environment management system that runs on Windows, macOS and Linux. In other words, it makes it super easy to get the tools you need installed! The official Conda documentation is a good starting point for getting started with Conda. Bactopia has been tested using the Miniconda installer , but the Anaconda installer should work the same. A Docker container based on the Bioconda install is also available.","title":"Installation"},{"location":"installation/#bioconda","text":"Once you have Conda all set up, you are ready to create an environment for Bactopia. To do so, you can use the following command: conda create -n bactopia -c conda-forge -c bioconda bactopia After a few minutes you will have a new conda environment suitably named bactopia . To activate this environment, you will can use the following command: conda activate bactopia And voil\u00e0, you are all set to get started processing your data! But first, it is highly recommended that you take the time to Build Datasets that Bactopia can take advantage of.","title":"Bioconda"},{"location":"installation/#container","text":"A Docker and Singularity container has been created that is based off the Conda install. # Docker docker pull bactopia/bactopia # Singularity singularity pull library://rpetit3/bactopia/bactopia","title":"Container"},{"location":"output-overview/","text":"Overview of Bactopia Output \u00b6 After a successful run, Bactopia will have produced numerous output files. Just how many output files depends on the input datasets used (e.g. none, general datasets, species specific datasets, user populated datasets). Here is the complete directory structure that is possible (using all available dataset options) with Bactopia. ${SAMPLE_NAME}/ \u251c\u2500\u2500 annotation \u251c\u2500\u2500 antimicrobial_resistance \u251c\u2500\u2500 ariba \u251c\u2500\u2500 assembly \u251c\u2500\u2500 blast \u251c\u2500\u2500 insertion-sequences \u251c\u2500\u2500 kmers \u251c\u2500\u2500 mapping \u251c\u2500\u2500 minmers \u251c\u2500\u2500 mlst \u251c\u2500\u2500 quality-control \u251c\u2500\u2500 variants \u2514\u2500\u2500 ${SAMPLE_NAME}-genome-size.txt For each type of analysis in Bactopia, a separate directory is created to hold the results. All samples processed by Bactopia will have this directory structure. The only difference is the usage of ${SAMPLE_NAME} as a prefix for naming some output files. Directories \u00b6 The following sections include a list of expected outputs as well as a brief description of each output file. There are instances where additional files (e.g. --keep_all_files and --ariba_noclean ) may be encountered. These files aren't described below, just the defaults. Also, using --compress will add a gz extension, but the original extension is maintained and its description still applies. Developer Descriptions Take Priority If a developer described their software's outputs, their description was used with a link back to the software's documentation (major thanks for taking the time to do that!). In some cases there may have been slight formatting modifications made. In any case, if descriptions are not original credit will be properly given to the source. annotation \u00b6 The annotation directory will contain the outputs from Prokka annotation. These outputs include FASTA (proteins and genes), GFF3, GenBank, and many more. By default the included Prokka databases are used for annotation. However, if a Species Specific Dataset was a created the RefSeq clustered proteins are used first for annotation. File descriptions were directly taken from Prokka's Output Files section and slight modifications were made to the order of rows. ${SAMPLE_NAME}/ \u2514\u2500\u2500 annotation \u251c\u2500\u2500 ${SAMPLE_NAME}.err \u251c\u2500\u2500 ${SAMPLE_NAME}.faa \u251c\u2500\u2500 ${SAMPLE_NAME}.ffn \u251c\u2500\u2500 ${SAMPLE_NAME}.fna \u251c\u2500\u2500 ${SAMPLE_NAME}.fsa \u251c\u2500\u2500 ${SAMPLE_NAME}.gbk \u251c\u2500\u2500 ${SAMPLE_NAME}.gff \u251c\u2500\u2500 ${SAMPLE_NAME}.log \u251c\u2500\u2500 ${SAMPLE_NAME}.sqn \u251c\u2500\u2500 ${SAMPLE_NAME}.tbl \u251c\u2500\u2500 ${SAMPLE_NAME}.tsv \u2514\u2500\u2500 ${SAMPLE_NAME}.txt Extension Description .err Unacceptable annotations - the NCBI discrepancy report. .faa Protein FASTA file of the translated CDS sequences. .ffn Nucleotide FASTA file of all the prediction transcripts (CDS, rRNA, tRNA, tmRNA, misc_RNA) .fna Nucleotide FASTA file of the input contig sequences. .fsa Nucleotide FASTA file of the input contig sequences, used by \"tbl2asn\" to create the .sqn file. It is mostly the same as the .fna file, but with extra Sequin tags in the sequence description lines. .gbk This is a standard GenBank file derived from the master .gff. If the input to prokka was a multi-FASTA, then this will be a multi-GenBank, with one record for each sequence. .gff This is the master annotation in GFF3 format, containing both sequences and annotations. It can be viewed directly in Artemis or IGV. .log Contains all the output that Prokka produced during its run. This is a record of what settings you used. .sqn An ASN1 format \"Sequin\" file for submission to GenBank. It needs to be edited to set the correct taxonomy, authors, related publication etc. .tbl Feature Table file, used by \"tbl2asn\" to create the .sqn file. .tsv Tab-separated file of all features: locus_tag,ftype,len_bp,gene,EC_number,COG,product .txt Statistics relating to the annotated features found. antimicrobial_resistance \u00b6 The antimicrobial_resistance directory will contain the output from NCBI's AMRFinderPlus . The results of AMRFinderPlus using genes as and input, and proteins as an input are available. More information about the output format is available from the AMRFinderPlus Wiki . ${SAMPLE_NAME}/ \u2514\u2500\u2500 antimicrobial_resistance/ \u251c\u2500\u2500 ${SAMPLE_NAME}-gene-report.txt \u2514\u2500\u2500 ${SAMPLE_NAME}-protein-report.txt Extension Description -gene-report.txt Results of using gene sequences as an input -protein-report.txt Results of using protein sequences as an input ariba \u00b6 The ariba directory will contain the results of any Ariba analysis (excluding MLST). Only the Ariba databases created during the dataset setup are used for analysis. For each Ariba database (e.g. card or vfdb ), a separate folder with the name of the database is included in the ariba folder. The file descriptions below were modified from Ariba's wiki entries for run and summary . ${SAMPLE_NAME}/ \u2514\u2500\u2500 ariba \u2514\u2500\u2500 ARIBA_DATABASE_NAME \u251c\u2500\u2500 assembled_genes.fa.gz \u251c\u2500\u2500 assembled_seqs.fa.gz \u251c\u2500\u2500 assemblies.fa.gz \u251c\u2500\u2500 debug.report.tsv \u251c\u2500\u2500 log.clusters.gz \u251c\u2500\u2500 report.tsv \u251c\u2500\u2500 summary.csv \u2514\u2500\u2500 version_info.txt Filename Description assembled_genes.fa.gz A gzipped FASTA file of only assembled gene sequences (with extensions). assembled_seqs.fa.gz A gzipped FASTA of the assembled sequences (genes and non-coding). assemblies.fa.gz A gzipped FASTA file of the assemblies (complete, unedited, contigs). debug.report.tsv The complete list of clusters, including those that did not pass filtering. log.clusters.gz Detailed logging for the progress of each cluster. report.tsv A detailed report file of clusters which passed filtering. summary.csv A more condensed summary of the report.tsv version_info.txt Information on the versions of ARIBA and its dependencies at runtime. assembly \u00b6 The assembly folder contains the results of the sample's assembly. Assembly is managed by Shovill and by default SKESA is used for assembly. Alternative assemblers include SPAdes , MEGAHIT , and Velvet . Depending on the choice of assembler, additional output files (e.g. assembly graphs) may be given. Files descriptions with some modifications were directly taken from Shovill's Output Files section as well as the FLASH usage . ${SAMPLE_NAME}/ \u2514\u2500\u2500 assembly \u251c\u2500\u2500 flash.hist \u251c\u2500\u2500 flash.histogram \u251c\u2500\u2500 shovill.corrections \u251c\u2500\u2500 shovill.log \u251c\u2500\u2500 ${SAMPLE_NAME}.fna \u2514\u2500\u2500 ${SAMPLE_NAME}.fna.json Filename Description flash.hist Numeric histogram of merged read lengths. flash.histogram Visual histogram of merged read lengths. shovill.log Full log file for bug reporting shovill.corrections List of post-assembly corrections ${SAMPLE_NAME}.fna The final assembly you should use ${SAMPLE_NAME}.fna.json Summary statistics of the assembly blast \u00b6 The blast directory contains the BLAST results and a BLAST database of the sample assembly. Each of the User Populated BLAST Sequences (gene, primer, or protein) is BLASTed against the sample assembly. Also if setup , annotated genes are BLASTed against the PLSDB BLAST database. By default, results are returned in tabular format. ${SAMPLE_NAME}/ \u2514\u2500\u2500 blast \u251c\u2500\u2500 blastdb \u2502 \u251c\u2500\u2500 ${SAMPLE_NAME}.nhr \u2502 \u251c\u2500\u2500 ${SAMPLE_NAME}.nin \u2502 \u2514\u2500\u2500 ${SAMPLE_NAME}.nsq \u251c\u2500\u2500 genes \u2502 \u2514\u2500\u2500 ${INPUT_NAME}.txt \u251c\u2500\u2500 primers \u2502 \u2514\u2500\u2500 ${INPUT_NAME}.txt \u251c\u2500\u2500 proteins \u2502 \u2514\u2500\u2500 ${INPUT_NAME}.txt \u2514\u2500\u2500 ${SAMPLE_NAME}-plsdb.txt Extension Description .nhr Sample assembly BLAST database header file .nin Sample assembly BLAST database index file .nsq Sample assembly BLAST database sequence file -plsdb.txt The BLAST results against the PLSDB BALST database assembly. .txt The BLAST results of user input sequence(s) against the sample assembly. genome-size \u00b6 For every sample ${SAMPLE_NAME}-genome-size.txt file is created. This file contains the genome size that was used for analysis. Genome size is used throughout Bactopia for various tasks including error correction, subsampling, and assembly. By default, the genome size is estimated with Mash, but users have the option to provide their own value or completely disable genome size related features. Learn more about changing the genome size at Basic Usage - Genome Size insertion-sequences \u00b6 The insertion-sequences directory contains ISMapper results for each of the User Populated Insertion Sequences . ${SAMPLE_NAME}/ \u2514\u2500\u2500 insertion-sequences \u251c\u2500\u2500 ${INSERTION_NAME} \u2502 \u251c\u2500\u2500 ${SAMPLE_NAME}_${INSERTION_NAME}_(left|right)_final.fastq \u2502 \u251c\u2500\u2500 ${SAMPLE_NAME}_(left|right)_${SAMPLE_NAME}_${CONTIG_NUMBER}_finalcov.bed \u2502 \u251c\u2500\u2500 ${SAMPLE_NAME}_(left|right)_${SAMPLE_NAME}_${CONTIG_NUMBER}_merged.sorted.bed \u2502 \u251c\u2500\u2500 ${SAMPLE_NAME}_(left|right)_${SAMPLE_NAME}_${CONTIG_NUMBER}.sorted.bam \u2502 \u251c\u2500\u2500 ${SAMPLE_NAME}_(left|right)_${SAMPLE_NAME}_${CONTIG_NUMBER}.sorted.bam.bai \u2502 \u251c\u2500\u2500 ${SAMPLE_NAME}_(left|right)_${SAMPLE_NAME}_${CONTIG_NUMBER}_unpaired.bed \u2502 \u251c\u2500\u2500 ${SAMPLE_NAME}__${SAMPLE_NAME}_${CONTIG_NUMBER}_closest.bed \u2502 \u251c\u2500\u2500 ${SAMPLE_NAME}__${SAMPLE_NAME}_${CONTIG_NUMBER}_intersect.bed \u2502 \u251c\u2500\u2500 ${SAMPLE_NAME}__${SAMPLE_NAME}_${CONTIG_NUMBER}_table.txt \u2514\u2500\u2500 ${SAMPLE_NAME}-${INSERTION_NAME}.log Extension Description _final.fastq Sequences (FASTQ format) that mapped to the flanking regions of the IS query _finalcov.bed Contains information about the coverage of the IS query _merged.sorted.bed Merged overlapping regions that passed coverage cutoffs .sorted.bam Reads mapped to the IS query. .sorted.bam.bai An index of the sorted BAM file. _unpaired.bed All unpaired mappings to the IS query _closest.bed Merged regions that are close but do not overlap _intersect.bed An intersection of merged regions from the left and right flanks. _table.txt A detailed description of the IS query results. .log Information logged during the execution of ISMapper kmers \u00b6 The kmers directory contains McCortex 31-mer counts of the cleaned up FASTQ sequences. ${SAMPLE_NAME}/ \u2514\u2500\u2500 kmers \u2514\u2500\u2500 ${SAMPLE_NAME}.ctx Extension Description .ctx A Cortex graph file of the 31-mer counts mapping \u00b6 The mapping-sequences directory contains BWA (bwa-mem) mapping results for each of the User Populated Mapping Sequences . ${SAMPLE_NAME}/ \u2514\u2500\u2500 mapping \u2514\u2500\u2500 ${MAPPING_INPUT} \u251c\u2500\u2500 ${MAPPING_INPUT}.bam \u2514\u2500\u2500 ${MAPPING_INPUT}.coverage.txt Extension Description .bam The alignments in BAM format. .coverage.txt The per-base coverage of the mapping results minmers \u00b6 The minmers directory contains Mash and Sourmash sketches of the cleaned FASTQs. If setup, it also contains the results of queries against RefSeq, GenBank and PLSDB ${SAMPLE_NAME}/ \u2514\u2500\u2500 minmers \u251c\u2500\u2500 ${SAMPLE_NAME}-genbank-k21.txt \u251c\u2500\u2500 ${SAMPLE_NAME}-genbank-k31.txt \u251c\u2500\u2500 ${SAMPLE_NAME}-genbank-k51.txt \u251c\u2500\u2500 ${SAMPLE_NAME}-k21.msh \u251c\u2500\u2500 ${SAMPLE_NAME}-k31.msh \u251c\u2500\u2500 ${SAMPLE_NAME}-plsdb-k21.txt \u251c\u2500\u2500 ${SAMPLE_NAME}-refseq-k21.txt \u2514\u2500\u2500 ${SAMPLE_NAME}.sig Extension Description -genbank-k(21|31|51).txt Sourmash LCA Gather results against Sourmash GenBank Signature (k=21,31,51) -k(21|31).msh A Mash sketch (k=21,31) of the sample -plsdb-k21.txt Mash Screen results against PLSDB Mash Sketch -refseq-k21.txt Mash Screen results against Mash Refseq Sketch .sig A Sourmash signature (k=21,31,51) of the sample mlst \u00b6 If a Species Specific Dataset has been set up, the mlst directory will contain Ariba and BLAST results for a PubMLST.org schema. ${SAMPLE_NAME}/ \u2514\u2500\u2500 mlst \u251c\u2500\u2500 ariba \u2502 \u251c\u2500\u2500 assembled_genes.fa.gz \u2502 \u251c\u2500\u2500 assembled_seqs.fa.gz \u2502 \u251c\u2500\u2500 assemblies.fa.gz \u2502 \u251c\u2500\u2500 debug.report.tsv \u2502 \u251c\u2500\u2500 log.clusters.gz \u2502 \u251c\u2500\u2500 mlst_report.details.tsv \u2502 \u251c\u2500\u2500 mlst_report.tsv \u2502 \u251c\u2500\u2500 report.tsv \u2502 \u2514\u2500\u2500 version_info.txt \u2514\u2500\u2500 blast \u2514\u2500\u2500 ${SAMPLE_NAME}-blast.json Filename Description assembled_genes.fa.gz A gzipped FASTA file of only assembled gene sequences (with extensions). assembled_seqs.fa.gz A gzipped FASTA of the assembled sequences (genes and non-coding). assemblies.fa.gz A gzipped FASTA file of the assemblies (complete, unedited, contigs). debug.report.tsv The complete list of clusters, including those that did not pass filtering. log.clusters.gz Detailed logging for the progress of each cluster. mlst_report.details.tsv A more detailed summary of the allele calls. mlst_report.tsv A summary of the allele calls and identified sequence type. report.tsv A detailed report file of clusters which passed filtering. summary.csv A more condensed summary of the report.tsv version_info.txt Information on the versions of ARIBA and its dependencies at runtime. -blast.json Allele calls and identified sequence type based on BLAST quality-control \u00b6 The quality-control directory contains the cleaned up FASTQs ( BBTools and Lighter ) and summary statitics ( FastQC and Fastq-Scan ) before and after cleanup. ${SAMPLE_NAME}/ \u2514\u2500\u2500 quality-control \u251c\u2500\u2500 logs \u2502 \u251c\u2500\u2500 bbduk-adapter.log \u2502 \u2514\u2500\u2500 bbduk-phix.log \u251c\u2500\u2500 ${SAMPLE_NAME}(|_R1|_R2).fastq.gz \u2514\u2500\u2500 summary-(original|final) \u251c\u2500\u2500 ${SAMPLE_NAME}(|_R1|_R2)-(original|final)_fastqc.html \u251c\u2500\u2500 ${SAMPLE_NAME}(|_R1|_R2)-(original|final)_fastqc.zip \u2514\u2500\u2500 ${SAMPLE_NAME}(|_R1|_R2)-(original|final).json Extension Description -adapter.log A description of how many reads were filtered during the adapter removal step -phix.log A description of how many reads were filtered during the PhiX removal step .fastq.gz The cleaned up FASTQ(s), _R1 and _R2 for paired-end reads, and an empty string for single-end reads. _fastqc.html The FastQC html report of the original and final FASTQ(s) _fastqc.zip The zipped FastQC full report of the original and final FASTQ(s) .json Summary statistics (e.g. qualities and read lengths) of the original and final FASTQ(s) variants \u00b6 The variants directory contains the results of Snippy variant calls against one or more reference genomes. There are two subdirectories auto and user . The auto directory includes variants calls against automatically selected reference genome(s) based on nearest Mash distance to RefSeq completed genomes. This process only happens if a Species Specific Dataset was a created. By default, only a single reference genome (nearest) is selected. This feature can be disabled ( --disable_auto_variants ) or the number of genomes changed ( --max_references INT ). The user directory contains variant calls against for each of the User Populated Reference Genomes . The following description of files was directly taken from Snippy's Output Files section. Slight modifications were made to the order of rows. ${SAMPLE_NAME}/ \u2514\u2500\u2500 variants \u2514\u2500\u2500 (auto|user) \u2514\u2500\u2500 ${REFERENCE_NAME} \u251c\u2500\u2500 ${SAMPLE_NAME}.aligned.fa \u251c\u2500\u2500 ${SAMPLE_NAME}.annotated.vcf \u251c\u2500\u2500 ${SAMPLE_NAME}.bam \u251c\u2500\u2500 ${SAMPLE_NAME}.bam.bai \u251c\u2500\u2500 ${SAMPLE_NAME}.bed \u251c\u2500\u2500 ${SAMPLE_NAME}.consensus.fa \u251c\u2500\u2500 ${SAMPLE_NAME}.consensus.subs.fa \u251c\u2500\u2500 ${SAMPLE_NAME}.consensus.subs.masked.fa \u251c\u2500\u2500 ${SAMPLE_NAME}.coverage.txt \u251c\u2500\u2500 ${SAMPLE_NAME}.csv \u251c\u2500\u2500 ${SAMPLE_NAME}.filt.vcf \u251c\u2500\u2500 ${SAMPLE_NAME}.gff \u251c\u2500\u2500 ${SAMPLE_NAME}.html \u251c\u2500\u2500 ${SAMPLE_NAME}.log \u251c\u2500\u2500 ${SAMPLE_NAME}.raw.vcf \u251c\u2500\u2500 ${SAMPLE_NAME}.subs.vcf \u251c\u2500\u2500 ${SAMPLE_NAME}.tab \u251c\u2500\u2500 ${SAMPLE_NAME}.txt \u2514\u2500\u2500 ${SAMPLE_NAME}.vcf Extension Description .aligned.fa A version of the reference but with - at position with depth=0 and N for 0 < depth < --mincov ( does not have variants ) .annotated.vcf The final variant calls with additional annotations from Reference genome's GenBank file .bam The alignments in BAM format. Includes unmapped, multimapping reads. Excludes duplicates. .bam.bai Index for the .bam file .bed The variants in BED format .consensus.fa A version of the reference genome with all variants instantiated .consensus.subs.fa A version of the reference genome with only substitution variants instantiated .consensus.subs.masked.fa A version of the reference genome with only substitution variants instantiated and low-coverage regions masked .coverage.txt The per-base coverage of each position in the reference genome .csv A comma-separated version of the .tab file .filt.vcf The filtered variant calls from Freebayes .gff The variants in GFF3 format .html A HTML version of the .tab file .log A log file with the commands run and their outputs .raw.vcf The unfiltered variant calls from Freebayes .subs.vcf Only substitution variants from the final annotated variants .tab A simple tab-separated summary of all the variants .txt A summary of the Snippy run. .vcf The final annotated variants in VCF format","title":"Output Overview"},{"location":"output-overview/#overview-of-bactopia-output","text":"After a successful run, Bactopia will have produced numerous output files. Just how many output files depends on the input datasets used (e.g. none, general datasets, species specific datasets, user populated datasets). Here is the complete directory structure that is possible (using all available dataset options) with Bactopia. ${SAMPLE_NAME}/ \u251c\u2500\u2500 annotation \u251c\u2500\u2500 antimicrobial_resistance \u251c\u2500\u2500 ariba \u251c\u2500\u2500 assembly \u251c\u2500\u2500 blast \u251c\u2500\u2500 insertion-sequences \u251c\u2500\u2500 kmers \u251c\u2500\u2500 mapping \u251c\u2500\u2500 minmers \u251c\u2500\u2500 mlst \u251c\u2500\u2500 quality-control \u251c\u2500\u2500 variants \u2514\u2500\u2500 ${SAMPLE_NAME}-genome-size.txt For each type of analysis in Bactopia, a separate directory is created to hold the results. All samples processed by Bactopia will have this directory structure. The only difference is the usage of ${SAMPLE_NAME} as a prefix for naming some output files.","title":"Overview of Bactopia Output"},{"location":"output-overview/#directories","text":"The following sections include a list of expected outputs as well as a brief description of each output file. There are instances where additional files (e.g. --keep_all_files and --ariba_noclean ) may be encountered. These files aren't described below, just the defaults. Also, using --compress will add a gz extension, but the original extension is maintained and its description still applies. Developer Descriptions Take Priority If a developer described their software's outputs, their description was used with a link back to the software's documentation (major thanks for taking the time to do that!). In some cases there may have been slight formatting modifications made. In any case, if descriptions are not original credit will be properly given to the source.","title":"Directories"},{"location":"output-overview/#annotation","text":"The annotation directory will contain the outputs from Prokka annotation. These outputs include FASTA (proteins and genes), GFF3, GenBank, and many more. By default the included Prokka databases are used for annotation. However, if a Species Specific Dataset was a created the RefSeq clustered proteins are used first for annotation. File descriptions were directly taken from Prokka's Output Files section and slight modifications were made to the order of rows. ${SAMPLE_NAME}/ \u2514\u2500\u2500 annotation \u251c\u2500\u2500 ${SAMPLE_NAME}.err \u251c\u2500\u2500 ${SAMPLE_NAME}.faa \u251c\u2500\u2500 ${SAMPLE_NAME}.ffn \u251c\u2500\u2500 ${SAMPLE_NAME}.fna \u251c\u2500\u2500 ${SAMPLE_NAME}.fsa \u251c\u2500\u2500 ${SAMPLE_NAME}.gbk \u251c\u2500\u2500 ${SAMPLE_NAME}.gff \u251c\u2500\u2500 ${SAMPLE_NAME}.log \u251c\u2500\u2500 ${SAMPLE_NAME}.sqn \u251c\u2500\u2500 ${SAMPLE_NAME}.tbl \u251c\u2500\u2500 ${SAMPLE_NAME}.tsv \u2514\u2500\u2500 ${SAMPLE_NAME}.txt Extension Description .err Unacceptable annotations - the NCBI discrepancy report. .faa Protein FASTA file of the translated CDS sequences. .ffn Nucleotide FASTA file of all the prediction transcripts (CDS, rRNA, tRNA, tmRNA, misc_RNA) .fna Nucleotide FASTA file of the input contig sequences. .fsa Nucleotide FASTA file of the input contig sequences, used by \"tbl2asn\" to create the .sqn file. It is mostly the same as the .fna file, but with extra Sequin tags in the sequence description lines. .gbk This is a standard GenBank file derived from the master .gff. If the input to prokka was a multi-FASTA, then this will be a multi-GenBank, with one record for each sequence. .gff This is the master annotation in GFF3 format, containing both sequences and annotations. It can be viewed directly in Artemis or IGV. .log Contains all the output that Prokka produced during its run. This is a record of what settings you used. .sqn An ASN1 format \"Sequin\" file for submission to GenBank. It needs to be edited to set the correct taxonomy, authors, related publication etc. .tbl Feature Table file, used by \"tbl2asn\" to create the .sqn file. .tsv Tab-separated file of all features: locus_tag,ftype,len_bp,gene,EC_number,COG,product .txt Statistics relating to the annotated features found.","title":"annotation"},{"location":"output-overview/#antimicrobial_resistance","text":"The antimicrobial_resistance directory will contain the output from NCBI's AMRFinderPlus . The results of AMRFinderPlus using genes as and input, and proteins as an input are available. More information about the output format is available from the AMRFinderPlus Wiki . ${SAMPLE_NAME}/ \u2514\u2500\u2500 antimicrobial_resistance/ \u251c\u2500\u2500 ${SAMPLE_NAME}-gene-report.txt \u2514\u2500\u2500 ${SAMPLE_NAME}-protein-report.txt Extension Description -gene-report.txt Results of using gene sequences as an input -protein-report.txt Results of using protein sequences as an input","title":"antimicrobial_resistance"},{"location":"output-overview/#ariba","text":"The ariba directory will contain the results of any Ariba analysis (excluding MLST). Only the Ariba databases created during the dataset setup are used for analysis. For each Ariba database (e.g. card or vfdb ), a separate folder with the name of the database is included in the ariba folder. The file descriptions below were modified from Ariba's wiki entries for run and summary . ${SAMPLE_NAME}/ \u2514\u2500\u2500 ariba \u2514\u2500\u2500 ARIBA_DATABASE_NAME \u251c\u2500\u2500 assembled_genes.fa.gz \u251c\u2500\u2500 assembled_seqs.fa.gz \u251c\u2500\u2500 assemblies.fa.gz \u251c\u2500\u2500 debug.report.tsv \u251c\u2500\u2500 log.clusters.gz \u251c\u2500\u2500 report.tsv \u251c\u2500\u2500 summary.csv \u2514\u2500\u2500 version_info.txt Filename Description assembled_genes.fa.gz A gzipped FASTA file of only assembled gene sequences (with extensions). assembled_seqs.fa.gz A gzipped FASTA of the assembled sequences (genes and non-coding). assemblies.fa.gz A gzipped FASTA file of the assemblies (complete, unedited, contigs). debug.report.tsv The complete list of clusters, including those that did not pass filtering. log.clusters.gz Detailed logging for the progress of each cluster. report.tsv A detailed report file of clusters which passed filtering. summary.csv A more condensed summary of the report.tsv version_info.txt Information on the versions of ARIBA and its dependencies at runtime.","title":"ariba"},{"location":"output-overview/#assembly","text":"The assembly folder contains the results of the sample's assembly. Assembly is managed by Shovill and by default SKESA is used for assembly. Alternative assemblers include SPAdes , MEGAHIT , and Velvet . Depending on the choice of assembler, additional output files (e.g. assembly graphs) may be given. Files descriptions with some modifications were directly taken from Shovill's Output Files section as well as the FLASH usage . ${SAMPLE_NAME}/ \u2514\u2500\u2500 assembly \u251c\u2500\u2500 flash.hist \u251c\u2500\u2500 flash.histogram \u251c\u2500\u2500 shovill.corrections \u251c\u2500\u2500 shovill.log \u251c\u2500\u2500 ${SAMPLE_NAME}.fna \u2514\u2500\u2500 ${SAMPLE_NAME}.fna.json Filename Description flash.hist Numeric histogram of merged read lengths. flash.histogram Visual histogram of merged read lengths. shovill.log Full log file for bug reporting shovill.corrections List of post-assembly corrections ${SAMPLE_NAME}.fna The final assembly you should use ${SAMPLE_NAME}.fna.json Summary statistics of the assembly","title":"assembly"},{"location":"output-overview/#blast","text":"The blast directory contains the BLAST results and a BLAST database of the sample assembly. Each of the User Populated BLAST Sequences (gene, primer, or protein) is BLASTed against the sample assembly. Also if setup , annotated genes are BLASTed against the PLSDB BLAST database. By default, results are returned in tabular format. ${SAMPLE_NAME}/ \u2514\u2500\u2500 blast \u251c\u2500\u2500 blastdb \u2502 \u251c\u2500\u2500 ${SAMPLE_NAME}.nhr \u2502 \u251c\u2500\u2500 ${SAMPLE_NAME}.nin \u2502 \u2514\u2500\u2500 ${SAMPLE_NAME}.nsq \u251c\u2500\u2500 genes \u2502 \u2514\u2500\u2500 ${INPUT_NAME}.txt \u251c\u2500\u2500 primers \u2502 \u2514\u2500\u2500 ${INPUT_NAME}.txt \u251c\u2500\u2500 proteins \u2502 \u2514\u2500\u2500 ${INPUT_NAME}.txt \u2514\u2500\u2500 ${SAMPLE_NAME}-plsdb.txt Extension Description .nhr Sample assembly BLAST database header file .nin Sample assembly BLAST database index file .nsq Sample assembly BLAST database sequence file -plsdb.txt The BLAST results against the PLSDB BALST database assembly. .txt The BLAST results of user input sequence(s) against the sample assembly.","title":"blast"},{"location":"output-overview/#genome-size","text":"For every sample ${SAMPLE_NAME}-genome-size.txt file is created. This file contains the genome size that was used for analysis. Genome size is used throughout Bactopia for various tasks including error correction, subsampling, and assembly. By default, the genome size is estimated with Mash, but users have the option to provide their own value or completely disable genome size related features. Learn more about changing the genome size at Basic Usage - Genome Size","title":"genome-size"},{"location":"output-overview/#insertion-sequences","text":"The insertion-sequences directory contains ISMapper results for each of the User Populated Insertion Sequences . ${SAMPLE_NAME}/ \u2514\u2500\u2500 insertion-sequences \u251c\u2500\u2500 ${INSERTION_NAME} \u2502 \u251c\u2500\u2500 ${SAMPLE_NAME}_${INSERTION_NAME}_(left|right)_final.fastq \u2502 \u251c\u2500\u2500 ${SAMPLE_NAME}_(left|right)_${SAMPLE_NAME}_${CONTIG_NUMBER}_finalcov.bed \u2502 \u251c\u2500\u2500 ${SAMPLE_NAME}_(left|right)_${SAMPLE_NAME}_${CONTIG_NUMBER}_merged.sorted.bed \u2502 \u251c\u2500\u2500 ${SAMPLE_NAME}_(left|right)_${SAMPLE_NAME}_${CONTIG_NUMBER}.sorted.bam \u2502 \u251c\u2500\u2500 ${SAMPLE_NAME}_(left|right)_${SAMPLE_NAME}_${CONTIG_NUMBER}.sorted.bam.bai \u2502 \u251c\u2500\u2500 ${SAMPLE_NAME}_(left|right)_${SAMPLE_NAME}_${CONTIG_NUMBER}_unpaired.bed \u2502 \u251c\u2500\u2500 ${SAMPLE_NAME}__${SAMPLE_NAME}_${CONTIG_NUMBER}_closest.bed \u2502 \u251c\u2500\u2500 ${SAMPLE_NAME}__${SAMPLE_NAME}_${CONTIG_NUMBER}_intersect.bed \u2502 \u251c\u2500\u2500 ${SAMPLE_NAME}__${SAMPLE_NAME}_${CONTIG_NUMBER}_table.txt \u2514\u2500\u2500 ${SAMPLE_NAME}-${INSERTION_NAME}.log Extension Description _final.fastq Sequences (FASTQ format) that mapped to the flanking regions of the IS query _finalcov.bed Contains information about the coverage of the IS query _merged.sorted.bed Merged overlapping regions that passed coverage cutoffs .sorted.bam Reads mapped to the IS query. .sorted.bam.bai An index of the sorted BAM file. _unpaired.bed All unpaired mappings to the IS query _closest.bed Merged regions that are close but do not overlap _intersect.bed An intersection of merged regions from the left and right flanks. _table.txt A detailed description of the IS query results. .log Information logged during the execution of ISMapper","title":"insertion-sequences"},{"location":"output-overview/#kmers","text":"The kmers directory contains McCortex 31-mer counts of the cleaned up FASTQ sequences. ${SAMPLE_NAME}/ \u2514\u2500\u2500 kmers \u2514\u2500\u2500 ${SAMPLE_NAME}.ctx Extension Description .ctx A Cortex graph file of the 31-mer counts","title":"kmers"},{"location":"output-overview/#mapping","text":"The mapping-sequences directory contains BWA (bwa-mem) mapping results for each of the User Populated Mapping Sequences . ${SAMPLE_NAME}/ \u2514\u2500\u2500 mapping \u2514\u2500\u2500 ${MAPPING_INPUT} \u251c\u2500\u2500 ${MAPPING_INPUT}.bam \u2514\u2500\u2500 ${MAPPING_INPUT}.coverage.txt Extension Description .bam The alignments in BAM format. .coverage.txt The per-base coverage of the mapping results","title":"mapping"},{"location":"output-overview/#minmers","text":"The minmers directory contains Mash and Sourmash sketches of the cleaned FASTQs. If setup, it also contains the results of queries against RefSeq, GenBank and PLSDB ${SAMPLE_NAME}/ \u2514\u2500\u2500 minmers \u251c\u2500\u2500 ${SAMPLE_NAME}-genbank-k21.txt \u251c\u2500\u2500 ${SAMPLE_NAME}-genbank-k31.txt \u251c\u2500\u2500 ${SAMPLE_NAME}-genbank-k51.txt \u251c\u2500\u2500 ${SAMPLE_NAME}-k21.msh \u251c\u2500\u2500 ${SAMPLE_NAME}-k31.msh \u251c\u2500\u2500 ${SAMPLE_NAME}-plsdb-k21.txt \u251c\u2500\u2500 ${SAMPLE_NAME}-refseq-k21.txt \u2514\u2500\u2500 ${SAMPLE_NAME}.sig Extension Description -genbank-k(21|31|51).txt Sourmash LCA Gather results against Sourmash GenBank Signature (k=21,31,51) -k(21|31).msh A Mash sketch (k=21,31) of the sample -plsdb-k21.txt Mash Screen results against PLSDB Mash Sketch -refseq-k21.txt Mash Screen results against Mash Refseq Sketch .sig A Sourmash signature (k=21,31,51) of the sample","title":"minmers"},{"location":"output-overview/#mlst","text":"If a Species Specific Dataset has been set up, the mlst directory will contain Ariba and BLAST results for a PubMLST.org schema. ${SAMPLE_NAME}/ \u2514\u2500\u2500 mlst \u251c\u2500\u2500 ariba \u2502 \u251c\u2500\u2500 assembled_genes.fa.gz \u2502 \u251c\u2500\u2500 assembled_seqs.fa.gz \u2502 \u251c\u2500\u2500 assemblies.fa.gz \u2502 \u251c\u2500\u2500 debug.report.tsv \u2502 \u251c\u2500\u2500 log.clusters.gz \u2502 \u251c\u2500\u2500 mlst_report.details.tsv \u2502 \u251c\u2500\u2500 mlst_report.tsv \u2502 \u251c\u2500\u2500 report.tsv \u2502 \u2514\u2500\u2500 version_info.txt \u2514\u2500\u2500 blast \u2514\u2500\u2500 ${SAMPLE_NAME}-blast.json Filename Description assembled_genes.fa.gz A gzipped FASTA file of only assembled gene sequences (with extensions). assembled_seqs.fa.gz A gzipped FASTA of the assembled sequences (genes and non-coding). assemblies.fa.gz A gzipped FASTA file of the assemblies (complete, unedited, contigs). debug.report.tsv The complete list of clusters, including those that did not pass filtering. log.clusters.gz Detailed logging for the progress of each cluster. mlst_report.details.tsv A more detailed summary of the allele calls. mlst_report.tsv A summary of the allele calls and identified sequence type. report.tsv A detailed report file of clusters which passed filtering. summary.csv A more condensed summary of the report.tsv version_info.txt Information on the versions of ARIBA and its dependencies at runtime. -blast.json Allele calls and identified sequence type based on BLAST","title":"mlst"},{"location":"output-overview/#quality-control","text":"The quality-control directory contains the cleaned up FASTQs ( BBTools and Lighter ) and summary statitics ( FastQC and Fastq-Scan ) before and after cleanup. ${SAMPLE_NAME}/ \u2514\u2500\u2500 quality-control \u251c\u2500\u2500 logs \u2502 \u251c\u2500\u2500 bbduk-adapter.log \u2502 \u2514\u2500\u2500 bbduk-phix.log \u251c\u2500\u2500 ${SAMPLE_NAME}(|_R1|_R2).fastq.gz \u2514\u2500\u2500 summary-(original|final) \u251c\u2500\u2500 ${SAMPLE_NAME}(|_R1|_R2)-(original|final)_fastqc.html \u251c\u2500\u2500 ${SAMPLE_NAME}(|_R1|_R2)-(original|final)_fastqc.zip \u2514\u2500\u2500 ${SAMPLE_NAME}(|_R1|_R2)-(original|final).json Extension Description -adapter.log A description of how many reads were filtered during the adapter removal step -phix.log A description of how many reads were filtered during the PhiX removal step .fastq.gz The cleaned up FASTQ(s), _R1 and _R2 for paired-end reads, and an empty string for single-end reads. _fastqc.html The FastQC html report of the original and final FASTQ(s) _fastqc.zip The zipped FastQC full report of the original and final FASTQ(s) .json Summary statistics (e.g. qualities and read lengths) of the original and final FASTQ(s)","title":"quality-control"},{"location":"output-overview/#variants","text":"The variants directory contains the results of Snippy variant calls against one or more reference genomes. There are two subdirectories auto and user . The auto directory includes variants calls against automatically selected reference genome(s) based on nearest Mash distance to RefSeq completed genomes. This process only happens if a Species Specific Dataset was a created. By default, only a single reference genome (nearest) is selected. This feature can be disabled ( --disable_auto_variants ) or the number of genomes changed ( --max_references INT ). The user directory contains variant calls against for each of the User Populated Reference Genomes . The following description of files was directly taken from Snippy's Output Files section. Slight modifications were made to the order of rows. ${SAMPLE_NAME}/ \u2514\u2500\u2500 variants \u2514\u2500\u2500 (auto|user) \u2514\u2500\u2500 ${REFERENCE_NAME} \u251c\u2500\u2500 ${SAMPLE_NAME}.aligned.fa \u251c\u2500\u2500 ${SAMPLE_NAME}.annotated.vcf \u251c\u2500\u2500 ${SAMPLE_NAME}.bam \u251c\u2500\u2500 ${SAMPLE_NAME}.bam.bai \u251c\u2500\u2500 ${SAMPLE_NAME}.bed \u251c\u2500\u2500 ${SAMPLE_NAME}.consensus.fa \u251c\u2500\u2500 ${SAMPLE_NAME}.consensus.subs.fa \u251c\u2500\u2500 ${SAMPLE_NAME}.consensus.subs.masked.fa \u251c\u2500\u2500 ${SAMPLE_NAME}.coverage.txt \u251c\u2500\u2500 ${SAMPLE_NAME}.csv \u251c\u2500\u2500 ${SAMPLE_NAME}.filt.vcf \u251c\u2500\u2500 ${SAMPLE_NAME}.gff \u251c\u2500\u2500 ${SAMPLE_NAME}.html \u251c\u2500\u2500 ${SAMPLE_NAME}.log \u251c\u2500\u2500 ${SAMPLE_NAME}.raw.vcf \u251c\u2500\u2500 ${SAMPLE_NAME}.subs.vcf \u251c\u2500\u2500 ${SAMPLE_NAME}.tab \u251c\u2500\u2500 ${SAMPLE_NAME}.txt \u2514\u2500\u2500 ${SAMPLE_NAME}.vcf Extension Description .aligned.fa A version of the reference but with - at position with depth=0 and N for 0 < depth < --mincov ( does not have variants ) .annotated.vcf The final variant calls with additional annotations from Reference genome's GenBank file .bam The alignments in BAM format. Includes unmapped, multimapping reads. Excludes duplicates. .bam.bai Index for the .bam file .bed The variants in BED format .consensus.fa A version of the reference genome with all variants instantiated .consensus.subs.fa A version of the reference genome with only substitution variants instantiated .consensus.subs.masked.fa A version of the reference genome with only substitution variants instantiated and low-coverage regions masked .coverage.txt The per-base coverage of each position in the reference genome .csv A comma-separated version of the .tab file .filt.vcf The filtered variant calls from Freebayes .gff The variants in GFF3 format .html A HTML version of the .tab file .log A log file with the commands run and their outputs .raw.vcf The unfiltered variant calls from Freebayes .subs.vcf Only substitution variants from the final annotated variants .tab A simple tab-separated summary of all the variants .txt A summary of the Snippy run. .vcf The final annotated variants in VCF format","title":"variants"},{"location":"quick-start/","text":"Quick Start \u00b6 Here we go! No time to waste, let's get the ball rolling! Why are you still reading this?!? Go! Go! Go! Installation \u00b6 conda create -y -n bactopia -c conda-forge -c bioconda bactopia conda activate bactopia Build Dataset \u00b6 bactopia datasets datasets/ This will build the following datasets: CARD VFDB RefSeq Mash Sketch GenBank Sourmash Signatures PLSDB Mash Sketch & BLAST More information about these datasets is available at Build Datasets . Run Bactopia! \u00b6 Single Sample \u00b6 Paired-End \u00b6 bactopia --R1 ${SAMPLE}_R1.fastq.gz --R2 ${SAMPLE}_R2.fastq.gz --sample ${SAMPLE} \\ --dataset datasets/ --outdir ${OUTDIR} Single-End \u00b6 bactopia --SE ${SAMPLE}.fastq.gz --sample ${SAMPLE} --dataset datasets/ --outdir ${OUTDIR} Multiple Samples \u00b6 bactopia prepare directory-of-fastqs/ > fastqs.txt bactopia --fastqs fastqs.txt --dataset datasets --outdir ${OUTDIR}","title":"Quick Start"},{"location":"quick-start/#quick-start","text":"Here we go! No time to waste, let's get the ball rolling! Why are you still reading this?!? Go! Go! Go!","title":"Quick Start"},{"location":"quick-start/#installation","text":"conda create -y -n bactopia -c conda-forge -c bioconda bactopia conda activate bactopia","title":"Installation"},{"location":"quick-start/#build-dataset","text":"bactopia datasets datasets/ This will build the following datasets: CARD VFDB RefSeq Mash Sketch GenBank Sourmash Signatures PLSDB Mash Sketch & BLAST More information about these datasets is available at Build Datasets .","title":"Build Dataset"},{"location":"quick-start/#run-bactopia","text":"","title":"Run Bactopia!"},{"location":"quick-start/#single-sample","text":"","title":"Single Sample"},{"location":"quick-start/#paired-end","text":"bactopia --R1 ${SAMPLE}_R1.fastq.gz --R2 ${SAMPLE}_R2.fastq.gz --sample ${SAMPLE} \\ --dataset datasets/ --outdir ${OUTDIR}","title":"Paired-End"},{"location":"quick-start/#single-end","text":"bactopia --SE ${SAMPLE}.fastq.gz --sample ${SAMPLE} --dataset datasets/ --outdir ${OUTDIR}","title":"Single-End"},{"location":"quick-start/#multiple-samples","text":"bactopia prepare directory-of-fastqs/ > fastqs.txt bactopia --fastqs fastqs.txt --dataset datasets --outdir ${OUTDIR}","title":"Multiple Samples"},{"location":"troubleshooting/","text":"Troubleshooting Bactopia \u00b6 It was bound to happen, an error has occurred or a bug has shown itself. Now let's see if we can fix it! Don't see your error/bug? Post an issue on GitHub If you've encountered an error or bug not seen here, please post an issue at Bactopia GitHub Issues . This will help greatly to track down the error and fix it! Failed to create Conda Environment \u00b6 Occasionally on the first run of Bactopia you will encounter this error: Caused by: Failed to create Conda environment command: conda env create --prefix /data/apps/bactopia/conda/cache/envs/bactopia-gather_fastqs-8bddd22dc63ec39a71c4ea8fd7704f7a --file /data/apps/bactopia/conda/gather_fastqs.yml status : 1 message: InvalidArchiveError(\"Error with archive /home/rpetit/miniconda3/envs/bactopia/pkgs/python-3.7.3-h33d41f4_1.tar.bz2. You probably need to delete and re-download or re-create this file. Message from libarchive was:\\n\\nFailed to create dir 'lib'\",) While it may look like this is related to Nextflow, it is actually a Conda error that occurs when installing multiple environments at once which Nextflow likes to do. This can also occur from a timeout or loss of internet connectivity (under a different error name). Recommended Solution Using --dry_run at runtime will run Bactopia with dummy data on a single core to prevent parallel creation of conda environments. This process will take only as long as it takes to create the environments. If you have suggestions for how to better handle this, check out the submitted Better handling of conda environments? issue. Your feedback would be greatly appreciated!","title":"Troubleshooting"},{"location":"troubleshooting/#troubleshooting-bactopia","text":"It was bound to happen, an error has occurred or a bug has shown itself. Now let's see if we can fix it! Don't see your error/bug? Post an issue on GitHub If you've encountered an error or bug not seen here, please post an issue at Bactopia GitHub Issues . This will help greatly to track down the error and fix it!","title":"Troubleshooting Bactopia"},{"location":"troubleshooting/#failed-to-create-conda-environment","text":"Occasionally on the first run of Bactopia you will encounter this error: Caused by: Failed to create Conda environment command: conda env create --prefix /data/apps/bactopia/conda/cache/envs/bactopia-gather_fastqs-8bddd22dc63ec39a71c4ea8fd7704f7a --file /data/apps/bactopia/conda/gather_fastqs.yml status : 1 message: InvalidArchiveError(\"Error with archive /home/rpetit/miniconda3/envs/bactopia/pkgs/python-3.7.3-h33d41f4_1.tar.bz2. You probably need to delete and re-download or re-create this file. Message from libarchive was:\\n\\nFailed to create dir 'lib'\",) While it may look like this is related to Nextflow, it is actually a Conda error that occurs when installing multiple environments at once which Nextflow likes to do. This can also occur from a timeout or loss of internet connectivity (under a different error name). Recommended Solution Using --dry_run at runtime will run Bactopia with dummy data on a single core to prevent parallel creation of conda environments. This process will take only as long as it takes to create the environments. If you have suggestions for how to better handle this, check out the submitted Better handling of conda environments? issue. Your feedback would be greatly appreciated!","title":"Failed to create Conda Environment"},{"location":"tutorial/","text":"You should now have a directory named datasets that has all the available datasets to be used by Bactopia.# Tutorial For this tutorial, we will attempt to replicate the Staphopia analysis pipeline with Bactopia. We will use S. aureus samples associated with cystic fibrosis lung infections that were recently published (details below, shameless self plug!) and are available from BioProject accession PRJNA480016 . Bernardy, Eryn E., et al. \"Whole-Genome Sequences of Staphylococcus aureus Isolates from Cystic Fibrosis Lung Infections.\" Microbiol Resour Announc 8.3 (2019): e01564-18. Overall the goal of the tutorial is to: Build datasets Acquire Staphopia datasets Use Bactopia to process: A sample from ENA Multiple samples from ENA Single local sample Multiple local samples using FOFN Bactopia Should Be Installed This tutorial assumes you have already installed Bactopia. If you have not, please check out how to at Installation . Build Datasets \u00b6 First let's create a directory to work in and activate our Bactopia environment. mkdir bactopia-tutorial cd bactopia-tutorial conda activate bactopia Now we are ready to build our datasets! bactopia datasets datasets/ --ariba \"vfdb_core,card\" \\ --species \"Staphylococcus aureus\" \\ --include_genus \\ --cpus 4 Let's review what is happening here. datasets/ is where our datasets will be downloaded, processed and stored. --ariba \"vfdb_core,card\" says to download and setup the VFDB Core and the CARD databases to be used by Ariba. --species \"Staphylococcus aureus\" will download MLST schemas associated with S. aureus it will also download completed S. aureus genomes (RefSeq only) that are used to create a set of protein set for annotation, a Mash sketch for automatic variant calling to the nearest neighbor, and calulate genome size statistics. --include_genus will also download completed genomes from the Staphylococcus genus that will be used for the protein set. These completed genomes are not used for the sketch creation or genome size calculation. --cpus 4 use 4 cpus for downloading and the clustering step. Adjust this number according to your setup! Use CARD over MEGARes Staphopia v1 made use of MEGARes, for the purposes of this tutorial we are going to use the CARD database instead. If all goes well, the newly created datasets are now available in the folder datasets/ . We have now completed the dataset creation step! Pat yourself on the back! Next we'll supplement these datasets with some optional S. aureus specific datasets. Acquire Staphopia Datasets \u00b6 Staphopia includes a few optional datasets such as S. aureus N315 reference genome and SCCmec sequences (primers, proteins, full cassettes). We will acquire these files using the Bactopia Datasets GitHub repository. For this tutorial a Staphopia v1 branch has been created, which includes this optional dataset. Now let's clone the repository. git clone -b staphopia-v1 https://github.com/bactopia/bactopia-datasets.git Next we'll copy the files into our recently built datasets folder and delete the bactopia-datasets repository since we no longer need it. cp -r bactopia-datasets/species-specific/ datasets/ rm -rf bactopia-datasets/ ~Voil\u00e0! That should be it. You should now have the Staphopia v1 datasets included with your recentely built datasets (e.g. S. aureus protein clusters, RefSeq sketch, etc...) Running Bactopia \u00b6 OK! Get your servers started up! It is time to get processing! Samples on ENA \u00b6 Single Sample \u00b6 Let's start this by downloading a single sample from ENA, and processing it through Bactopia. bactopia --accession SRX4563634 \\ --dataset datasets/ \\ --species staphylococcus-aureus \\ --coverage 100 \\ --genome_size median \\ --max_cpus 8 \\ --cpus 2 \\ --outdir ena-single-sample So, what's happening here? --accession SRX4563634 is telling Bactopia to download FASTQs associated with Exeriment accession SRX4563634. --dataset datasets/ tells Bactopia your pre-built datasets are in the folder datasets . --species staphylococcus-aureus tells Bactopia, within the datasets folder, use the species specific dataset for S. aureus . --coverage 100 will limit the cleaned up FASTQ file to an estimated 100x coverage based on the genome size. --genome_size median tells Bactopia to use the median genome size of completed S. aureus genomes. The minimum, maximum, median, and mean genome sizes were calculated during the dataset building step. If a genome size was not given, it would have been estimated by Mash. --max_cpus 8 and --cpus 2 tells Bactopia to use a maximum of 8 cpus ( --max_cpus ) and each job within the workflow can use a maximum of 2 cpus ( --cpus ). So at most 8 jobs (using 1 cpu each) can run at a time, and a minimum of 4 jobs (using 2 cpus each). Adjust these parameters to fit your setup! --outdir ena-single-sample tells Bactopia to dump the results into the ena-single-sample folder. Please keep in mind, this will not stop Nextflow from creating files (.nextflow, trace.txt, etc...) and directories (work and .nextflow/) within your current directory. Once you launch this command, sit back, relax and watch the Nextflow give realtime updates for SRX4563634's analysis! The approximate completion time is ~15-30 minutes depending on the number of cpus given and download times from ENA. Once complete, the results from numerous tools available to you in ena-single-sample/SRX4563634/ . Multiple Samples \u00b6 Now we are going to have Bactopia download and process 5 samples from ENA. To do this we can use the bactopia search function. bactopia search PRJNA480016 --limit 5 This will produce three files: ena-accessions.txt , ena-results.txt and ena-summary.txt . To learn more about these files see Generating Accession List . For this tutorial, ena-accessions.txt is the file we need. It contains five Experiment accessions, a single one per line. Just like this: SRX4563688 SRX4563687 SRX4563686 SRX4563689 SRX4563690 Note: you may have 5 different accessions from the PRJNA480016 project. To process these samples, we will adjust our command used previously. bactopia --accessions ena-accessions.txt \\ --dataset datasets/ \\ --species staphylococcus-aureus \\ --coverage 100 \\ --genome_size median \\ --max_cpus 8 \\ --cpus 2 \\ --outdir ena-multiple-samples Instead of --accession we are now using --accessions ena-accessions.txt which tells Bactopia to read ena-accessions.txt , and download each Experiment accession from ENA and then process it. At this point, you might want to go for a walk or make yourself a coffee! This step has an approximate completion time of ~45-120 minutes , which again is fully dependent on the cpus used and the download times from ENA. Once this is complete, the results for all five samples will be found in the ena-multiple-samples directory. Each sample will have there own folder of results. Local Samples \u00b6 So for the local samples, we're going to recycle some of the samples we downloaded from ENA. First let's make a directory to put the FASTQs into: mkdir fastqs Now let's move some the FASTQs from our SRX4563634 sample into this folder. cp ena-single-sample/SRX4563634/quality-control/SRX4563634*.fastq.gz fastqs/ Finally let's also make a single-end version of SRX4563634. cat fastqs/SRX4563634*.fastq.gz > fastqs/SRX4563634-SE.fastq.gz OK! Now we are ready to continue the tutorial! Single Sample \u00b6 Again we'll mostly be using the same parameters as previous, but with a few new ones. To process a single sample you can use the --R1 / --R2 (paired-end), --SE (single-end), and --sample parameters. Paired-End \u00b6 For paired-end reads you will want to use --R1 , --R2 , and --sample . For this paired-end example we'll use SRX4563634 again which we've copied to the fastqs folder. bactopia --R1 fastqs/SRX4563634_R1.fastq.gz \\ --R2 fastqs/SRX4563634_R2.fastq.gz \\ --sample SRX4563634 \\ --dataset datasets/ \\ --species staphylococcus-aureus \\ --coverage 100 \\ --genome_size median \\ --max_cpus 8 \\ --cpus 2 \\ --outdir local-single-sample Now Bactopia will recognize the --R1 and --R2 parameters as paired-end reads and process. The --sample is required and will be used for naming the output. Similar to the single ENA sample, the approximate completion time is ~15-30 minutes depending on the number of cpus given. Once complete, results can be found in local-single-sample/SRX4563634/ . Single-End \u00b6 In the case of Illumina reads, you're very unlikely to produce single-end reads, but they do exist in the wild (early days of Illumina!). Nevertheless, because single-end reads do exist, single-end support was built into Bactopia. To analyze single-end reads, the --SE parameter will replace --R1 and --R2 . bactopia --SE fastqs/SRX4563634-SE.fastq.gz \\ --sample SRX4563634-SE \\ --dataset datasets/ \\ --species staphylococcus-aureus \\ --coverage 100 \\ --genome_size median \\ --max_cpus 8 \\ --cpus 2 \\ --outdir local-single-sample Now SRX4563634-SE will be processed as a single-end sample. For single-end processing there are some paired-end only analyses (e.g. error correction, insertion sequences) that will be skipped. This leads to single-end samples being processed a little bit faster than pair-end samples. But, the approximate completion time is still ~15-30 minutes . Once complete, you'll the results from numerous tools available to you in local-single-sample/SRX4563634-SE/ . If you made it this far, you're almost done! Multiple Samples (FOFN) \u00b6 Here we go! The final way you can process samples in Bactopia! Bactopia allows you to give a text file describing the input samples. This file of file names (FOFN), contains sample names and location to associated FASTQs. The Bactopia FOFN format is described in detail at Basic Usage -> Multiple Samples . First we'll need to prepare a FOFN describing the FASTQ files in our fastqs folder. We can use bactopia prepare to do so: bactopia prepare fastqs/ > fastqs.txt This command will try to create a FOFN for you. For this turorial, the FASTQ names are pretty straight forward and should produce a correct FOFN (or at least it should! ... hopefully!). If that wasn't the case for you, there are ways to tweak bactopia prepare . Now we can use the --fastqs parameters to process samples in the FOFN. bactopia --fastqs fastqs.txt \\ --dataset datasets/ \\ --species staphylococcus-aureus \\ --coverage 100 \\ --genome_size median \\ --max_cpus 8 \\ --cpus 2 \\ --outdir local-multiple-samples We no longer need --R1 , --R2 , --SE , or --sample as the values for these parameters can be determined from the FOFN. Here it is, the final wait! This step has an approximate completion time of ~45-120 minutes . So, you will definitely want to go for a walk or make yourself a coffee! You've earned it! Once this is complete, the results for each sample (within their own folder) will be found in the local-multiple-samples directory. FOFN is more cpu efficient, making it faster The real benefit of using the FOFN method to process multiple samples is Nextflow's queue system will make better use of cpus. Processing multiple samples one at a time (via --R1 / --R2 or --SE ) will lead more instances of jobs waiting on other jobs to finish, during which cpus aren't being used. What's next? \u00b6 That should do it! Hopefully you have succeeded (yay! \ud83c\udf89) and would like to use Bactopia on your own data! In this tutorial we covered how to build datasets ( bactopia datasets ) and how process samples. We also covered the bactopia search and bactopia prepare to prepare file for multiple sample processing. If your ran into any issues, please let me know by submitting a GitHub Issue .","title":"Tutorial"},{"location":"tutorial/#build-datasets","text":"First let's create a directory to work in and activate our Bactopia environment. mkdir bactopia-tutorial cd bactopia-tutorial conda activate bactopia Now we are ready to build our datasets! bactopia datasets datasets/ --ariba \"vfdb_core,card\" \\ --species \"Staphylococcus aureus\" \\ --include_genus \\ --cpus 4 Let's review what is happening here. datasets/ is where our datasets will be downloaded, processed and stored. --ariba \"vfdb_core,card\" says to download and setup the VFDB Core and the CARD databases to be used by Ariba. --species \"Staphylococcus aureus\" will download MLST schemas associated with S. aureus it will also download completed S. aureus genomes (RefSeq only) that are used to create a set of protein set for annotation, a Mash sketch for automatic variant calling to the nearest neighbor, and calulate genome size statistics. --include_genus will also download completed genomes from the Staphylococcus genus that will be used for the protein set. These completed genomes are not used for the sketch creation or genome size calculation. --cpus 4 use 4 cpus for downloading and the clustering step. Adjust this number according to your setup! Use CARD over MEGARes Staphopia v1 made use of MEGARes, for the purposes of this tutorial we are going to use the CARD database instead. If all goes well, the newly created datasets are now available in the folder datasets/ . We have now completed the dataset creation step! Pat yourself on the back! Next we'll supplement these datasets with some optional S. aureus specific datasets.","title":"Build Datasets"},{"location":"tutorial/#acquire-staphopia-datasets","text":"Staphopia includes a few optional datasets such as S. aureus N315 reference genome and SCCmec sequences (primers, proteins, full cassettes). We will acquire these files using the Bactopia Datasets GitHub repository. For this tutorial a Staphopia v1 branch has been created, which includes this optional dataset. Now let's clone the repository. git clone -b staphopia-v1 https://github.com/bactopia/bactopia-datasets.git Next we'll copy the files into our recently built datasets folder and delete the bactopia-datasets repository since we no longer need it. cp -r bactopia-datasets/species-specific/ datasets/ rm -rf bactopia-datasets/ ~Voil\u00e0! That should be it. You should now have the Staphopia v1 datasets included with your recentely built datasets (e.g. S. aureus protein clusters, RefSeq sketch, etc...)","title":"Acquire Staphopia Datasets"},{"location":"tutorial/#running-bactopia","text":"OK! Get your servers started up! It is time to get processing!","title":"Running Bactopia"},{"location":"tutorial/#samples-on-ena","text":"","title":"Samples on ENA"},{"location":"tutorial/#single-sample","text":"Let's start this by downloading a single sample from ENA, and processing it through Bactopia. bactopia --accession SRX4563634 \\ --dataset datasets/ \\ --species staphylococcus-aureus \\ --coverage 100 \\ --genome_size median \\ --max_cpus 8 \\ --cpus 2 \\ --outdir ena-single-sample So, what's happening here? --accession SRX4563634 is telling Bactopia to download FASTQs associated with Exeriment accession SRX4563634. --dataset datasets/ tells Bactopia your pre-built datasets are in the folder datasets . --species staphylococcus-aureus tells Bactopia, within the datasets folder, use the species specific dataset for S. aureus . --coverage 100 will limit the cleaned up FASTQ file to an estimated 100x coverage based on the genome size. --genome_size median tells Bactopia to use the median genome size of completed S. aureus genomes. The minimum, maximum, median, and mean genome sizes were calculated during the dataset building step. If a genome size was not given, it would have been estimated by Mash. --max_cpus 8 and --cpus 2 tells Bactopia to use a maximum of 8 cpus ( --max_cpus ) and each job within the workflow can use a maximum of 2 cpus ( --cpus ). So at most 8 jobs (using 1 cpu each) can run at a time, and a minimum of 4 jobs (using 2 cpus each). Adjust these parameters to fit your setup! --outdir ena-single-sample tells Bactopia to dump the results into the ena-single-sample folder. Please keep in mind, this will not stop Nextflow from creating files (.nextflow, trace.txt, etc...) and directories (work and .nextflow/) within your current directory. Once you launch this command, sit back, relax and watch the Nextflow give realtime updates for SRX4563634's analysis! The approximate completion time is ~15-30 minutes depending on the number of cpus given and download times from ENA. Once complete, the results from numerous tools available to you in ena-single-sample/SRX4563634/ .","title":"Single Sample"},{"location":"tutorial/#multiple-samples","text":"Now we are going to have Bactopia download and process 5 samples from ENA. To do this we can use the bactopia search function. bactopia search PRJNA480016 --limit 5 This will produce three files: ena-accessions.txt , ena-results.txt and ena-summary.txt . To learn more about these files see Generating Accession List . For this tutorial, ena-accessions.txt is the file we need. It contains five Experiment accessions, a single one per line. Just like this: SRX4563688 SRX4563687 SRX4563686 SRX4563689 SRX4563690 Note: you may have 5 different accessions from the PRJNA480016 project. To process these samples, we will adjust our command used previously. bactopia --accessions ena-accessions.txt \\ --dataset datasets/ \\ --species staphylococcus-aureus \\ --coverage 100 \\ --genome_size median \\ --max_cpus 8 \\ --cpus 2 \\ --outdir ena-multiple-samples Instead of --accession we are now using --accessions ena-accessions.txt which tells Bactopia to read ena-accessions.txt , and download each Experiment accession from ENA and then process it. At this point, you might want to go for a walk or make yourself a coffee! This step has an approximate completion time of ~45-120 minutes , which again is fully dependent on the cpus used and the download times from ENA. Once this is complete, the results for all five samples will be found in the ena-multiple-samples directory. Each sample will have there own folder of results.","title":"Multiple Samples"},{"location":"tutorial/#local-samples","text":"So for the local samples, we're going to recycle some of the samples we downloaded from ENA. First let's make a directory to put the FASTQs into: mkdir fastqs Now let's move some the FASTQs from our SRX4563634 sample into this folder. cp ena-single-sample/SRX4563634/quality-control/SRX4563634*.fastq.gz fastqs/ Finally let's also make a single-end version of SRX4563634. cat fastqs/SRX4563634*.fastq.gz > fastqs/SRX4563634-SE.fastq.gz OK! Now we are ready to continue the tutorial!","title":"Local Samples"},{"location":"tutorial/#single-sample_1","text":"Again we'll mostly be using the same parameters as previous, but with a few new ones. To process a single sample you can use the --R1 / --R2 (paired-end), --SE (single-end), and --sample parameters.","title":"Single Sample"},{"location":"tutorial/#paired-end","text":"For paired-end reads you will want to use --R1 , --R2 , and --sample . For this paired-end example we'll use SRX4563634 again which we've copied to the fastqs folder. bactopia --R1 fastqs/SRX4563634_R1.fastq.gz \\ --R2 fastqs/SRX4563634_R2.fastq.gz \\ --sample SRX4563634 \\ --dataset datasets/ \\ --species staphylococcus-aureus \\ --coverage 100 \\ --genome_size median \\ --max_cpus 8 \\ --cpus 2 \\ --outdir local-single-sample Now Bactopia will recognize the --R1 and --R2 parameters as paired-end reads and process. The --sample is required and will be used for naming the output. Similar to the single ENA sample, the approximate completion time is ~15-30 minutes depending on the number of cpus given. Once complete, results can be found in local-single-sample/SRX4563634/ .","title":"Paired-End"},{"location":"tutorial/#single-end","text":"In the case of Illumina reads, you're very unlikely to produce single-end reads, but they do exist in the wild (early days of Illumina!). Nevertheless, because single-end reads do exist, single-end support was built into Bactopia. To analyze single-end reads, the --SE parameter will replace --R1 and --R2 . bactopia --SE fastqs/SRX4563634-SE.fastq.gz \\ --sample SRX4563634-SE \\ --dataset datasets/ \\ --species staphylococcus-aureus \\ --coverage 100 \\ --genome_size median \\ --max_cpus 8 \\ --cpus 2 \\ --outdir local-single-sample Now SRX4563634-SE will be processed as a single-end sample. For single-end processing there are some paired-end only analyses (e.g. error correction, insertion sequences) that will be skipped. This leads to single-end samples being processed a little bit faster than pair-end samples. But, the approximate completion time is still ~15-30 minutes . Once complete, you'll the results from numerous tools available to you in local-single-sample/SRX4563634-SE/ . If you made it this far, you're almost done!","title":"Single-End"},{"location":"tutorial/#multiple-samples-fofn","text":"Here we go! The final way you can process samples in Bactopia! Bactopia allows you to give a text file describing the input samples. This file of file names (FOFN), contains sample names and location to associated FASTQs. The Bactopia FOFN format is described in detail at Basic Usage -> Multiple Samples . First we'll need to prepare a FOFN describing the FASTQ files in our fastqs folder. We can use bactopia prepare to do so: bactopia prepare fastqs/ > fastqs.txt This command will try to create a FOFN for you. For this turorial, the FASTQ names are pretty straight forward and should produce a correct FOFN (or at least it should! ... hopefully!). If that wasn't the case for you, there are ways to tweak bactopia prepare . Now we can use the --fastqs parameters to process samples in the FOFN. bactopia --fastqs fastqs.txt \\ --dataset datasets/ \\ --species staphylococcus-aureus \\ --coverage 100 \\ --genome_size median \\ --max_cpus 8 \\ --cpus 2 \\ --outdir local-multiple-samples We no longer need --R1 , --R2 , --SE , or --sample as the values for these parameters can be determined from the FOFN. Here it is, the final wait! This step has an approximate completion time of ~45-120 minutes . So, you will definitely want to go for a walk or make yourself a coffee! You've earned it! Once this is complete, the results for each sample (within their own folder) will be found in the local-multiple-samples directory. FOFN is more cpu efficient, making it faster The real benefit of using the FOFN method to process multiple samples is Nextflow's queue system will make better use of cpus. Processing multiple samples one at a time (via --R1 / --R2 or --SE ) will lead more instances of jobs waiting on other jobs to finish, during which cpus aren't being used.","title":"Multiple Samples (FOFN)"},{"location":"tutorial/#whats-next","text":"That should do it! Hopefully you have succeeded (yay! \ud83c\udf89) and would like to use Bactopia on your own data! In this tutorial we covered how to build datasets ( bactopia datasets ) and how process samples. We also covered the bactopia search and bactopia prepare to prepare file for multiple sample processing. If your ran into any issues, please let me know by submitting a GitHub Issue .","title":"What's next?"},{"location":"usage-basic/","text":"Basic Usage For Bactopia \u00b6 Bactopia is a wrapper around many different tools. Each of these tools may (or may not) have there own configurable parameters for you to tweak. In order to facilitate getting started with Bactopia, this section has been limited to discussion of only a few parameters. However, if you are interested in the full list of configurable parameters in Bactopia, please check out the Complete Usage section. Usage \u00b6 bactopia Required Parameters: ### For Procesessing Multiple Samples --fastqs STR An input file containing the sample name and absolute paths to FASTQs to process ### For Processing A Single Sample --R1 STR First set of reads for paired end in compressed (gzip) FASTQ format --R2 STR Second set of reads for paired end in compressed (gzip) FASTQ format --SE STR Single end set of reads in compressed (gzip) FASTQ format --sample STR The name of the input sequences ### For Downloading from ENA --accessions An input file containing ENA/SRA experiement accessions to be processed --accession A single ENA/SRA Experiment accession to be processed Dataset Parameters: --datasets DIR The path to available datasets that have already been set up --species STR Determines which species-specific dataset to use for the input sequencing Optional Parameters: --coverage INT Reduce samples to a given coverage Default: 100x --genome_size INT Expected genome size (bp) for all samples, a value of '0' will disable read error correction and read subsampling. Special values (requires --species): 'min': uses minimum completed genome size of species 'median': uses median completed genome size of species 'mean': uses mean completed genome size of species 'max': uses max completed genome size of species Default: Mash estimate --outdir DIR Directory to write results to Default . --max_memory INT The maximum amount of memory (Gb) allowed to a single process. Default: 32 Gb --max_cpus INT The maximum number of processors this workflow should have access to at any given moment Default: 1 --cpus INT Number of processors made available to a single process. If greater than \"--max_cpus\" it will be set equal to \"--max_cpus\" Default: 1 Useful Parameters: --available_datasets Print a list of available datasets found based on location given by \"--datasets\" --example_fastqs Print example of expected input for FASTQs file --check_fastqs Verify \"--fastqs\" produces the expected inputs --compress Compress (gzip) select outputs (e.g. annotation, variant calls) to reduce overall storage footprint. --keep_all_files Keeps all analysis files created. By default, intermediate files are removed. This will not affect the ability to resume Nextflow runs, and only occurs at the end of the process. --version Print workflow version information --help Show this message and exit --help_all Show a complete list of adjustable parameters FASTQ Inputs \u00b6 Bactopia has multiple approaches to specify your input sequences. You can make use of your local FASTQs or download FASTQs from the European Nucleotide Archive (ENA) . Which approach really depends on what you need to achieve! The following sections describe methods to process single samples, multiple samples, downloading samples from the ENA. Local \u00b6 Single Sample \u00b6 When you only need to process a single sample at a time, Bactopia allows that! You only have to the sample name ( --sample ) and the whether the read set is paired-end ( --R1 and --R2 ) or a single-end ( --SE ). Use --R1, --R2 for Paired-End FASTQs bactopia --sample my-sample --R1 /path/to/my-sample_R1.fastq.gz --R2 /path/to/my-sample_R2.fastq.gz Use --SE for Single-End FASTQs bactopia --sample my-sample --SE /path/to/my-sample.fastq.gz Multiple Samples \u00b6 For multiple samples, you must create a file with information about the inputs, a file of filenames (FOFN). This file specifies sample names and location of FASTQs to be processed. Using this information, paired-end or single-end information can be extracted as well as naming output files. While this is an additional step for you, the user, it helps to avoid potential pattern matching errors. Most importantly, by taking this approach, you can process hundreds of samples in a single command. There is also the added benefit of knowing which FASTQs were analysed and their location at a later time! Use --fastqs for Multiple Samples bactopia --fastqs my-samples.txt The FOFN Format \u00b6 You can use the --example_fastqs to get an example of the expected structure for the input FASTQs FOFN. bactopia --example_fastqs N E X T F L O W ~ version 19.01.0 Launching `/home/rpetit/illumina-cleanup/bin/illumina-cleanup` [naughty_borg] - revision: 0416ba407c Printing example input for \"--fastqs\" sample r1 r2 test001 /path/to/fastqs/test_R1.fastq.gz /path/to/fastqs/test_R2.fastq.gz test002 /path/to/fastqs/test.fastq.gz The expected structure is a tab-delimited table with three columns: sample : A unique prefix, or unique name, to be used for naming output files r1 : If paired-end, the first pair of reads, else the single-end reads r2 : If paired-end, the second pair of reads These three columns are used as the header for the file. In other words, all input FOFNs require their first line to be: sample r1 r2 All lines after the header line, contain unique sample names and location(s) to associated FASTQ file(s). Absolute paths should be used to prevent any file not found errors due to the relative path changing. In the example above, two samples would be processed by Bactopia. Sample test001 has two FASTQs and would be processed as pair-end reads. While sample test002 only has a single FASTQ and would be processed as single-end reads. Generating A FOFN \u00b6 bactopia prepare has been included to help aid (hopefully!) the process of creating a FOFN for your samples. This script will attempt to find FASTQ files in a given directory and output the expected FOFN format. It will also output any potential issues associated with the pattern matching. Verify accuracy of FOFN This is currently an experimental function. There are likely bugs to be ironed out. Please be sure to give the resulting FOFN a quick look over. Usage \u00b6 bactopia prepare [-h] [-e STR] [-s STR] [--pattern STR] [--version] STR bactopia prepare - Read a directory and prepare a FOFN of FASTQs positional arguments: STR Directory where FASTQ files are stored optional arguments: -h, --help show this help message and exit -e STR, --ext STR Extension of the FASTQs. Default: .fastq.gz -s STR, --sep STR Split FASTQ name on the last occurrence of the separator. Default: _ --pattern STR Glob pattern to match FASTQs. Default: *.fastq.gz --version show program's version number and exit Examples \u00b6 Here is an example using the default parameters. In the example, sample SRR00000 has more than 2 FASTQs matched to it, which is recognized as an error. bactopia prepare tests/dummy-fastqs/ sample r1 r2 SRR00000 /home/rpetit/projects/bactopia/bactopia/tests/dummy-fastqs/SRR00000.fastq.gz /home/rpetit/projects/bactopia/bactopia/tests/dummy-fastqs/SRR00000_1.fastq.gz /home/rpetit/projects/bactopia/bactopia/tests/dummy-fastqs/SRR00000_2.fastq.gz ERROR: \"SRR00000\" has more than two different FASTQ files, please check. After tweaking the --pattern parameter a little bit. The error is corrected and sample SRR00000 is properly recognized as a paired-end read set. bactopia prepare tests/dummy-fastqs/ --pattern *_[12].fastq.gz sample r1 r2 SRR00000 /home/rpetit/projects/bactopia/bactopia/tests/dummy-fastqs/SRR00000_1.fastq.gz /home/rpetit/projects/bactopia/bactopia/tests/dummy-fastqs/SRR00000_2.fastq.gz There are a number of ways to tweak the pattern. Just please be sure to give a quick look over of the resulting FOFN. Validating FOFN \u00b6 When a FOFN is given, the first thing Bactopia does is verify all FASTQ files are found. If everything checks out, each sample will then be processed, otherwise a list of samples with errors will be output to STDERR. If you would like to only validate your FOFN (and not run the full pipeline), you can use the --check_fastqs parameter. Without Errors \u00b6 bactopia --check_fastqs --fastqs example-data/good-fastqs.txt N E X T F L O W ~ version 19.01.0 Launching `/home/rpetit3/bactopia/bactopia` [astonishing_colden] - revision: 96c6a1a7ae Printing what would have been processed. Each line consists of an array of three elements: [SAMPLE_NAME, IS_SINGLE_END, [FASTQ_1, FASTQ_2]] Found: [test001, false, [/home/rpetit3/bactopia/tests/fastqs/test_R1.fastq.gz, /home/rpetit3/bactopia/tests/fastqs/test_R2.fastq.gz]] [test002, true, [/home/rpetit3/bactopia/tests/fastqs/test.fastq.gz]] Each sample has passed validation and is put into a three element array: sample - the name for this sample is_single_end - the reads are single-end (true) or paired-end (false) fastq_array - the fastqs associated with the sample This array is then automatically queued up for proccessing by Nextflow. With errors \u00b6 bactopia --check_fastqs --fastqs tests/data/bad-fastqs.txt N E X T F L O W ~ version 19.01.0 Launching `/home/rpetit3/bactopia/bactopia` [kickass_mestorf] - revision: 222a5ad8b1 LINE 4:ERROR: Please verify /home/rpetit3/bactopia/test/fastqs/test003_R1.fastq.gz exists, and try again LINE 5:ERROR: Please verify /home/rpetit3/bactopia/test/fastqs/test003_R1.fastq.gz exists, and try again LINE 5:ERROR: Please verify /home/rpetit3/bactopia/test/fastqs/test002_R2.fastq.gz exists, and try again Sample name \"test002\" is not unique, please revise sample names The header line (line 1) does not follow expected structure. Verify sample names are unique and/or FASTQ paths are correct See \"--example_fastqs\" for an example Exiting In the above example, there are mulitple errors. Lines 4 and 5 ( LINE 4:ERROR or LINE 5:ERROR ) suggest that based on the given paths the FASTQs do not exist. The sample name test002 has been used multiple times, and must be corrected. There is also an issue with the header line that must be looked into. European Nucleotide Archive \u00b6 There are a lot of publicly avilable sequences, and you might want to include some of those in your analysis! If that sounds like you, Bactopia has that built in for you! You can give a single Experiment accession ( --accession ) or a file where each line is a single Experiment accession ( --accessions ). Bactopia will then query ENA to determine Run accession(s) associated with the given Experiment accession and proceed download (from ENA) corresponding FASTQ files. After the download is completed, it will be processed through Bactopia. Use --accession for a Single Experiment Accession bactopia --accession SRX476958 Use --accessions for Multiple Experiment Accessions bactopia --accessions my-accessions.txt Generating Accession List \u00b6 bactopia search has been made to help assist in generating a list of Experiment accessions to be procesed by Bactopia (via --accessions ). Users can provide a Taxon ID (e.g. 1280), a binary name (e.g. Staphylococcus aureus), or Study accessions (e.g. PRJNA480016). This value is then queried against ENA's Data Warehouse API ), and a list of all Experiment accessions associated with the query is returned. Usage \u00b6 usage: bactopia search [-h] [--exact_taxon] [--outdir OUTPUT_DIRECTORY] [--prefix PREFIX] [--limit INT] [--version] STR bactopia search - Search ENA for associated WGS samples positional arguments: STR Taxon ID or Study accession optional arguments: -h, --help show this help message and exit --exact_taxon Exclude Taxon ID descendents. --outdir OUTPUT_DIRECTORY Directory to write output. (Default: .) --prefix PREFIX Prefix to use for output file names. (Default: ena) --limit INT Maximum number of results to return. (Default: 1000000) --version show program's version number and exit example usage: bactopia search PRJNA480016 --limit 20 bactopia search 1280 --exact_taxon --limit 20' bactopia search \"staphylococcus aureus\" --limit 20 Example \u00b6 bactopia search PRJNA480016 --limit 5 When completed three files are produced: ena-accessions.txt - Contains a list of Experiment accessions to be processed. SRX4563686 SRX4563689 SRX4563687 SRX4563690 SRX4563688 Input for Bactopia This file can be used in conjunction with the --accessions parameter for Bactopia processing. ena-results.txt - Contains the full results of the API query. This includes multiples fields (sample_accession, tax_id, sample_alias, center_name, etc...) ena-summary.txt - Contains a small summary of the completed request QUERY: (study_accession=PRJNA480016 OR secondary_study_accession=PRJNA480016) LIMIT: 5 RESULTS: 5 (./ena-results.txt) ILLUMINA ACCESSIONS: 5 (./ena-accessions.txt) --genome_size \u00b6 Throughout the Bactopia workflow a genome size is used for various tasks. By default, a genome size is estimated using Mash. However, users can provide their own value for genome size, use values based on Species Specific Datasets , or completely disable it. Value Result empty Mash is used to estimate the genome size integer Uses the genome size (e.g. --genome_size 2800000 ) provided by the user 0 Read error correct and read subsampling will be disabled. min Requires --species , the minimum completed genome size for a species is used median Requires --species , the median completed genome size for a species is used mean Requires --species , the mean completed genome size for a species is used max Requires --species , the maximum completed genome size for a species is used Mash may not be the most accurate estimate Mash is very convenient to quickly estimate a genome size, but it may not be the most accurate in all cases and will differ between samples. It is recommended that when possible a known genome size or one based off completed genomes should be used. --max_cpus & --cpus \u00b6 When Nextflow executes, it uses all available cpus to queue up processes. As you might imagine, if you are on a single server with multiple users, this approach of using all cpus might annoy other users! (Whoops sorry!) To circumvent this feature, two parmeters have been included --max_cpus and --cpus . --max_cpus INT The maximum number of processors this workflow should have access to at any given moment Default: 1 --cpus INT Number of processors made available to a single process. If greater than \"--max_cpus\" it will be set equal to \"--max_cpus\" Default: 1 What --max_cpus does is specify to Nextflow the maximum number of cpus it is allowed to occupy at any given time. --cpus on the other hand, specifies how many cpus any given step (qc, assembly, annotation, etc...) can occupy. By default --max_cpus is set to 1 and if --cpus is set to a value greater than --max_cpus it will be set equal to --max_cpus . This appoach errs on the side of caution, by not occupying all cpus on the server without the user's consent! --keep_all_files \u00b6 In some processes, Bactopia will delete large intermediate files (e.g. multiple uncompressed FASTQs) only after a process successfully completes. Since this a per-process function, it does not affect Nextflow's ability to resume ( -resume )a workflow. You can deactivate this feature using --keep_all_files . Please, keep in mind the work directory is already large, this will make it 2-3 times larger.","title":"Basic Usage"},{"location":"usage-basic/#basic-usage-for-bactopia","text":"Bactopia is a wrapper around many different tools. Each of these tools may (or may not) have there own configurable parameters for you to tweak. In order to facilitate getting started with Bactopia, this section has been limited to discussion of only a few parameters. However, if you are interested in the full list of configurable parameters in Bactopia, please check out the Complete Usage section.","title":"Basic Usage For Bactopia"},{"location":"usage-basic/#usage","text":"bactopia Required Parameters: ### For Procesessing Multiple Samples --fastqs STR An input file containing the sample name and absolute paths to FASTQs to process ### For Processing A Single Sample --R1 STR First set of reads for paired end in compressed (gzip) FASTQ format --R2 STR Second set of reads for paired end in compressed (gzip) FASTQ format --SE STR Single end set of reads in compressed (gzip) FASTQ format --sample STR The name of the input sequences ### For Downloading from ENA --accessions An input file containing ENA/SRA experiement accessions to be processed --accession A single ENA/SRA Experiment accession to be processed Dataset Parameters: --datasets DIR The path to available datasets that have already been set up --species STR Determines which species-specific dataset to use for the input sequencing Optional Parameters: --coverage INT Reduce samples to a given coverage Default: 100x --genome_size INT Expected genome size (bp) for all samples, a value of '0' will disable read error correction and read subsampling. Special values (requires --species): 'min': uses minimum completed genome size of species 'median': uses median completed genome size of species 'mean': uses mean completed genome size of species 'max': uses max completed genome size of species Default: Mash estimate --outdir DIR Directory to write results to Default . --max_memory INT The maximum amount of memory (Gb) allowed to a single process. Default: 32 Gb --max_cpus INT The maximum number of processors this workflow should have access to at any given moment Default: 1 --cpus INT Number of processors made available to a single process. If greater than \"--max_cpus\" it will be set equal to \"--max_cpus\" Default: 1 Useful Parameters: --available_datasets Print a list of available datasets found based on location given by \"--datasets\" --example_fastqs Print example of expected input for FASTQs file --check_fastqs Verify \"--fastqs\" produces the expected inputs --compress Compress (gzip) select outputs (e.g. annotation, variant calls) to reduce overall storage footprint. --keep_all_files Keeps all analysis files created. By default, intermediate files are removed. This will not affect the ability to resume Nextflow runs, and only occurs at the end of the process. --version Print workflow version information --help Show this message and exit --help_all Show a complete list of adjustable parameters","title":"Usage"},{"location":"usage-basic/#fastq-inputs","text":"Bactopia has multiple approaches to specify your input sequences. You can make use of your local FASTQs or download FASTQs from the European Nucleotide Archive (ENA) . Which approach really depends on what you need to achieve! The following sections describe methods to process single samples, multiple samples, downloading samples from the ENA.","title":"FASTQ Inputs"},{"location":"usage-basic/#local","text":"","title":"Local"},{"location":"usage-basic/#single-sample","text":"When you only need to process a single sample at a time, Bactopia allows that! You only have to the sample name ( --sample ) and the whether the read set is paired-end ( --R1 and --R2 ) or a single-end ( --SE ). Use --R1, --R2 for Paired-End FASTQs bactopia --sample my-sample --R1 /path/to/my-sample_R1.fastq.gz --R2 /path/to/my-sample_R2.fastq.gz Use --SE for Single-End FASTQs bactopia --sample my-sample --SE /path/to/my-sample.fastq.gz","title":"Single Sample"},{"location":"usage-basic/#multiple-samples","text":"For multiple samples, you must create a file with information about the inputs, a file of filenames (FOFN). This file specifies sample names and location of FASTQs to be processed. Using this information, paired-end or single-end information can be extracted as well as naming output files. While this is an additional step for you, the user, it helps to avoid potential pattern matching errors. Most importantly, by taking this approach, you can process hundreds of samples in a single command. There is also the added benefit of knowing which FASTQs were analysed and their location at a later time! Use --fastqs for Multiple Samples bactopia --fastqs my-samples.txt","title":"Multiple Samples"},{"location":"usage-basic/#the-fofn-format","text":"You can use the --example_fastqs to get an example of the expected structure for the input FASTQs FOFN. bactopia --example_fastqs N E X T F L O W ~ version 19.01.0 Launching `/home/rpetit/illumina-cleanup/bin/illumina-cleanup` [naughty_borg] - revision: 0416ba407c Printing example input for \"--fastqs\" sample r1 r2 test001 /path/to/fastqs/test_R1.fastq.gz /path/to/fastqs/test_R2.fastq.gz test002 /path/to/fastqs/test.fastq.gz The expected structure is a tab-delimited table with three columns: sample : A unique prefix, or unique name, to be used for naming output files r1 : If paired-end, the first pair of reads, else the single-end reads r2 : If paired-end, the second pair of reads These three columns are used as the header for the file. In other words, all input FOFNs require their first line to be: sample r1 r2 All lines after the header line, contain unique sample names and location(s) to associated FASTQ file(s). Absolute paths should be used to prevent any file not found errors due to the relative path changing. In the example above, two samples would be processed by Bactopia. Sample test001 has two FASTQs and would be processed as pair-end reads. While sample test002 only has a single FASTQ and would be processed as single-end reads.","title":"The FOFN Format"},{"location":"usage-basic/#generating-a-fofn","text":"bactopia prepare has been included to help aid (hopefully!) the process of creating a FOFN for your samples. This script will attempt to find FASTQ files in a given directory and output the expected FOFN format. It will also output any potential issues associated with the pattern matching. Verify accuracy of FOFN This is currently an experimental function. There are likely bugs to be ironed out. Please be sure to give the resulting FOFN a quick look over.","title":"Generating A FOFN"},{"location":"usage-basic/#usage_1","text":"bactopia prepare [-h] [-e STR] [-s STR] [--pattern STR] [--version] STR bactopia prepare - Read a directory and prepare a FOFN of FASTQs positional arguments: STR Directory where FASTQ files are stored optional arguments: -h, --help show this help message and exit -e STR, --ext STR Extension of the FASTQs. Default: .fastq.gz -s STR, --sep STR Split FASTQ name on the last occurrence of the separator. Default: _ --pattern STR Glob pattern to match FASTQs. Default: *.fastq.gz --version show program's version number and exit","title":"Usage"},{"location":"usage-basic/#examples","text":"Here is an example using the default parameters. In the example, sample SRR00000 has more than 2 FASTQs matched to it, which is recognized as an error. bactopia prepare tests/dummy-fastqs/ sample r1 r2 SRR00000 /home/rpetit/projects/bactopia/bactopia/tests/dummy-fastqs/SRR00000.fastq.gz /home/rpetit/projects/bactopia/bactopia/tests/dummy-fastqs/SRR00000_1.fastq.gz /home/rpetit/projects/bactopia/bactopia/tests/dummy-fastqs/SRR00000_2.fastq.gz ERROR: \"SRR00000\" has more than two different FASTQ files, please check. After tweaking the --pattern parameter a little bit. The error is corrected and sample SRR00000 is properly recognized as a paired-end read set. bactopia prepare tests/dummy-fastqs/ --pattern *_[12].fastq.gz sample r1 r2 SRR00000 /home/rpetit/projects/bactopia/bactopia/tests/dummy-fastqs/SRR00000_1.fastq.gz /home/rpetit/projects/bactopia/bactopia/tests/dummy-fastqs/SRR00000_2.fastq.gz There are a number of ways to tweak the pattern. Just please be sure to give a quick look over of the resulting FOFN.","title":"Examples"},{"location":"usage-basic/#validating-fofn","text":"When a FOFN is given, the first thing Bactopia does is verify all FASTQ files are found. If everything checks out, each sample will then be processed, otherwise a list of samples with errors will be output to STDERR. If you would like to only validate your FOFN (and not run the full pipeline), you can use the --check_fastqs parameter.","title":"Validating FOFN"},{"location":"usage-basic/#without-errors","text":"bactopia --check_fastqs --fastqs example-data/good-fastqs.txt N E X T F L O W ~ version 19.01.0 Launching `/home/rpetit3/bactopia/bactopia` [astonishing_colden] - revision: 96c6a1a7ae Printing what would have been processed. Each line consists of an array of three elements: [SAMPLE_NAME, IS_SINGLE_END, [FASTQ_1, FASTQ_2]] Found: [test001, false, [/home/rpetit3/bactopia/tests/fastqs/test_R1.fastq.gz, /home/rpetit3/bactopia/tests/fastqs/test_R2.fastq.gz]] [test002, true, [/home/rpetit3/bactopia/tests/fastqs/test.fastq.gz]] Each sample has passed validation and is put into a three element array: sample - the name for this sample is_single_end - the reads are single-end (true) or paired-end (false) fastq_array - the fastqs associated with the sample This array is then automatically queued up for proccessing by Nextflow.","title":"Without Errors"},{"location":"usage-basic/#with-errors","text":"bactopia --check_fastqs --fastqs tests/data/bad-fastqs.txt N E X T F L O W ~ version 19.01.0 Launching `/home/rpetit3/bactopia/bactopia` [kickass_mestorf] - revision: 222a5ad8b1 LINE 4:ERROR: Please verify /home/rpetit3/bactopia/test/fastqs/test003_R1.fastq.gz exists, and try again LINE 5:ERROR: Please verify /home/rpetit3/bactopia/test/fastqs/test003_R1.fastq.gz exists, and try again LINE 5:ERROR: Please verify /home/rpetit3/bactopia/test/fastqs/test002_R2.fastq.gz exists, and try again Sample name \"test002\" is not unique, please revise sample names The header line (line 1) does not follow expected structure. Verify sample names are unique and/or FASTQ paths are correct See \"--example_fastqs\" for an example Exiting In the above example, there are mulitple errors. Lines 4 and 5 ( LINE 4:ERROR or LINE 5:ERROR ) suggest that based on the given paths the FASTQs do not exist. The sample name test002 has been used multiple times, and must be corrected. There is also an issue with the header line that must be looked into.","title":"With errors"},{"location":"usage-basic/#european-nucleotide-archive","text":"There are a lot of publicly avilable sequences, and you might want to include some of those in your analysis! If that sounds like you, Bactopia has that built in for you! You can give a single Experiment accession ( --accession ) or a file where each line is a single Experiment accession ( --accessions ). Bactopia will then query ENA to determine Run accession(s) associated with the given Experiment accession and proceed download (from ENA) corresponding FASTQ files. After the download is completed, it will be processed through Bactopia. Use --accession for a Single Experiment Accession bactopia --accession SRX476958 Use --accessions for Multiple Experiment Accessions bactopia --accessions my-accessions.txt","title":"European Nucleotide Archive"},{"location":"usage-basic/#generating-accession-list","text":"bactopia search has been made to help assist in generating a list of Experiment accessions to be procesed by Bactopia (via --accessions ). Users can provide a Taxon ID (e.g. 1280), a binary name (e.g. Staphylococcus aureus), or Study accessions (e.g. PRJNA480016). This value is then queried against ENA's Data Warehouse API ), and a list of all Experiment accessions associated with the query is returned.","title":"Generating Accession List"},{"location":"usage-basic/#usage_2","text":"usage: bactopia search [-h] [--exact_taxon] [--outdir OUTPUT_DIRECTORY] [--prefix PREFIX] [--limit INT] [--version] STR bactopia search - Search ENA for associated WGS samples positional arguments: STR Taxon ID or Study accession optional arguments: -h, --help show this help message and exit --exact_taxon Exclude Taxon ID descendents. --outdir OUTPUT_DIRECTORY Directory to write output. (Default: .) --prefix PREFIX Prefix to use for output file names. (Default: ena) --limit INT Maximum number of results to return. (Default: 1000000) --version show program's version number and exit example usage: bactopia search PRJNA480016 --limit 20 bactopia search 1280 --exact_taxon --limit 20' bactopia search \"staphylococcus aureus\" --limit 20","title":"Usage"},{"location":"usage-basic/#example","text":"bactopia search PRJNA480016 --limit 5 When completed three files are produced: ena-accessions.txt - Contains a list of Experiment accessions to be processed. SRX4563686 SRX4563689 SRX4563687 SRX4563690 SRX4563688 Input for Bactopia This file can be used in conjunction with the --accessions parameter for Bactopia processing. ena-results.txt - Contains the full results of the API query. This includes multiples fields (sample_accession, tax_id, sample_alias, center_name, etc...) ena-summary.txt - Contains a small summary of the completed request QUERY: (study_accession=PRJNA480016 OR secondary_study_accession=PRJNA480016) LIMIT: 5 RESULTS: 5 (./ena-results.txt) ILLUMINA ACCESSIONS: 5 (./ena-accessions.txt)","title":"Example"},{"location":"usage-basic/#-genome_size","text":"Throughout the Bactopia workflow a genome size is used for various tasks. By default, a genome size is estimated using Mash. However, users can provide their own value for genome size, use values based on Species Specific Datasets , or completely disable it. Value Result empty Mash is used to estimate the genome size integer Uses the genome size (e.g. --genome_size 2800000 ) provided by the user 0 Read error correct and read subsampling will be disabled. min Requires --species , the minimum completed genome size for a species is used median Requires --species , the median completed genome size for a species is used mean Requires --species , the mean completed genome size for a species is used max Requires --species , the maximum completed genome size for a species is used Mash may not be the most accurate estimate Mash is very convenient to quickly estimate a genome size, but it may not be the most accurate in all cases and will differ between samples. It is recommended that when possible a known genome size or one based off completed genomes should be used.","title":"--genome_size"},{"location":"usage-basic/#-max_cpus-cpus","text":"When Nextflow executes, it uses all available cpus to queue up processes. As you might imagine, if you are on a single server with multiple users, this approach of using all cpus might annoy other users! (Whoops sorry!) To circumvent this feature, two parmeters have been included --max_cpus and --cpus . --max_cpus INT The maximum number of processors this workflow should have access to at any given moment Default: 1 --cpus INT Number of processors made available to a single process. If greater than \"--max_cpus\" it will be set equal to \"--max_cpus\" Default: 1 What --max_cpus does is specify to Nextflow the maximum number of cpus it is allowed to occupy at any given time. --cpus on the other hand, specifies how many cpus any given step (qc, assembly, annotation, etc...) can occupy. By default --max_cpus is set to 1 and if --cpus is set to a value greater than --max_cpus it will be set equal to --max_cpus . This appoach errs on the side of caution, by not occupying all cpus on the server without the user's consent!","title":"--max_cpus &amp; --cpus"},{"location":"usage-basic/#-keep_all_files","text":"In some processes, Bactopia will delete large intermediate files (e.g. multiple uncompressed FASTQs) only after a process successfully completes. Since this a per-process function, it does not affect Nextflow's ability to resume ( -resume )a workflow. You can deactivate this feature using --keep_all_files . Please, keep in mind the work directory is already large, this will make it 2-3 times larger.","title":"--keep_all_files"},{"location":"usage-complete/","text":"Runtime Parameters \u00b6 Bactopia includes numerous (100+) configurable parameters. Basically for each step of the pipeline, you can modify the default parameters of a specific tool. Required \u00b6 The required parameters depends on how many samples are to be proccessed. You can learn more about which approach to take at Specifying Input FASTQs . ### For Procesessing Multiple Samples --fastqs STR An input file containing the sample name and absolute paths to FASTQs to process ### For Processing A Single Sample --R1 STR First set of reads for paired end in compressed (gzip) FASTQ format --R2 STR Second set of reads for paired end in compressed (gzip) FASTQ format --SE STR Single end set of reads in compressed (gzip) FASTQ format --sample STR The name of the input sequences ### For Downloading from ENA --accessions An input file containing ENA/SRA experiement accessions to be processed --accession A single ENA/SRA Experiment accession to be processed Dataset \u00b6 If you followed the steps in Build Datasets , you can use the following parameters to point Bactopia to you datasets. --datasets DIR The path to available public datasets that have already been set up --species STR Determines which species-specific dataset to use for the input sequencing Optional \u00b6 These optional parameters, while not required, will be quite useful (especially --max_cpus and --cpus !) to tweak. --coverage INT Reduce samples to a given coverage Default: 100x --genome_size INT Expected genome size (bp) for all samples, a value of '0' will disable read error correction and read subsampling. Special values (requires --species): 'min': uses minimum completed genome size of species 'median': uses median completed genome size of species 'mean': uses mean completed genome size of species 'max': uses max completed genome size of species Default: Mash estimate --outdir DIR Directory to write results to Default . --max_memory INT The maximum amount of memory (Gb) allowed to a single process. Default: 32 Gb --genome_size \u00b6 Throughout the Bactopia workflow a genome size is used for various tasks. By default, a genome size is estimated using Mash. However, users can provide their own value for genome size, use values based on Species Specific Datasets , or completely disable it. Value Result empty Mash is used to estimate the genome size integer Uses the genome size (e.g. --genome_size 2800000 ) provided by the user 0 Read error correct and read subsampling will be disabled. min Requires --species , the minimum completed genome size for a species is used median Requires --species , the median completed genome size for a species is used mean Requires --species , the mean completed genome size for a species is used max Requires --species , the maximum completed genome size for a species is used Mash may not be the most accurate estimate Mash is very convenient to quickly estimate a genome size, but it may not be the most accurate in all cases and will differ between samples. It is recommended that when possible a known genome size or one based off completed genomes should be used. --max_cpus vs --cpus \u00b6 By default when Nextflow executes, it uses all available cpus to queue up processes. As you might imagine, if you are on a single server with multiple users, this approach of using all cpus might annoy other users! (Whoops sorry!) To circumvent this feature, two parmeters have been included --max_cpus and --cpus . --max_cpus INT The maximum number of processors this workflow should have access to at any given moment Default: 1 --cpus INT Number of processors made available to a single process. If greater than \"--max_cpus\" it will be set equal to \"--max_cpus\" Default: 1 What --max_cpus does is specify to Nextflow the maximum number of cpus it is allowed to occupy at any given time. --cpus on the other hand, specifies how many cpus any given step (qc, assembly, annotation, etc...) can occupy. By default --max_cpus is set to 1 and if --cpus is set to a value greater than --max_cpus it will be set equal to --max_cpus . This appoach errs on the side of caution, by not occupying all cpus on the server without the users consent! Helpers \u00b6 The following parameters are useful to test to test input parameters. --available_datasets Print a list of available datasets found based on location given by \"--datasets\" --example_fastqs Print example of expected input for FASTQs file --check_fastqs Verify \"--fastqs\" produces the expected inputs --compress Compress (gzip) select outputs (e.g. annotation, variant calls) to reduce overall storage footprint. --keep_all_files Keeps all analysis files created. By default, intermediate files are removed. This will not affect the ability to resume Nextflow runs, and only occurs at the end of the process. --version Print workflow version information --help Show this message and exit --help_all Show a complete list of adjustable parameters --keep_all_files \u00b6 In some processes, Bactopia will delete large intermediate files (e.g. multiple uncompressed FASTQs) only after a process successfully completes. Since this a per-process function, it does not affect Nextflow's ability to resume ( -resume )a workflow. You can deactivate this feature using --keep_all_files . Please, keep in mind the work directory is already large, this will make it 2-3 times larger. Program Specific \u00b6 The remaining parameters are associated with specific programs. In the following sections, these parameters are grouped by which Nextflow process they are applicable to. The description and default values for these parameters were taken from the program to which they apply. It is important to note, not all of the available parameters for each and every program are available in Bactopia. If there is a parameter that was overlooked and should probably be included, please make a suggestion! Annotation \u00b6 --centre STR Sequencing centre ID Default: '' --addgenes Add 'gene' features for each 'CDS' feature --addmrna Add 'mRNA' features for each 'CDS' feature --rawproduct Do not clean up /product annotation --cdsrnaolap Allow [tr]RNA to overlap CDS --prokka_evalue STR Similarity e-value cut-off Default: 1e-09 --prokka_coverage INT Minimum coverage on query protein Default: 80 --norrna Don't run rRNA search --notrna Don't run tRNA search --rnammer Prefer RNAmmer over Barrnap for rRNA prediction Antimicrobial Resistance \u00b6 --update_amr Force amrfinder to update its database. --amr_ident_min Minimum identity for nucleotide hit (0..1). -1 means use a curated threshold if it exists and 0.9 otherwise Default: -1 --amr_coverage_min Minimum coverage of the reference protein (0..1) Default: 0.5 --amr_organism Taxonomy group: Campylobacter, Escherichia, Klebsiella Salmonella, Staphylococcus, Vibrio Default: '' --amr_translation_table NCBI genetic code for translated BLAST Default: 11 --amr_plus Add the plus genes to the report --amr_report_common Suppress proteins common to a taxonomy group Ariba \u00b6 --nucmer_min_id INT Minimum alignment identity (delta-filter -i) Default: 90 --nucmer_min_len INT Minimum alignment length (delta-filter -i) Default: 20 --nucmer_breaklen INT Value to use for -breaklen when running nucmer Default: 200 --assembly_cov INT Target read coverage when sampling reads for assembly Default: 50 --min_scaff_depth INT Minimum number of read pairs needed as evidence for scaffold link between two contigs Default: 10 --spades_options STR Extra options to pass to Spades assembler Default: null --assembled_threshold FLOAT (between 0 and 1) If proportion of gene assembled (regardless of into how many contigs) is at least this value then the flag gene_assembled is set Default: 0.95 --gene_nt_extend INT Max number of nucleotides to extend ends of gene matches to look for start/stop codons Default: 30 --unique_threshold FLOAT (between 0 and 1) If proportion of bases in gene assembled more than once is <= this value, then the flag unique_contig is set Default: 0.03 --ariba_no_clean Do not clean up intermediate files created by Ariba. By default, the local assemblies are deleted. Assembly \u00b6 --shovill_ram INT Try to keep RAM usage below this many GB Default: 32 --assembler STR Assembler: megahit velvet skesa spades Default: skesa --min_contig_len INT Minimum contig length <0=AUTO> Default: 500 --min_contig_cov INT Minimum contig coverage <0=AUTO> Default: 2 --contig_namefmt STR Format of contig FASTA IDs in 'printf' style Default: contig%05d --shovill_opts STR Extra assembler options in quotes eg. spades: \"--untrusted-contigs locus.fna\" ... Default: '' --shovill_kmers STR K-mers to use <blank=AUTO> Default: '' --trim Enable adaptor trimming --nostitch Disable read stitching --nocorr Disable post-assembly correction BLAST \u00b6 --perc_identity INT Percent identity Default: 50 --qcov_hsp_perc INT Percent query coverage per hsp Default: 50 --max_target_seqs INT Maximum number of aligned sequences to keep Default: 2000 --outfmt STR BLAST alignment view options Default: '6 qseqid qlen qstart qend sseqid slen sstart send length evalue bitscore pident nident mismatch gaps qcovs qcovhsp' Counting 31mers \u00b6 --keep_singletons Keep all counted 31-mers Default: Filter out singletons Download FASTQ \u00b6 --aspera_speed STR Speed at which Aspera Connect will download. Default: 100M --max_retry INT Maximum times to retry downloads Default: 10 --ftp_only Only use FTP to download FASTQs from ENA Download Reference Genome \u00b6 --max_references INT Maximum number of nearest neighbor reference genomes to download for variant calling. Default: 1 --random_tie_break On references with matching distances, randomly select one. Default: Pick earliest accession number --disable_auto_variants Disable automatic selection of reference genome based on Mash distances. Insertion Mapping \u00b6 --min_clip INT Minimum size for softclipped region to be extracted from initial mapping Default: 10 --max_clip INT Maximum size for softclipped regions to be included Default: 30 --cutoff INT Minimum depth for mapped region to be kept in bed file Default: 6 --novel_gap_size INT Distance in base pairs between left and right flanks to be called a novel hit Default: 15 --min_range FLOAT Minimum percent size of the gap to be called a known hit Default: 0.9 --max_range FLOAT Maximum percent size of the gap to be called a known hit Default: 1.1 --merging INT Value for merging left and right hits in bed files together to simply calculation of closest and intersecting regions Default: 100 --ismap_all Switch on all alignment reporting for bwa --ismap_minqual INT Mapping quality score for bwa Default: 30 Mapping \u00b6 --keep_unmapped_reads Keep unmapped reads, this does not affect variant calling. --bwa_mem_opts STR Extra BWA MEM options Default: '' --bwa_aln_opts STR Extra BWA ALN options Default: '' --bwa_samse_opts STR Extra BWA SAMSE options Default: '' --bwa_sampe_opts STR Extra BWA SAMPE options Default: '' --bwa_n INT Maximum number of alignments to output in the XA tag for reads paired properly. If a read has more than INT hits, the XA tag will not be written. Default: 9999 Minmer Query \u00b6 --screen_w Winner-takes-all strategy for identity estimates. After counting hashes for each query, hashes that appear in multiple queries will be removed from all except the one with the best identity (ties broken by larger query), and other identities will be reduced. This removes output redundancy, providing a rough compositional outline. Default: True --screen_i FLOAT Minimum identity to report. Inclusive unless set to zero, in which case only identities greater than zero (i.e. with at least one shared hash) will be reported. Set to -1 to output everything. Default: 0.8 Minmer Sketch \u00b6 --mash_sketch INT Sketch size. Each sketch will have at most this many non-redundant min-hashes. Default: 10000 --sourmash_scale INT Choose number of hashes as 1 in FRACTION of input k-mers Default: 10000 Quality Control \u00b6 --adapters FASTA Illumina adapters to remove Default: BBmap adapters --adapter_k INT Kmer length used for finding adapters. Adapters shorter than k will not be found Default: 23 --phix FASTA phiX174 reference genome to remove Default: NC_001422 --phix_k INT Kmer length used for finding phiX174. Contaminants shorter than k will not be found Default: 31 --ktrim STR Trim reads to remove bases matching reference kmers Values: f (do not trim) r (trim to the right, Default) l (trim to the left) --mink INT Look for shorter kmers at read tips down to this length, when k-trimming or masking. 0 means disabled. Enabling this will disable maskmiddle Default: 11 --hdist INT Maximum Hamming distance for ref kmers (subs only) Memory use is proportional to (3*K)^hdist Default: 1 --tpe BOOL When kmer right-trimming, trim both reads to the minimum length of either Values: f (do not equally trim) t (equally trim to the right, Default) --tbo BOOL Trim adapters based on where paired reads overlap Values: f (do not trim by overlap) t (trim by overlap, Default) --qtrim STR Trim read ends to remove bases with quality below trimq. Performed AFTER looking for kmers Values: rl (trim both ends, Default) f (neither end) r (right end only) l (left end only) w (sliding window) --trimq FLOAT Regions with average quality BELOW this will be trimmed if qtrim is set to something other than f Default: 6 --maq INT Reads with average quality (after trimming) below this will be discarded Default: 20 --minlength INT Reads shorter than this after trimming will be discarded. Pairs will be discarded if both are shorter Default: 35 --ftm INT If positive, right-trim length to be equal to zero, modulo this number Default: 5 --tossjunk Discard reads with invalid characters as bases Values: f (keep all reads) t (toss reads with ambiguous bases, Default) --qout STR Output quality offset Values: 33 (PHRED33 offset quality scores, Default) 64 (PHRED64 offset quality scores) auto (keeps the current input offset) --xmx STR This will be passed to Java to set memory usage Examples: '8g' will specify 8 gigs of RAM (Default) '20g' will specify 20 gigs of RAM '200m' will specify 200 megs of RAM --maxcor INT Max number of corrections within a 20bp window Default: 1 --sampleseed INT Set to a positive number to use as the rng seed for sampling Default: 42 Variant Calling \u00b6 --snippy_ram INT Try and keep RAM under this many GB Default: 8 --mapqual INT Minimum read mapping quality to consider Default: 60 --basequal INT Minimum base quality to consider Default: 13 --mincov INT Minimum site depth to for calling alleles Default: 10 --minfrac FLOAT Minimum proportion for variant evidence (0=AUTO) Default: 0 --minqual INT Minimum QUALITY in VCF column 6 Default: 100 --maxsoft INT Maximum soft clipping to allow Default: 10 --bwaopt STR Extra BWA MEM options, eg. -x pacbio Default: '' --fbopt STR Extra Freebayes options, eg. --theta 1E-6 --read-snp-limit 2 Default: ''","title":"Complete Usage"},{"location":"usage-complete/#runtime-parameters","text":"Bactopia includes numerous (100+) configurable parameters. Basically for each step of the pipeline, you can modify the default parameters of a specific tool.","title":"Runtime Parameters"},{"location":"usage-complete/#required","text":"The required parameters depends on how many samples are to be proccessed. You can learn more about which approach to take at Specifying Input FASTQs . ### For Procesessing Multiple Samples --fastqs STR An input file containing the sample name and absolute paths to FASTQs to process ### For Processing A Single Sample --R1 STR First set of reads for paired end in compressed (gzip) FASTQ format --R2 STR Second set of reads for paired end in compressed (gzip) FASTQ format --SE STR Single end set of reads in compressed (gzip) FASTQ format --sample STR The name of the input sequences ### For Downloading from ENA --accessions An input file containing ENA/SRA experiement accessions to be processed --accession A single ENA/SRA Experiment accession to be processed","title":"Required"},{"location":"usage-complete/#dataset","text":"If you followed the steps in Build Datasets , you can use the following parameters to point Bactopia to you datasets. --datasets DIR The path to available public datasets that have already been set up --species STR Determines which species-specific dataset to use for the input sequencing","title":"Dataset"},{"location":"usage-complete/#optional","text":"These optional parameters, while not required, will be quite useful (especially --max_cpus and --cpus !) to tweak. --coverage INT Reduce samples to a given coverage Default: 100x --genome_size INT Expected genome size (bp) for all samples, a value of '0' will disable read error correction and read subsampling. Special values (requires --species): 'min': uses minimum completed genome size of species 'median': uses median completed genome size of species 'mean': uses mean completed genome size of species 'max': uses max completed genome size of species Default: Mash estimate --outdir DIR Directory to write results to Default . --max_memory INT The maximum amount of memory (Gb) allowed to a single process. Default: 32 Gb","title":"Optional"},{"location":"usage-complete/#-genome_size","text":"Throughout the Bactopia workflow a genome size is used for various tasks. By default, a genome size is estimated using Mash. However, users can provide their own value for genome size, use values based on Species Specific Datasets , or completely disable it. Value Result empty Mash is used to estimate the genome size integer Uses the genome size (e.g. --genome_size 2800000 ) provided by the user 0 Read error correct and read subsampling will be disabled. min Requires --species , the minimum completed genome size for a species is used median Requires --species , the median completed genome size for a species is used mean Requires --species , the mean completed genome size for a species is used max Requires --species , the maximum completed genome size for a species is used Mash may not be the most accurate estimate Mash is very convenient to quickly estimate a genome size, but it may not be the most accurate in all cases and will differ between samples. It is recommended that when possible a known genome size or one based off completed genomes should be used.","title":"--genome_size"},{"location":"usage-complete/#-max_cpus-vs-cpus","text":"By default when Nextflow executes, it uses all available cpus to queue up processes. As you might imagine, if you are on a single server with multiple users, this approach of using all cpus might annoy other users! (Whoops sorry!) To circumvent this feature, two parmeters have been included --max_cpus and --cpus . --max_cpus INT The maximum number of processors this workflow should have access to at any given moment Default: 1 --cpus INT Number of processors made available to a single process. If greater than \"--max_cpus\" it will be set equal to \"--max_cpus\" Default: 1 What --max_cpus does is specify to Nextflow the maximum number of cpus it is allowed to occupy at any given time. --cpus on the other hand, specifies how many cpus any given step (qc, assembly, annotation, etc...) can occupy. By default --max_cpus is set to 1 and if --cpus is set to a value greater than --max_cpus it will be set equal to --max_cpus . This appoach errs on the side of caution, by not occupying all cpus on the server without the users consent!","title":"--max_cpus vs --cpus"},{"location":"usage-complete/#helpers","text":"The following parameters are useful to test to test input parameters. --available_datasets Print a list of available datasets found based on location given by \"--datasets\" --example_fastqs Print example of expected input for FASTQs file --check_fastqs Verify \"--fastqs\" produces the expected inputs --compress Compress (gzip) select outputs (e.g. annotation, variant calls) to reduce overall storage footprint. --keep_all_files Keeps all analysis files created. By default, intermediate files are removed. This will not affect the ability to resume Nextflow runs, and only occurs at the end of the process. --version Print workflow version information --help Show this message and exit --help_all Show a complete list of adjustable parameters","title":"Helpers"},{"location":"usage-complete/#-keep_all_files","text":"In some processes, Bactopia will delete large intermediate files (e.g. multiple uncompressed FASTQs) only after a process successfully completes. Since this a per-process function, it does not affect Nextflow's ability to resume ( -resume )a workflow. You can deactivate this feature using --keep_all_files . Please, keep in mind the work directory is already large, this will make it 2-3 times larger.","title":"--keep_all_files"},{"location":"usage-complete/#program-specific","text":"The remaining parameters are associated with specific programs. In the following sections, these parameters are grouped by which Nextflow process they are applicable to. The description and default values for these parameters were taken from the program to which they apply. It is important to note, not all of the available parameters for each and every program are available in Bactopia. If there is a parameter that was overlooked and should probably be included, please make a suggestion!","title":"Program Specific"},{"location":"usage-complete/#annotation","text":"--centre STR Sequencing centre ID Default: '' --addgenes Add 'gene' features for each 'CDS' feature --addmrna Add 'mRNA' features for each 'CDS' feature --rawproduct Do not clean up /product annotation --cdsrnaolap Allow [tr]RNA to overlap CDS --prokka_evalue STR Similarity e-value cut-off Default: 1e-09 --prokka_coverage INT Minimum coverage on query protein Default: 80 --norrna Don't run rRNA search --notrna Don't run tRNA search --rnammer Prefer RNAmmer over Barrnap for rRNA prediction","title":"Annotation"},{"location":"usage-complete/#antimicrobial-resistance","text":"--update_amr Force amrfinder to update its database. --amr_ident_min Minimum identity for nucleotide hit (0..1). -1 means use a curated threshold if it exists and 0.9 otherwise Default: -1 --amr_coverage_min Minimum coverage of the reference protein (0..1) Default: 0.5 --amr_organism Taxonomy group: Campylobacter, Escherichia, Klebsiella Salmonella, Staphylococcus, Vibrio Default: '' --amr_translation_table NCBI genetic code for translated BLAST Default: 11 --amr_plus Add the plus genes to the report --amr_report_common Suppress proteins common to a taxonomy group","title":"Antimicrobial Resistance"},{"location":"usage-complete/#ariba","text":"--nucmer_min_id INT Minimum alignment identity (delta-filter -i) Default: 90 --nucmer_min_len INT Minimum alignment length (delta-filter -i) Default: 20 --nucmer_breaklen INT Value to use for -breaklen when running nucmer Default: 200 --assembly_cov INT Target read coverage when sampling reads for assembly Default: 50 --min_scaff_depth INT Minimum number of read pairs needed as evidence for scaffold link between two contigs Default: 10 --spades_options STR Extra options to pass to Spades assembler Default: null --assembled_threshold FLOAT (between 0 and 1) If proportion of gene assembled (regardless of into how many contigs) is at least this value then the flag gene_assembled is set Default: 0.95 --gene_nt_extend INT Max number of nucleotides to extend ends of gene matches to look for start/stop codons Default: 30 --unique_threshold FLOAT (between 0 and 1) If proportion of bases in gene assembled more than once is <= this value, then the flag unique_contig is set Default: 0.03 --ariba_no_clean Do not clean up intermediate files created by Ariba. By default, the local assemblies are deleted.","title":"Ariba"},{"location":"usage-complete/#assembly","text":"--shovill_ram INT Try to keep RAM usage below this many GB Default: 32 --assembler STR Assembler: megahit velvet skesa spades Default: skesa --min_contig_len INT Minimum contig length <0=AUTO> Default: 500 --min_contig_cov INT Minimum contig coverage <0=AUTO> Default: 2 --contig_namefmt STR Format of contig FASTA IDs in 'printf' style Default: contig%05d --shovill_opts STR Extra assembler options in quotes eg. spades: \"--untrusted-contigs locus.fna\" ... Default: '' --shovill_kmers STR K-mers to use <blank=AUTO> Default: '' --trim Enable adaptor trimming --nostitch Disable read stitching --nocorr Disable post-assembly correction","title":"Assembly"},{"location":"usage-complete/#blast","text":"--perc_identity INT Percent identity Default: 50 --qcov_hsp_perc INT Percent query coverage per hsp Default: 50 --max_target_seqs INT Maximum number of aligned sequences to keep Default: 2000 --outfmt STR BLAST alignment view options Default: '6 qseqid qlen qstart qend sseqid slen sstart send length evalue bitscore pident nident mismatch gaps qcovs qcovhsp'","title":"BLAST"},{"location":"usage-complete/#counting-31mers","text":"--keep_singletons Keep all counted 31-mers Default: Filter out singletons","title":"Counting 31mers"},{"location":"usage-complete/#download-fastq","text":"--aspera_speed STR Speed at which Aspera Connect will download. Default: 100M --max_retry INT Maximum times to retry downloads Default: 10 --ftp_only Only use FTP to download FASTQs from ENA","title":"Download FASTQ"},{"location":"usage-complete/#download-reference-genome","text":"--max_references INT Maximum number of nearest neighbor reference genomes to download for variant calling. Default: 1 --random_tie_break On references with matching distances, randomly select one. Default: Pick earliest accession number --disable_auto_variants Disable automatic selection of reference genome based on Mash distances.","title":"Download Reference Genome"},{"location":"usage-complete/#insertion-mapping","text":"--min_clip INT Minimum size for softclipped region to be extracted from initial mapping Default: 10 --max_clip INT Maximum size for softclipped regions to be included Default: 30 --cutoff INT Minimum depth for mapped region to be kept in bed file Default: 6 --novel_gap_size INT Distance in base pairs between left and right flanks to be called a novel hit Default: 15 --min_range FLOAT Minimum percent size of the gap to be called a known hit Default: 0.9 --max_range FLOAT Maximum percent size of the gap to be called a known hit Default: 1.1 --merging INT Value for merging left and right hits in bed files together to simply calculation of closest and intersecting regions Default: 100 --ismap_all Switch on all alignment reporting for bwa --ismap_minqual INT Mapping quality score for bwa Default: 30","title":"Insertion Mapping"},{"location":"usage-complete/#mapping","text":"--keep_unmapped_reads Keep unmapped reads, this does not affect variant calling. --bwa_mem_opts STR Extra BWA MEM options Default: '' --bwa_aln_opts STR Extra BWA ALN options Default: '' --bwa_samse_opts STR Extra BWA SAMSE options Default: '' --bwa_sampe_opts STR Extra BWA SAMPE options Default: '' --bwa_n INT Maximum number of alignments to output in the XA tag for reads paired properly. If a read has more than INT hits, the XA tag will not be written. Default: 9999","title":"Mapping"},{"location":"usage-complete/#minmer-query","text":"--screen_w Winner-takes-all strategy for identity estimates. After counting hashes for each query, hashes that appear in multiple queries will be removed from all except the one with the best identity (ties broken by larger query), and other identities will be reduced. This removes output redundancy, providing a rough compositional outline. Default: True --screen_i FLOAT Minimum identity to report. Inclusive unless set to zero, in which case only identities greater than zero (i.e. with at least one shared hash) will be reported. Set to -1 to output everything. Default: 0.8","title":"Minmer Query"},{"location":"usage-complete/#minmer-sketch","text":"--mash_sketch INT Sketch size. Each sketch will have at most this many non-redundant min-hashes. Default: 10000 --sourmash_scale INT Choose number of hashes as 1 in FRACTION of input k-mers Default: 10000","title":"Minmer Sketch"},{"location":"usage-complete/#quality-control","text":"--adapters FASTA Illumina adapters to remove Default: BBmap adapters --adapter_k INT Kmer length used for finding adapters. Adapters shorter than k will not be found Default: 23 --phix FASTA phiX174 reference genome to remove Default: NC_001422 --phix_k INT Kmer length used for finding phiX174. Contaminants shorter than k will not be found Default: 31 --ktrim STR Trim reads to remove bases matching reference kmers Values: f (do not trim) r (trim to the right, Default) l (trim to the left) --mink INT Look for shorter kmers at read tips down to this length, when k-trimming or masking. 0 means disabled. Enabling this will disable maskmiddle Default: 11 --hdist INT Maximum Hamming distance for ref kmers (subs only) Memory use is proportional to (3*K)^hdist Default: 1 --tpe BOOL When kmer right-trimming, trim both reads to the minimum length of either Values: f (do not equally trim) t (equally trim to the right, Default) --tbo BOOL Trim adapters based on where paired reads overlap Values: f (do not trim by overlap) t (trim by overlap, Default) --qtrim STR Trim read ends to remove bases with quality below trimq. Performed AFTER looking for kmers Values: rl (trim both ends, Default) f (neither end) r (right end only) l (left end only) w (sliding window) --trimq FLOAT Regions with average quality BELOW this will be trimmed if qtrim is set to something other than f Default: 6 --maq INT Reads with average quality (after trimming) below this will be discarded Default: 20 --minlength INT Reads shorter than this after trimming will be discarded. Pairs will be discarded if both are shorter Default: 35 --ftm INT If positive, right-trim length to be equal to zero, modulo this number Default: 5 --tossjunk Discard reads with invalid characters as bases Values: f (keep all reads) t (toss reads with ambiguous bases, Default) --qout STR Output quality offset Values: 33 (PHRED33 offset quality scores, Default) 64 (PHRED64 offset quality scores) auto (keeps the current input offset) --xmx STR This will be passed to Java to set memory usage Examples: '8g' will specify 8 gigs of RAM (Default) '20g' will specify 20 gigs of RAM '200m' will specify 200 megs of RAM --maxcor INT Max number of corrections within a 20bp window Default: 1 --sampleseed INT Set to a positive number to use as the rng seed for sampling Default: 42","title":"Quality Control"},{"location":"usage-complete/#variant-calling","text":"--snippy_ram INT Try and keep RAM under this many GB Default: 8 --mapqual INT Minimum read mapping quality to consider Default: 60 --basequal INT Minimum base quality to consider Default: 13 --mincov INT Minimum site depth to for calling alleles Default: 10 --minfrac FLOAT Minimum proportion for variant evidence (0=AUTO) Default: 0 --minqual INT Minimum QUALITY in VCF column 6 Default: 100 --maxsoft INT Maximum soft clipping to allow Default: 10 --bwaopt STR Extra BWA MEM options, eg. -x pacbio Default: '' --fbopt STR Extra Freebayes options, eg. --theta 1E-6 --read-snp-limit 2 Default: ''","title":"Variant Calling"},{"location":"workflow-overview/","text":"Workflow Overview \u00b6 Bactopia is an extensive workflow integrating numerous steps in bacterial genome analysis. Through out the workflow there are steps that are always enabled and dataset enabled . Each of the steps depicted in the image below are described in this section. A list of software directly used in each step is also listed. Please check out the Acknowledgements section to get the full list of software as well how to download and cite said software. Always Enabled Steps \u00b6 The Always Enabled Steps are always executed by Bactopia. These steps do not depend of external datasets and thus are always enabled. Gather FASTQs \u00b6 Specifies exactly where the input FASTQs are coming from. If you are using local inputs (e.g. --R1/--R2 , --fastqs ) it will verify they can be accessed. If ENA/SRA accessions ( --accession or --accessions ) were given, they corresponding FASTQs are downloaded in this step. Software Usage ena-dl Download FASTQ files from ENA Validate FASTQs \u00b6 Determines if the FASTQ file contains enough sequencing to continue processing. The --min_reads and --min_basepairs parameters adjust the minimum amount of sequencing required to continue processing. This step does not directly test the validity of the FASTQ format (although, it would fail if the format is invalid!). Software Usage fastq-scan Determine total read and basepairs of FASTQ Original Summary \u00b6 Produces summary statistics (read lengths, quality scores, etc...) based on the original input FASTQs. Software Usage FastQC Generates a HTML report of original FASTQ summary statistics fastq-scan Generates original FASTQ summary statistics in JSON format Genome Size \u00b6 The genome size is by various programs in the Bactopia workflow. By default, if no genome size is given one is estimated using Mash. Otherwise, a specific genome size can be specified or completely disabled using the --genome_size parameter. See Genome Size Parameter to learn more about specifying the genome size. Software Usage Mash If not given, estimates genome size of sample Quality Control \u00b6 The input FASTQs go through a few clean up steps. First, Illumina related adapters and phiX contaminants are removed. Then reads that fail to pass length and/or quality requirements are filtered out. If the genome size is available, sequence error-corrections are made and the total sequencing is reduced to a specified coverage. After this step, all downstream analyses are based on the QC'd FASTQ and the original is no longer used. Software Description BBTools Removes Illumina adapters, phiX contaminants, filters reads based on length and quality score, and reduces inputs to a specified coverage. Lighter Corrects sequencing errors QC Summary \u00b6 Produces summary statistics (read lengths, quality scores, etc...) based on the final set of QC'd FASTQs. Software Usage FastQC Generates a HTML report of QC'd FASTQ summary statistics fastq-scan Generates QC'd FASTQ summary statistics in JSON format Count 31-mers \u00b6 All 31 basepair (31-mers) sequences are counted and the singletons (those 31-mers only counted once) are filtered out. Software Description McCortex Counts 31-mers in the input FASTQ Minmer Sketch \u00b6 A minmer sketch and signature is created based on the QC'd FASTQs for multiple values of k . If datasets are available, the sketches/signatures are used for further downstream analysis. Software Usage Mash Produces a sketch ( k =21,31) of tje QC'd FASTQ Sourmash Produces a signature ( k =21,31,51) of the QC'd FASTQ De novo Assembly \u00b6 The QC'd FASTQs are assembled using the Shovill pipeline. This allows for a seamless assembly process using MEGAHIT , SKESA , SPAdes or Velvet . Software Usage assembly-scan Generates summary statistics of the final assembly Shovill Manages multiple steps in the Illumina assembly process Genome Annotation \u00b6 Genes are predicted and annotated from the assembled genome using Prokka . If available, a clustered RefSeq protein set is used for the first pass of annotation. Software Usage Prokka Predicts and annotates assembled genomes Antimicrobial Resistance \u00b6 Searches for antimicrobial resistance genes and assosiated point mutations in the annotated gene and protein sequences. If datasets are available, local assemblies can also be used to predict antibiotic resistance. Software Usage AMRFinderPlus Predicts antimicrobial resistance based on genes and point mutations Dataset Enabled Steps \u00b6 The remaining Dataset Enabled Steps require supplemental datasets to be available to be executed. There are many datasets available that Bactopia can take advantage of. To learn more about setting up these datasets, check out Build Datasets . These datasets can be broken into two groups, Public Datasets and User Datasets . Public Datasets \u00b6 Publicly available datasets can be used for futher analysis. Call Variants (Auto) \u00b6 Variants are predicted using Snippy . The QC'd FASTQs are aligned to the nearest (based on Mash distance) RefSeq completed genome. By default, only the nearest genome is selected, but multiple genomes can be selected ( --max_references ) or this feature can be completely disabled ( disable_auto_variants ). Software Usage Bedtools Generates the per-base coverage of the reference alignment NCBI Genome Download Downloads the RefSeq completed genome Snippy Manages multiple steps in the haploid variant calling process vcf-annotator Adds annotations from reference GenBank to the final VCF Local Assembly \u00b6 Using available Ariba reference datasets , determines which reference sequences were found with an additional detailed report summarizing the results. Software Usage Ariba Creates local assemblies of reference sequences Minmer Query \u00b6 Screens QC'd FASTQs and signatures against available Minmer Datasets . Software Usage Mash Screens against RefSeq and/or PLSDB sketches Sourmash Screens signature against GenBank Sequence Type \u00b6 Uses a PubMLST.org MLST schema to determine the sequence type of the sample. Software Usage Ariba Runs QC'd FASTQ against a MLST database BLAST Aligns MLST loci against the assembled genome User Datasets \u00b6 Another option is for users to provide their own data to include in the analysis. BLAST Alignment \u00b6 Each gene, protein, or primer sequence provided by the user is aligned against the assembled genome. Software Usage BLAST Aligns reference sequences against the assembled genome Call Variants (User) \u00b6 Uses the same procedure as Call Variants (Auto) , except variants are called against each reference provided by the user. Software Usage Bedtools Generates the per-base coverage of the reference alignment Snippy Manages multiple steps in the haploid variant calling process vcf-annotator Adds annotations from reference GenBank to the final VCF Insertion Sites \u00b6 Identifies the location of insertion sites (if any) for each insertion sequence provided by the user. Software Usage ISMapper Predicts insertion site locations for insertion sequences Reference Mapping \u00b6 Aligns the QC'd FASTQs to each sequence provided by the user. Software Usage Bedtools Generates the per-base coverage of the reference alignment BWA Aligns QC'd FASTQ to a reference sequence Samtools Converts alignment from SAM to BAM","title":"Workflow Overview"},{"location":"workflow-overview/#workflow-overview","text":"Bactopia is an extensive workflow integrating numerous steps in bacterial genome analysis. Through out the workflow there are steps that are always enabled and dataset enabled . Each of the steps depicted in the image below are described in this section. A list of software directly used in each step is also listed. Please check out the Acknowledgements section to get the full list of software as well how to download and cite said software.","title":"Workflow Overview"},{"location":"workflow-overview/#always-enabled-steps","text":"The Always Enabled Steps are always executed by Bactopia. These steps do not depend of external datasets and thus are always enabled.","title":"Always Enabled Steps"},{"location":"workflow-overview/#gather-fastqs","text":"Specifies exactly where the input FASTQs are coming from. If you are using local inputs (e.g. --R1/--R2 , --fastqs ) it will verify they can be accessed. If ENA/SRA accessions ( --accession or --accessions ) were given, they corresponding FASTQs are downloaded in this step. Software Usage ena-dl Download FASTQ files from ENA","title":"Gather FASTQs"},{"location":"workflow-overview/#validate-fastqs","text":"Determines if the FASTQ file contains enough sequencing to continue processing. The --min_reads and --min_basepairs parameters adjust the minimum amount of sequencing required to continue processing. This step does not directly test the validity of the FASTQ format (although, it would fail if the format is invalid!). Software Usage fastq-scan Determine total read and basepairs of FASTQ","title":"Validate FASTQs"},{"location":"workflow-overview/#original-summary","text":"Produces summary statistics (read lengths, quality scores, etc...) based on the original input FASTQs. Software Usage FastQC Generates a HTML report of original FASTQ summary statistics fastq-scan Generates original FASTQ summary statistics in JSON format","title":"Original Summary"},{"location":"workflow-overview/#genome-size","text":"The genome size is by various programs in the Bactopia workflow. By default, if no genome size is given one is estimated using Mash. Otherwise, a specific genome size can be specified or completely disabled using the --genome_size parameter. See Genome Size Parameter to learn more about specifying the genome size. Software Usage Mash If not given, estimates genome size of sample","title":"Genome Size"},{"location":"workflow-overview/#quality-control","text":"The input FASTQs go through a few clean up steps. First, Illumina related adapters and phiX contaminants are removed. Then reads that fail to pass length and/or quality requirements are filtered out. If the genome size is available, sequence error-corrections are made and the total sequencing is reduced to a specified coverage. After this step, all downstream analyses are based on the QC'd FASTQ and the original is no longer used. Software Description BBTools Removes Illumina adapters, phiX contaminants, filters reads based on length and quality score, and reduces inputs to a specified coverage. Lighter Corrects sequencing errors","title":"Quality Control"},{"location":"workflow-overview/#qc-summary","text":"Produces summary statistics (read lengths, quality scores, etc...) based on the final set of QC'd FASTQs. Software Usage FastQC Generates a HTML report of QC'd FASTQ summary statistics fastq-scan Generates QC'd FASTQ summary statistics in JSON format","title":"QC Summary"},{"location":"workflow-overview/#count-31-mers","text":"All 31 basepair (31-mers) sequences are counted and the singletons (those 31-mers only counted once) are filtered out. Software Description McCortex Counts 31-mers in the input FASTQ","title":"Count 31-mers"},{"location":"workflow-overview/#minmer-sketch","text":"A minmer sketch and signature is created based on the QC'd FASTQs for multiple values of k . If datasets are available, the sketches/signatures are used for further downstream analysis. Software Usage Mash Produces a sketch ( k =21,31) of tje QC'd FASTQ Sourmash Produces a signature ( k =21,31,51) of the QC'd FASTQ","title":"Minmer Sketch"},{"location":"workflow-overview/#de-novo-assembly","text":"The QC'd FASTQs are assembled using the Shovill pipeline. This allows for a seamless assembly process using MEGAHIT , SKESA , SPAdes or Velvet . Software Usage assembly-scan Generates summary statistics of the final assembly Shovill Manages multiple steps in the Illumina assembly process","title":"De novo Assembly"},{"location":"workflow-overview/#genome-annotation","text":"Genes are predicted and annotated from the assembled genome using Prokka . If available, a clustered RefSeq protein set is used for the first pass of annotation. Software Usage Prokka Predicts and annotates assembled genomes","title":"Genome Annotation"},{"location":"workflow-overview/#antimicrobial-resistance","text":"Searches for antimicrobial resistance genes and assosiated point mutations in the annotated gene and protein sequences. If datasets are available, local assemblies can also be used to predict antibiotic resistance. Software Usage AMRFinderPlus Predicts antimicrobial resistance based on genes and point mutations","title":"Antimicrobial Resistance"},{"location":"workflow-overview/#dataset-enabled-steps","text":"The remaining Dataset Enabled Steps require supplemental datasets to be available to be executed. There are many datasets available that Bactopia can take advantage of. To learn more about setting up these datasets, check out Build Datasets . These datasets can be broken into two groups, Public Datasets and User Datasets .","title":"Dataset Enabled Steps"},{"location":"workflow-overview/#public-datasets","text":"Publicly available datasets can be used for futher analysis.","title":"Public Datasets"},{"location":"workflow-overview/#call-variants-auto","text":"Variants are predicted using Snippy . The QC'd FASTQs are aligned to the nearest (based on Mash distance) RefSeq completed genome. By default, only the nearest genome is selected, but multiple genomes can be selected ( --max_references ) or this feature can be completely disabled ( disable_auto_variants ). Software Usage Bedtools Generates the per-base coverage of the reference alignment NCBI Genome Download Downloads the RefSeq completed genome Snippy Manages multiple steps in the haploid variant calling process vcf-annotator Adds annotations from reference GenBank to the final VCF","title":"Call Variants (Auto)"},{"location":"workflow-overview/#local-assembly","text":"Using available Ariba reference datasets , determines which reference sequences were found with an additional detailed report summarizing the results. Software Usage Ariba Creates local assemblies of reference sequences","title":"Local Assembly"},{"location":"workflow-overview/#minmer-query","text":"Screens QC'd FASTQs and signatures against available Minmer Datasets . Software Usage Mash Screens against RefSeq and/or PLSDB sketches Sourmash Screens signature against GenBank","title":"Minmer Query"},{"location":"workflow-overview/#sequence-type","text":"Uses a PubMLST.org MLST schema to determine the sequence type of the sample. Software Usage Ariba Runs QC'd FASTQ against a MLST database BLAST Aligns MLST loci against the assembled genome","title":"Sequence Type"},{"location":"workflow-overview/#user-datasets","text":"Another option is for users to provide their own data to include in the analysis.","title":"User Datasets"},{"location":"workflow-overview/#blast-alignment","text":"Each gene, protein, or primer sequence provided by the user is aligned against the assembled genome. Software Usage BLAST Aligns reference sequences against the assembled genome","title":"BLAST Alignment"},{"location":"workflow-overview/#call-variants-user","text":"Uses the same procedure as Call Variants (Auto) , except variants are called against each reference provided by the user. Software Usage Bedtools Generates the per-base coverage of the reference alignment Snippy Manages multiple steps in the haploid variant calling process vcf-annotator Adds annotations from reference GenBank to the final VCF","title":"Call Variants (User)"},{"location":"workflow-overview/#insertion-sites","text":"Identifies the location of insertion sites (if any) for each insertion sequence provided by the user. Software Usage ISMapper Predicts insertion site locations for insertion sequences","title":"Insertion Sites"},{"location":"workflow-overview/#reference-mapping","text":"Aligns the QC'd FASTQs to each sequence provided by the user. Software Usage Bedtools Generates the per-base coverage of the reference alignment BWA Aligns QC'd FASTQ to a reference sequence Samtools Converts alignment from SAM to BAM","title":"Reference Mapping"}]}